from __future__ import annotations

import json
import hashlib
import os
import platform
import queue
import re
import subprocess
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter
from collections.abc import Callable
from datetime import datetime, timedelta, timezone
from typing import Any
from urllib.parse import urlparse

from fastapi import APIRouter, Depends, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func, select
from sqlalchemy.orm import Session

from app import crud
from app.db import create_session
from app.connectors.douyin import _extract_sec_uid as extract_douyin_uid
from app.connectors.xiaohongshu import _extract_user_id as extract_xhs_uid
from app.connectors.weibo import _extract_uid as extract_weibo_uid
from app.db import get_session
from app.models import AgentMemoryNote, Contact, Message, User
from app.routers.auth import get_current_user
from app.schemas import (
    AelinAction,
    AelinChatRequest,
    AelinChatResponse,
    AelinCitation,
    AelinContextResponse,
    AelinDailyBrief,
    AelinDailyBriefAction,
    AelinDeviceModeApplyRequest,
    AelinDeviceModeApplyResponse,
    AelinDeviceOptimizeResponse,
    AelinDeviceProcessActionRequest,
    AelinDeviceProcessActionResponse,
    AelinDeviceProcessItem,
    AelinDeviceProcessResponse,
    AelinLayoutCard,
    AelinMemoryLayerItem,
    AelinMemoryLayers,
    AelinNotificationItem,
    AelinNotificationResponse,
    AelinProactivePollResponse,
    AelinTrackingItem,
    AelinTrackingListResponse,
    AelinPinRecommendationItem,
    AelinTrackConfirmRequest,
    AelinTrackConfirmResponse,
    AelinToolStep,
    AelinTodoItem,
    AgentConfigOut,
    AgentFocusItemOut,
    AgentMemoryNoteOut,
)
from app.services.agent_memory import AgentMemoryService
from app.services.encryption import decrypt_optional
from app.services.llm import LLMService
from app.services.summarizer import RuleBasedSummarizer
from app.services.sync_jobs import enqueue_sync_job
from app.services.web_search import WebSearchResult, WebSearchService

try:
    import psutil  # type: ignore
except Exception:  # pragma: no cover - optional runtime dependency
    psutil = None

router = APIRouter(prefix="/aelin", tags=["aelin"])

_memory = AgentMemoryService()
_summarizer = RuleBasedSummarizer()
_web_search = WebSearchService()

_TRACKABLE_SOURCES = {
    "auto",
    "web",
    "rss",
    "x",
    "douyin",
    "xiaohongshu",
    "weibo",
    "bilibili",
    "email",
}

_MAX_WEB_SUBAGENTS = 5
_MAX_LOCAL_SUBAGENTS = 5
_MAX_CONTEXT_BOUNDARIES = 10
_PROACTIVE_STATE_SOURCE_PREFIX = "proactive_state"
_PROACTIVE_SEEN_LIMIT = 180
_DEVICE_MODE_SOURCE = "device_mode_state"
_DEVICE_ALLOWED_PROCESS_ACTIONS = {"terminate", "set_low_priority", "set_high_priority"}

_AELIN_EXPRESSION_IDS = {
    "exp-01",
    "exp-02",
    "exp-03",
    "exp-04",
    "exp-05",
    "exp-06",
    "exp-07",
    "exp-08",
    "exp-09",
    "exp-10",
    "exp-11",
}

_AELIN_EXPRESSION_META: dict[str, dict[str, str]] = {
    "exp-01": {"label": "æ‚å˜´æƒŠå–œ", "usage": "å®³ç¾ã€æƒŠå–œã€è¢«å¤¸æ—¶çš„å¯çˆ±åé¦ˆ"},
    "exp-02": {"label": "çƒ­æƒ…å‡ºå‡»", "usage": "å¼€åœºæ‰“æ‹›å‘¼ã€æ¨è¿›æ‰§è¡Œã€å¼ºç§¯æåé¦ˆ"},
    "exp-03": {"label": "æ¸©æŸ”èµåŒ", "usage": "æ”¯æŒã€è®¤å¯ã€å®‰æŠšã€æ¸©å’Œé¼“åŠ±"},
    "exp-04": {"label": "æ‰˜è…®æ€è€ƒ", "usage": "è§£é‡Šã€åˆ†æã€ç­”ç–‘ã€é»˜è®¤äº¤æµ"},
    "exp-05": {"label": "è½»å£°æé†’", "usage": "æ³¨æ„äº‹é¡¹ã€é£é™©æç¤ºã€ä¿å®ˆå»ºè®®"},
    "exp-06": {"label": "å·çœ‹è§‚å¯Ÿ", "usage": "å›´è§‚è¿›å±•ã€æŒç»­å…³æ³¨ã€ç­‰å¾…æ›´å¤šçº¿ç´¢"},
    "exp-07": {"label": "ä½è½æ±‚åŠ©", "usage": "å¤±è´¥ã€é—æ†¾ã€é“æ­‰ã€éœ€è¦å¸®åŠ©"},
    "exp-08": {"label": "ä¸æ»¡å§”å±ˆ", "usage": "åæ§½ã€ä¸çˆ½ã€æŠ—è®®ã€æƒ…ç»ªæ€§åé¦ˆ"},
    "exp-09": {"label": "æŒ‡ç€å¤§ç¬‘", "usage": "ç©æ¢—ã€å¹½é»˜ã€è½»æ¾è°ƒä¾ƒ"},
    "exp-10": {"label": "å‘è´¢å¾—æ„", "usage": "æˆæœçªå‡ºã€æå®šä»»åŠ¡ã€é«˜ä»·å€¼æ”¶è·"},
    "exp-11": {"label": "è¶´æ¡Œèººå¹³", "usage": "å›°å€¦ã€è¿‡è½½ã€ç²¾åŠ›ä¸è¶³ã€éœ€è¦ä¼‘æ¯"},
}

_AELIN_EXPRESSION_ALIASES: dict[str, str] = {
    "æƒŠå–œ": "exp-01",
    "å®³ç¾": "exp-01",
    "è„¸çº¢": "exp-01",
    "å¼€å¿ƒ": "exp-02",
    "å…´å¥‹": "exp-02",
    "æœŸå¾…": "exp-02",
    "æ¯”å¿ƒ": "exp-03",
    "æ”¯æŒ": "exp-03",
    "å®‰æŠš": "exp-03",
    "é»˜è®¤": "exp-04",
    "å‹å¥½": "exp-04",
    "ç–‘é—®": "exp-04",
    "å›°æƒ‘": "exp-04",
    "ä¸¥è‚ƒ": "exp-05",
    "è­¦æƒ•": "exp-05",
    "æé†’": "exp-05",
    "å›´è§‚": "exp-06",
    "è§‚å¯Ÿ": "exp-06",
    "å¤±è´¥": "exp-07",
    "æŠ±æ­‰": "exp-07",
    "å§”å±ˆ": "exp-08",
    "ç”Ÿæ°”": "exp-08",
    "æ„¤æ€’": "exp-08",
    "ç¬‘": "exp-09",
    "å¤§ç¬‘": "exp-09",
    "è°ƒçš®": "exp-09",
    "çœ¨çœ¼": "exp-09",
    "å–œæ¬¢": "exp-10",
    "å¿ƒåŠ¨": "exp-10",
    "å‘è´¢": "exp-10",
    "å›°": "exp-11",
    "å›°å€¦": "exp-11",
    "å‘æ‡µ": "exp-11",
    "èººå¹³": "exp-11",
}

_AELIN_EMOJI_BY_EXPRESSION: dict[str, str] = {
    "exp-01": "ğŸ¥¹",
    "exp-02": "âœ¨",
    "exp-03": "ğŸ¤",
    "exp-04": "ğŸ™‚",
    "exp-05": "âš ï¸",
    "exp-06": "ğŸ‘€",
    "exp-07": "ğŸ¥²",
    "exp-08": "ğŸ˜¤",
    "exp-09": "ğŸ˜‚",
    "exp-10": "ğŸ’°",
    "exp-11": "ğŸ˜®â€ğŸ’¨",
}
_EMOJI_CHAR_RE = re.compile(r"[\u2600-\u27BF\U0001F300-\U0001FAFF]")


def _default_config() -> AgentConfigOut:
    return AgentConfigOut(
        provider="rule_based",
        base_url="https://api.openai.com/v1",
        model="gpt-4o-mini",
        temperature=0.2,
        has_api_key=False,
    )


def _config_out(db: Session, user_id: int) -> AgentConfigOut:
    config = crud.get_agent_config(db, user_id=user_id)
    if config is None:
        return _default_config()

    api_key = decrypt_optional(config.api_key)
    return AgentConfigOut(
        provider=(config.provider or "rule_based").lower(),
        base_url=config.base_url or "https://api.openai.com/v1",
        model=config.model or "gpt-4o-mini",
        temperature=float(config.temperature or 0.2),
        has_api_key=bool(api_key),
    )


def _resolve_llm_service(db: Session, user: User) -> tuple[LLMService, str]:
    config = _config_out(db, user.id)
    provider = (config.provider or "rule_based").lower()
    if provider in {"rule_based", "rule-based", "builtin", "local"}:
        return LLMService(config, None), "rule_based"

    stored = crud.get_agent_config(db, user_id=user.id)
    api_key = decrypt_optional(stored.api_key if stored else None) if stored else None
    if not api_key or not (config.base_url or "").strip():
        # Keep provider type so caller can show explicit configuration errors
        # instead of silently falling back to rule-based templated replies.
        return LLMService(config, None), "openai"
    return LLMService(config, api_key), "openai"


def _normalize_workspace(raw: str) -> str:
    clean = " ".join((raw or "").strip().split())
    return (clean[:64] if clean else "default") or "default"


def _proactive_state_source(workspace: str) -> str:
    workspace_norm = _normalize_workspace(workspace)
    if workspace_norm == "default":
        return _PROACTIVE_STATE_SOURCE_PREFIX
    return f"{_PROACTIVE_STATE_SOURCE_PREFIX}:{workspace_norm}"


def _json_from_text(raw: str) -> dict[str, Any]:
    try:
        parsed = json.loads(raw or "{}")
    except Exception:
        return {}
    return parsed if isinstance(parsed, dict) else {}


def _safe_int_list(raw: Any, *, max_items: int = _PROACTIVE_SEEN_LIMIT) -> list[int]:
    out: list[int] = []
    seen: set[int] = set()
    if not isinstance(raw, list):
        return out
    for item in raw:
        try:
            value = int(item)
        except Exception:
            continue
        if value <= 0 or value in seen:
            continue
        seen.add(value)
        out.append(value)
        if len(out) >= max_items:
            break
    return out


def _load_proactive_state(db: Session, *, user_id: int, workspace: str) -> tuple[AgentMemoryNote | None, dict[str, Any]]:
    source = _proactive_state_source(workspace)
    row = db.scalar(
        select(AgentMemoryNote)
        .where(AgentMemoryNote.user_id == user_id, AgentMemoryNote.source == source)
        .order_by(AgentMemoryNote.updated_at.desc(), AgentMemoryNote.id.desc())
        .limit(1)
    )
    if row is None:
        return None, {}
    return row, _json_from_text(row.content or "{}")


def _save_proactive_state(
    db: Session,
    *,
    user_id: int,
    workspace: str,
    existing: AgentMemoryNote | None,
    state: dict[str, Any],
) -> AgentMemoryNote:
    source = _proactive_state_source(workspace)
    payload = json.dumps(state, ensure_ascii=False, separators=(",", ":"))
    row = existing
    if row is None:
        row = AgentMemoryNote(
            user_id=user_id,
            kind="system",
            source=source,
            content=payload,
        )
        db.add(row)
        return row
    row.kind = "system"
    row.source = source
    row.content = payload
    db.add(row)
    return row


def _parse_iso_datetime(raw: str | None) -> datetime | None:
    if not raw:
        return None
    text = str(raw).strip()
    if not text:
        return None
    try:
        parsed = datetime.fromisoformat(text.replace("Z", "+00:00"))
    except Exception:
        return None
    if parsed.tzinfo is None:
        parsed = parsed.replace(tzinfo=timezone.utc)
    return parsed


def _device_is_windows() -> bool:
    return platform.system().strip().lower().startswith("win")


def _normalize_device_mode(raw: str) -> str:
    mode = str(raw or "").strip().lower()
    alias = {
        "meeting": "meeting",
        "focus": "focus",
        "sleep": "sleep",
        "normal": "normal",
        "default": "normal",
        "å¼€ä¼š": "meeting",
        "ä¸“æ³¨": "focus",
        "ç¡çœ ": "sleep",
        "æ¢å¤": "normal",
    }
    return alias.get(mode, "normal")


def _run_powershell(script: str, *, timeout_s: int = 8) -> tuple[bool, str]:
    try:
        proc = subprocess.run(
            ["powershell", "-NoProfile", "-ExecutionPolicy", "Bypass", "-Command", script],
            capture_output=True,
            text=True,
            timeout=max(1, int(timeout_s)),
            encoding="utf-8",
            errors="ignore",
        )
    except Exception as exc:
        return False, str(exc)
    output = (proc.stdout or proc.stderr or "").strip()
    return proc.returncode == 0, output


def _set_windows_toast_enabled(enabled: bool) -> tuple[bool, str]:
    value = "1" if enabled else "0"
    script = (
        "New-Item -Path 'HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\PushNotifications' "
        "-Force | Out-Null; "
        f"Set-ItemProperty -Path 'HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\PushNotifications' "
        f"-Name ToastEnabled -Type DWord -Value {value}; "
        "Write-Output 'ok'"
    )
    ok, detail = _run_powershell(script)
    return ok, detail or ("ok" if ok else "failed")


def _set_windows_brightness(percent: int) -> tuple[bool, str]:
    safe = max(10, min(100, int(percent or 35)))
    script = (
        "$m = Get-CimInstance -Namespace root/WMI -ClassName WmiMonitorBrightnessMethods -ErrorAction SilentlyContinue; "
        f"if ($m) {{ $null = $m.WmiSetBrightness(1,{safe}); Write-Output 'ok'; }} "
        "else { Write-Output 'unsupported'; exit 1; }"
    )
    ok, detail = _run_powershell(script)
    return ok, detail or ("ok" if ok else "brightness unsupported")


def _set_process_priority(pid: int, level: str) -> tuple[bool, str]:
    if psutil is None:
        return False, "psutil unavailable"
    try:
        proc = psutil.Process(int(pid))
    except Exception as exc:
        return False, str(exc)
    target = str(level or "").strip().lower()
    try:
        if _device_is_windows():
            if target == "high":
                proc.nice(psutil.HIGH_PRIORITY_CLASS)
            else:
                proc.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)
        else:
            proc.nice(-5 if target == "high" else 10)
        return True, f"priority set to {target or 'low'}"
    except Exception as exc:
        return False, str(exc)


def _collect_device_process_items(*, sort_by: str, limit: int) -> list[AelinDeviceProcessItem]:
    if psutil is None:
        return []
    max_items = max(1, min(200, int(limit or 40)))
    sort_key = "memory" if str(sort_by or "").strip().lower() == "memory" else "cpu"
    now = datetime.now(timezone.utc)
    current_user = str(os.environ.get("USERNAME") or os.environ.get("USER") or "").strip().lower()
    critical_names = {
        "system",
        "idle",
        "registry",
        "csrss.exe",
        "wininit.exe",
        "services.exe",
        "lsass.exe",
        "svchost.exe",
        "explorer.exe",
    }

    procs: list[Any] = []
    for proc in psutil.process_iter(attrs=["pid", "name", "username", "status", "memory_info", "create_time"]):
        try:
            proc.cpu_percent(None)
            procs.append(proc)
        except Exception:
            continue
    time.sleep(0.12)

    rows: list[AelinDeviceProcessItem] = []
    for proc in procs:
        try:
            with proc.oneshot():
                pid = int(proc.pid)
                name = str(proc.info.get("name") or proc.name() or f"pid-{pid}").strip()
                username = str(proc.info.get("username") or "").strip()
                status = str(proc.info.get("status") or proc.status() or "").strip().lower()
                cpu = float(proc.cpu_percent(None) or 0.0)
                mem = proc.info.get("memory_info") or proc.memory_info()
                memory_mb = float(getattr(mem, "rss", 0) / (1024 * 1024))
                created = proc.info.get("create_time") or proc.create_time()
                created_iso = datetime.fromtimestamp(float(created), tz=timezone.utc).isoformat() if created else None
        except Exception:
            continue
        reasons: list[str] = []
        score = 0.0
        if cpu >= 80:
            reasons.append("CPU æŒç»­é«˜å ç”¨")
            score += 2.8
        elif cpu >= 55:
            reasons.append("CPU åé«˜")
            score += 1.5
        if memory_mb >= 1400:
            reasons.append("å†…å­˜å ç”¨è¿‡é«˜")
            score += 2.5
        elif memory_mb >= 800:
            reasons.append("å†…å­˜å ç”¨åé«˜")
            score += 1.2
        if status in {"zombie", "stopped"}:
            reasons.append(f"è¿›ç¨‹çŠ¶æ€å¼‚å¸¸: {status}")
            score += 1.8

        name_lower = name.lower()
        user_match = bool(current_user and current_user in username.lower())
        safe_to_terminate = (name_lower not in critical_names) and user_match and (pid > 120)
        rows.append(
            AelinDeviceProcessItem(
                pid=pid,
                name=name,
                cpu_percent=round(cpu, 2),
                memory_mb=round(memory_mb, 1),
                status=status,
                username=username,
                create_time=created_iso,
                anomaly_score=round(score, 2),
                anomaly_reasons=reasons[:3],
                safe_to_terminate=safe_to_terminate,
            )
        )

    if sort_key == "memory":
        rows.sort(key=lambda x: (x.anomaly_score, x.memory_mb, x.cpu_percent), reverse=True)
    else:
        rows.sort(key=lambda x: (x.anomaly_score, x.cpu_percent, x.memory_mb), reverse=True)
    return rows[:max_items]


def _load_device_mode_state(db: Session, *, user_id: int) -> tuple[AgentMemoryNote | None, dict[str, Any]]:
    row = db.scalar(
        select(AgentMemoryNote)
        .where(AgentMemoryNote.user_id == user_id, AgentMemoryNote.source == _DEVICE_MODE_SOURCE)
        .order_by(AgentMemoryNote.updated_at.desc(), AgentMemoryNote.id.desc())
        .limit(1)
    )
    if row is None:
        return None, {}
    parsed = _json_from_text(row.content or "{}")
    return row, parsed


def _save_device_mode_state(
    db: Session,
    *,
    user_id: int,
    existing: AgentMemoryNote | None,
    payload: dict[str, Any],
) -> None:
    text = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))
    row = existing
    if row is None:
        row = AgentMemoryNote(
            user_id=user_id,
            kind="system",
            source=_DEVICE_MODE_SOURCE,
            content=text,
        )
        db.add(row)
        return
    row.kind = "system"
    row.source = _DEVICE_MODE_SOURCE
    row.content = text
    db.add(row)


def _apply_device_mode(mode: str) -> tuple[str, str, str, list[str], list[str]]:
    mode_norm = _normalize_device_mode(mode)
    steps: list[str] = []
    warnings: list[str] = []

    if not _device_is_windows():
        warnings.append("å½“å‰ä»…åœ¨ Windows æä¾›ç³»ç»Ÿçº§æ¨¡å¼æ§åˆ¶ï¼Œå…¶å®ƒç³»ç»Ÿå°†åªè®°å½•æ¨¡å¼çŠ¶æ€ã€‚")
        return mode_norm, "partial", f"æ¨¡å¼å·²åˆ‡æ¢ä¸º {mode_norm}ï¼ˆç³»ç»Ÿæ§åˆ¶å—é™ï¼‰", steps, warnings

    if mode_norm in {"meeting", "focus", "sleep"}:
        ok_toast, detail_toast = _set_windows_toast_enabled(False)
        if ok_toast:
            steps.append("å·²é™åˆ¶ç³»ç»Ÿé€šçŸ¥æ¨ªå¹…ï¼ˆToastï¼‰ã€‚")
        else:
            warnings.append(f"é™åˆ¶ç³»ç»Ÿé€šçŸ¥å¤±è´¥: {detail_toast}")
    else:
        ok_toast, detail_toast = _set_windows_toast_enabled(True)
        if ok_toast:
            steps.append("å·²æ¢å¤ç³»ç»Ÿé€šçŸ¥æ¨ªå¹…ã€‚")
        else:
            warnings.append(f"æ¢å¤ç³»ç»Ÿé€šçŸ¥å¤±è´¥: {detail_toast}")

    if mode_norm == "focus":
        wechat_hits = 0
        if psutil is not None:
            for proc in psutil.process_iter(attrs=["pid", "name"]):
                try:
                    name = str(proc.info.get("name") or "").lower()
                    if "wechat" not in name:
                        continue
                    ok, detail = _set_process_priority(int(proc.pid), "low")
                    if ok:
                        wechat_hits += 1
                    else:
                        warnings.append(f"WeChat ä¼˜å…ˆçº§è°ƒæ•´å¤±è´¥: {detail}")
                except Exception:
                    continue
        if wechat_hits > 0:
            steps.append(f"å·²é™ä½ {wechat_hits} ä¸ª WeChat è¿›ç¨‹ä¼˜å…ˆçº§ï¼ˆå‡å°‘æ‰“æ‰°ï¼‰ã€‚")
        else:
            warnings.append("æœªæ£€æµ‹åˆ° WeChat è¿›ç¨‹ï¼Œå¾®ä¿¡æç¤ºéŸ³éœ€æ‰‹åŠ¨åœ¨ç³»ç»Ÿæ··éŸ³å™¨ä¸­å…³é—­ã€‚")

    if mode_norm == "sleep":
        ok_brightness, detail_brightness = _set_windows_brightness(35)
        if ok_brightness:
            steps.append("å·²å°è¯•é™ä½å±å¹•äº®åº¦è‡³ 35%ã€‚")
        else:
            warnings.append(f"äº®åº¦è°ƒæ•´å¤±è´¥æˆ–è®¾å¤‡ä¸æ”¯æŒ: {detail_brightness}")

    if mode_norm == "meeting":
        warnings.append("ç³»ç»Ÿé™éŸ³å¼€å…³åœ¨éƒ¨åˆ†è®¾å¤‡ä¸Šéœ€æ‰‹åŠ¨ç¡®è®¤ï¼ˆå·²ä¿ç•™å¼€ä¼šæ¨¡å¼çŠ¶æ€ï¼‰ã€‚")

    status = "applied" if not warnings else "partial"
    summary = (
        f"{mode_norm} æ¨¡å¼å·²åº”ç”¨ã€‚"
        if status == "applied"
        else f"{mode_norm} æ¨¡å¼å·²éƒ¨åˆ†åº”ç”¨ï¼Œè¯·æŸ¥çœ‹è­¦å‘Šé¡¹ã€‚"
    )
    return mode_norm, status, summary, steps, warnings


def _to_layout_cards(raw_cards: list[dict]) -> list[AelinLayoutCard]:
    out: list[AelinLayoutCard] = []
    for row in raw_cards[:120]:
        try:
            card = AelinLayoutCard(
                contact_id=int(row.get("contact_id") or 0),
                display_name=str(row.get("display_name") or f"contact-{row.get('contact_id') or 'unknown'}"),
                pinned=bool(row.get("pinned")),
                order=max(0, int(row.get("order") or 0)),
                x=max(0.0, float(row.get("x") or 0.0)),
                y=max(0.0, float(row.get("y") or 0.0)),
                width=float(row.get("width") or 312.0),
                height=float(row.get("height") or 316.0),
            )
        except Exception:
            continue
        if card.contact_id <= 0:
            continue
        out.append(card)
    out.sort(key=lambda x: (x.y, x.x, x.order, x.display_name))
    return out[:80]


def _build_context_bundle(db: Session, user_id: int, *, workspace: str, query: str) -> dict:
    workspace_norm = _normalize_workspace(workspace)
    snap = _memory.snapshot(db, user_id, query=query)
    note_rows = _memory.list_notes(db, user_id, limit=24)
    notes: list[AgentMemoryNoteOut] = []
    for row in note_rows:
        src = (row.source or "").strip().lower()
        if src == "todo" or src.startswith("card_layout"):
            continue
        notes.append(
            AgentMemoryNoteOut(
                id=row.id,
                kind=row.kind,
                content=row.content,
                source=row.source,
                updated_at=row.updated_at.isoformat() if row.updated_at else "",
            )
        )
        if len(notes) >= 12:
            break

    todos_raw = _memory.list_todos(db, user_id, include_done=False, limit=10)
    todos: list[AelinTodoItem] = []
    for row in todos_raw:
        try:
            todos.append(AelinTodoItem(**row))
        except Exception:
            continue

    pins_raw = _memory.recommend_pins(db, user_id, limit=6)
    pin_recommendations: list[AelinPinRecommendationItem] = []
    for row in pins_raw:
        try:
            pin_recommendations.append(AelinPinRecommendationItem(**row))
        except Exception:
            continue

    brief_raw = _memory.build_daily_brief(db, user_id)
    daily_brief = AelinDailyBrief(
        generated_at=brief_raw["generated_at"],
        summary=str(brief_raw.get("summary") or ""),
        top_updates=[AgentFocusItemOut(**item) for item in brief_raw.get("top_updates", [])],
        actions=[AelinDailyBriefAction(**item) for item in brief_raw.get("actions", [])],
    )

    layout_cards = _to_layout_cards(_memory.get_latest_layout_cards(db, user_id, workspace=workspace_norm))
    memory_layers_raw = _memory.build_memory_layers(db, user_id, workspace=workspace_norm, query=query)
    memory_layers = AelinMemoryLayers(
        facts=[AelinMemoryLayerItem(**item) for item in (memory_layers_raw.get("facts") or [])],
        preferences=[AelinMemoryLayerItem(**item) for item in (memory_layers_raw.get("preferences") or [])],
        in_progress=[AelinMemoryLayerItem(**item) for item in (memory_layers_raw.get("in_progress") or [])],
        generated_at=datetime.now(timezone.utc),
    )
    notifications = [
        AelinNotificationItem(**item)
        for item in _memory.build_notifications(db, user_id, limit=24)
    ]

    return {
        "workspace": workspace_norm,
        "summary": str(snap.get("summary") or ""),
        "focus_items": [AgentFocusItemOut(**item) for item in snap.get("focus_items", [])],
        "focus_items_raw": list(snap.get("focus_items", [])),
        "notes": notes,
        "notes_count": len(notes),
        "todos": todos,
        "pin_recommendations": pin_recommendations,
        "daily_brief": daily_brief,
        "layout_cards": layout_cards,
        "memory_layers": memory_layers,
        "notifications": notifications,
    }


def _to_citations(raw_focus_items: list[dict], max_items: int) -> list[AelinCitation]:
    items: list[AelinCitation] = []
    for row in raw_focus_items[: max(1, min(20, max_items))]:
        try:
            items.append(
                AelinCitation(
                    message_id=int(row.get("message_id") or 0),
                    source=str(row.get("source") or "unknown"),
                    source_label=str(row.get("source_label") or row.get("source") or "unknown"),
                    sender=str(row.get("sender") or ""),
                    sender_avatar_url=(
                        str(row.get("sender_avatar_url") or "").strip() or None
                    ),
                    title=str(row.get("title") or ""),
                    received_at=str(row.get("received_at") or ""),
                    score=float(row.get("score") or 0.0),
                )
            )
        except Exception:
            continue
    return items


def _hydrate_citation_avatars(
    db: Session,
    user_id: int,
    citations: list[AelinCitation],
) -> list[AelinCitation]:
    missing_ids = [int(it.message_id) for it in citations if not it.sender_avatar_url and int(it.message_id or 0) > 0]
    if not missing_ids:
        return citations

    rows = db.execute(
        select(Message.id, Contact.avatar_url)
        .join(Contact, Contact.id == Message.contact_id)
        .where(
            Message.user_id == user_id,
            Contact.user_id == user_id,
            Message.id.in_(missing_ids),
        )
    ).all()
    avatar_by_message_id: dict[int, str] = {}
    for message_id, avatar_url in rows:
        if avatar_url:
            avatar_by_message_id[int(message_id)] = str(avatar_url)

    if not avatar_by_message_id:
        return citations

    out: list[AelinCitation] = []
    for it in citations:
        if it.sender_avatar_url:
            out.append(it)
            continue
        avatar = avatar_by_message_id.get(int(it.message_id or 0))
        if avatar:
            out.append(it.model_copy(update={"sender_avatar_url": avatar}))
        else:
            out.append(it)
    return out


def _rule_based_answer(
    query: str,
    summary: str,
    citations: list[AelinCitation],
    *,
    brief_summary: str = "",
    todo_titles: list[str] | None = None,
    image_count: int = 0,
) -> str:
    image_tip = (
        f"\n\nä½ ä¸Šä¼ äº† {image_count} å¼ å›¾ç‰‡ã€‚å½“å‰è§„åˆ™æ¨¡å¼ä¸å…·å¤‡å›¾ç‰‡ç†è§£èƒ½åŠ›ï¼Œè‹¥éœ€å›¾åƒåˆ†æè¯·é…ç½®æ”¯æŒè§†è§‰çš„æ¨¡å‹ã€‚"
        if image_count > 0
        else ""
    )
    if not citations:
        todo_line = ""
        if todo_titles:
            todo_line = "\n\nä½ å½“å‰å¾…è·Ÿè¿›äº‹é¡¹ï¼š\n" + "\n".join(f"- {title}" for title in todo_titles[:4])
        if summary:
            return (
                "æˆ‘å·²åœ¨ä½ çš„é•¿æœŸè®°å¿†ä¸­æ£€ç´¢ç›¸å…³å†…å®¹ï¼Œä½†å½“å‰ç¼ºå°‘è¶³å¤Ÿçš„æ–°è¯æ®ã€‚"
                f"\n\nå½“å‰è®°å¿†æ‘˜è¦ï¼š{_summarizer.summarize(summary)}"
                + (f"\n\nä»Šæ—¥ç®€æŠ¥ï¼š{brief_summary}" if brief_summary else "")
                + todo_line
                + "\n\nå»ºè®®ï¼šæ‰©å¤§è¿½è¸ªè¾¹ç•Œæˆ–å…ˆè§¦å‘ä¸€æ¬¡åŒæ­¥ã€‚"
                + image_tip
            )
        return (
            "å½“å‰è¿˜æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡å·è¯æ®ã€‚å…ˆè¿æ¥æ•°æ®æºå¹¶åŒæ­¥åï¼Œæˆ‘å°±èƒ½ç»™å‡ºå¯è¿½æº¯ç»“è®ºã€‚"
            + (f"\n\nä»Šæ—¥ç®€æŠ¥ï¼š{brief_summary}" if brief_summary else "")
            + todo_line
            + image_tip
        )

    top = citations[0]
    bullets = [
        f"- [{it.source_label}] {it.title}ï¼ˆ{it.sender}ï¼Œ{it.received_at}ï¼‰"
        for it in citations[:4]
    ]
    return (
        f"åŸºäºä½ æœ€è¿‘çš„ä¿¡å·è¯æ®ï¼Œå’Œâ€œ{query.strip()}â€æœ€ç›¸å…³çš„çº¿ç´¢æ˜¯ï¼š\n"
        + "\n".join(bullets)
        + f"\n\nå½“å‰ä¼˜å…ˆå…³æ³¨ï¼š{top.title}"
        + ("\n\næˆ‘ä¹Ÿå‚è€ƒäº†ä½ çš„é•¿æœŸè®°å¿†æ‘˜è¦ã€‚" if summary else "")
        + (f"\n\nä»Šæ—¥ç®€æŠ¥ï¼š{brief_summary}" if brief_summary else "")
        + (
            "\n\nå»ºè®®å…ˆå¤„ç†å¾…è·Ÿè¿›äº‹é¡¹ï¼š\n" + "\n".join(f"- {title}" for title in (todo_titles or [])[:3])
            if todo_titles
            else ""
        )
        + image_tip
    )


def _build_actions(
    query: str,
    citations: list[AelinCitation],
    *,
    has_todos: bool,
    track_suggestion: dict[str, str] | None = None,
) -> list[AelinAction]:
    actions: list[AelinAction] = [
        AelinAction(
            kind="open_desk",
            title="åœ¨ Desk æŸ¥çœ‹å¯è§†åŒ–è¯æ®",
            detail="æ‰“å¼€ /deskï¼Œåœ¨å¡ç‰‡ä¸æ—¶é—´çº¿é‡Œæ ¸éªŒä¸Šä¸‹æ–‡",
            payload={"path": "/desk", "query": query.strip()[:180]},
        ),
    ]
    if citations:
        actions.insert(
            0,
            AelinAction(
                kind="open_message",
                title="æ‰“å¼€æœ€é«˜ç›¸å…³æ¶ˆæ¯",
                detail=f"æŸ¥çœ‹ï¼š{citations[0].title}",
                payload={"message_id": str(citations[0].message_id), "query": query.strip()[:180]},
            ),
        )
    if track_suggestion:
        target = str(track_suggestion.get("target") or "").strip()
        source = str(track_suggestion.get("source") or "auto").strip().lower()
        reason = str(track_suggestion.get("reason") or "").strip()
        if target:
            actions.append(
                AelinAction(
                    kind="confirm_track",
                    title=f"è·Ÿè¸ª {target} çš„åç»­åŠ¨æ€ï¼Ÿ",
                    detail=reason or "Aelin åˆ¤æ–­è¿™å¯èƒ½å€¼å¾—æŒç»­è·Ÿè¸ªã€‚",
                    payload={
                        "target": target[:240],
                        "source": source[:32] or "auto",
                        "query": query.strip()[:500],
                    },
                ),
            )
    if "è¿½è¸ª" not in query and "follow" not in query.lower():
        actions.append(
            AelinAction(
                kind="track_topic",
                title="æŒç»­è¿½è¸ªè¯¥ä¸»é¢˜",
                detail="å°†å½“å‰é—®é¢˜åŠ å…¥é•¿æœŸè¿½è¸ªè¾¹ç•Œ",
                payload={"query": query.strip()},
            )
        )
    if has_todos:
        actions.append(
            AelinAction(
                kind="open_todos",
                title="æŸ¥çœ‹å¾…åŠè·Ÿè¿›",
                detail="åœ¨ Desk çš„ Agent é¢æ¿é‡Œå¤„ç†å¾…åŠ",
                payload={"path": "/desk", "query": query.strip()[:180]},
            )
        )
    return actions[:4]


def _normalize_images(raw_images: list[Any]) -> list[dict[str, str]]:
    out: list[dict[str, str]] = []
    for item in raw_images[:4]:
        data_url = str(getattr(item, "data_url", "") or "").strip()
        name = str(getattr(item, "name", "") or "").strip()[:120]
        if not data_url.startswith("data:image/"):
            continue
        if ";base64," not in data_url:
            continue
        if len(data_url) > 3_000_000:
            continue
        out.append({"data_url": data_url, "name": name})
    return out


def _normalize_history(raw_turns: list[Any]) -> list[dict[str, str]]:
    out: list[dict[str, str]] = []
    for item in raw_turns[-12:]:
        role = str(getattr(item, "role", "") or "").strip().lower()
        content = str(getattr(item, "content", "") or "").strip()
        if role not in {"user", "assistant"}:
            continue
        if not content:
            continue
        out.append({"role": role, "content": content[:3000]})
    return out


def _parse_json_object(raw: str) -> dict[str, Any] | None:
    text = (raw or "").strip()
    if not text:
        return None
    try:
        parsed = json.loads(text)
        return parsed if isinstance(parsed, dict) else None
    except Exception:
        pass

    # Accept common fenced format: ```json { ... } ```
    match = re.search(r"\{[\s\S]*\}", text)
    if not match:
        return None
    try:
        parsed = json.loads(match.group(0))
        return parsed if isinstance(parsed, dict) else None
    except Exception:
        return None


def _parse_json_payload(raw: str) -> Any | None:
    text = (raw or "").strip()
    if not text:
        return None
    try:
        return json.loads(text)
    except Exception:
        pass

    # Accept fenced JSON payloads and both object/array roots.
    for pattern in (r"\{[\s\S]*\}", r"\[[\s\S]*\]"):
        match = re.search(pattern, text)
        if not match:
            continue
        try:
            return json.loads(match.group(0))
        except Exception:
            continue
    return None


def _normalize_track_source(raw: str) -> str:
    src = (raw or "").strip().lower()
    alias = {
        "mail": "email",
        "imap": "email",
        "twitter": "x",
        "xhs": "xiaohongshu",
        "bç«™": "bilibili",
    }
    src = alias.get(src, src)
    if src in _TRACKABLE_SOURCES:
        return src
    return "auto"


def _normalize_web_queries(query: str, items: Any, *, limit: int = _MAX_WEB_SUBAGENTS) -> list[str]:
    safe_limit = max(1, min(_MAX_WEB_SUBAGENTS, int(limit or _MAX_WEB_SUBAGENTS)))
    out: list[str] = []
    seen: set[str] = set()
    seen_sig: set[str] = set()

    def _query_sig(text: str) -> str:
        base = str(text or "").strip().lower()
        if not base:
            return ""
        normalized = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9]+", " ", base)
        for phrase in (
            "latest",
            "recent",
            "today",
            "yesterday",
            "now",
            "current",
            "\u6700\u65b0",  # æœ€æ–°
            "\u6700\u8fd1",  # æœ€è¿‘
            "\u4eca\u5929",  # ä»Šå¤©
            "\u6628\u5929",  # æ˜¨å¤©
            "\u524d\u5929",  # å‰å¤©
            "\u521a\u521a",  # åˆšåˆš
            "\u5b9e\u65f6",  # å®æ—¶
            "\u76ee\u524d",  # ç›®å‰
            "\u6709\u4ec0\u4e48",  # æœ‰ä»€ä¹ˆ
            "\u6709\u54ea\u4e9b",  # æœ‰å“ªäº›
            "\u6709\u5565",  # æœ‰å•¥
            "\u6709\u6ca1\u6709",  # æœ‰æ²¡æœ‰
            "\u8bf7\u95ee",  # è¯·é—®
            "\u5e2e\u6211",  # å¸®æˆ‘
            "\u544a\u8bc9\u6211",  # å‘Šè¯‰æˆ‘
        ):
            normalized = normalized.replace(phrase, " ")
        normalized = re.sub(r"\s+", " ", normalized).strip()
        normalized = re.sub(r"[\u6709\u662f\u4e86\u5417\u5462\u5427\u5440\u554a\u4e48\u561b]+$", "", normalized).strip()
        return normalized or base

    if isinstance(items, list):
        for it in items:
            text = str(it or "").strip()[:180]
            if not text:
                continue
            key = text.lower()
            if key in seen:
                continue
            sig = _query_sig(text)
            if sig and sig in seen_sig:
                continue
            seen.add(key)
            if sig:
                seen_sig.add(sig)
            out.append(text)
            if len(out) >= safe_limit:
                break
    if not out and query.strip():
        out.append(query.strip()[:180])
    return out[:safe_limit]


def _is_cjk_text(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", text or ""))


def _extract_search_subject_dynamic(query: str) -> str:
    text = (query or "").strip()
    if not text:
        return ""

    cleaned = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9]+", " ", text)
    lowered = cleaned.lower()
    stop_phrases_cjk = [
        "\u6700\u8fd1",
        "\u6700\u65b0",
        "\u4eca\u5929",
        "\u6628\u5929",
        "\u524d\u5929",
        "\u521a\u521a",
        "\u5b9e\u65f6",
        "\u6253\u4e86",
        "\u6253\u4ec0\u4e48",
        "\u8fdb\u884c\u4e86",
        "\u6709\u4ec0\u4e48",
        "\u6709\u54ea\u4e9b",
        "\u6709\u5565",
        "\u6709\u6ca1\u6709",
        "\u6709\u5426",
        "\u4ec0\u4e48",
        "\u54ea\u4e9b",
        "\u51e0\u573a",
        "\u6bd4\u8d5b",
        "\u8d5b\u679c",
        "\u6bd4\u5206",
        "\u7ed3\u679c",
        "\u60c5\u51b5",
        "\u662f\u591a\u5c11",
        "\u591a\u5c11",
        "\u544a\u8bc9\u6211",
        "\u5e2e\u6211",
        "\u4e00\u4e0b",
        "\u8bf7\u95ee",
        "\u600e\u4e48",
        "\u5982\u4f55",
    ]
    stop_phrases_en = [
        "who won",
        "what",
        "latest",
        "recent",
        "today",
        "yesterday",
        "result",
        "results",
        "score",
        "scores",
        "game",
        "games",
        "match",
        "matches",
    ]
    subject = lowered
    for phrase in stop_phrases_cjk:
        subject = subject.replace(phrase, " ")
    for phrase in stop_phrases_en:
        subject = re.sub(rf"\b{re.escape(phrase)}\b", " ", subject)
    subject = re.sub(r"\s+", " ", subject).strip()
    # Drop dangling one-letter latin leftovers such as the trailing "s" from "games".
    subject = " ".join(token for token in subject.split(" ") if (len(token) > 1 or bool(re.search(r"[\u4e00-\u9fff]", token))))
    subject = re.sub(r"[\u6709\u662f\u4e86\u5417\u5462\u5427\u5440\u554a\u4e48\u561b]+$", "", subject).strip()
    if len(subject) >= 2:
        return subject[:90]

    leagues = re.findall(r"\b(?:nba|wnba|cba|nfl|nhl|mlb|epl)\b", lowered, flags=re.I)
    if leagues:
        uniq: list[str] = []
        seen: set[str] = set()
        for row in leagues:
            key = row.lower()
            if key in seen:
                continue
            seen.add(key)
            uniq.append(row.upper())
        return " ".join(uniq)[:90]

    tokens = re.findall(r"[A-Za-z0-9]{2,}|[\u4e00-\u9fff]{2,}", cleaned)
    if tokens:
        return " ".join(tokens[:4])[:90]
    return text[:90]


def _build_web_query_pack_dynamic(
    *,
    query: str,
    base_queries: list[str] | None,
    intent_contract: dict[str, Any] | None,
    tracking_snapshot: dict[str, Any] | None = None,
    limit: int = _MAX_WEB_SUBAGENTS,
) -> list[str]:
    query_text = (query or "").strip()
    if not query_text:
        return []

    is_cjk = _is_cjk_text(query_text)
    contract = intent_contract if isinstance(intent_contract, dict) else {}
    tracking = tracking_snapshot if isinstance(tracking_snapshot, dict) else {}

    time_scope = str(contract.get("time_scope") or "").strip().lower()
    sports_intent = bool(contract.get("sports_result_intent")) or _is_sports_result_query(query_text)
    requires_citations = bool(contract.get("requires_citations"))
    freshness_hours = max(1, min(720, _safe_int(contract.get("freshness_hours"), 72)))
    time_sensitive = time_scope in {"today", "recent", "realtime"} or _is_time_sensitive_query(query_text)

    now = datetime.now(timezone.utc)
    today = now.strftime("%Y-%m-%d")
    yesterday = (now - timedelta(days=1)).strftime("%Y-%m-%d")
    subject = _extract_search_subject_dynamic(query_text) or query_text
    focused = subject if len(subject) >= 2 else query_text

    seeds: list[str] = []
    if focused and focused != query_text:
        seeds.append(focused[:180])

    # Put one recency-aware facet early so it survives top-k truncation.
    if time_sensitive:
        if is_cjk:
            seeds.extend(
                [
                    f"{focused} \u4eca\u5929",
                    f"{focused} {today}",
                ]
            )
        else:
            seeds.extend(
                [
                    f"{focused} today",
                    f"{focused} {today}",
                    f"{focused} latest",
                ]
            )

    if sports_intent:
        if is_cjk:
            seeds.extend(
                [
                    f"{focused} \u6bd4\u8d5b\u7ed3\u679c",
                    f"{focused} \u8d5b\u7a0b",
                    f"{focused} \u6218\u62a5",
                    f"{focused} \u5b98\u65b9 \u8d5b\u7a0b",
                    f"{focused} box score",
                    f"{focused} game recap",
                    f"{focused} {today} \u6bd4\u8d5b\u7ed3\u679c",
                ]
            )
        else:
            seeds.extend(
                [
                    f"{focused} match result",
                    f"{focused} fixtures",
                    f"{focused} recap",
                    f"{focused} official schedule",
                    f"{focused} box score",
                    f"{focused} game recap",
                    f"{focused} {today} result",
                ]
            )

    if time_sensitive:
        if is_cjk:
            seeds.extend(
                [
                    f"{focused} \u6700\u65b0",
                    f"{focused} \u4eca\u5929",
                    f"{focused} {today}",
                    f"{focused} {yesterday}",
                ]
            )
            if freshness_hours <= 48:
                seeds.append(f"{focused} \u6700\u8fd124\u5c0f\u65f6")
        else:
            seeds.extend(
                [
                    f"{focused} latest",
                    f"{focused} today",
                    f"{focused} {today}",
                    f"{focused} {yesterday}",
                ]
            )
            if freshness_hours <= 48:
                seeds.append(f"{focused} last 24 hours")

    if requires_citations:
        if is_cjk:
            seeds.extend(
                [
                    f"{focused} \u5b98\u65b9",
                    f"{focused} \u6570\u636e",
                    f"{focused} \u6765\u6e90",
                ]
            )
        else:
            seeds.extend([f"{focused} official", f"{focused} data", f"{focused} source"])

    matched_items = tracking.get("matched_items") if isinstance(tracking.get("matched_items"), list) else []
    for row in matched_items[:2]:
        target = str(row.get("target") or row.get("query") or "").strip()[:140]
        if not target:
            continue
        if is_cjk:
            seeds.append(f"{target} \u6700\u65b0")
        else:
            seeds.append(f"{target} latest")

    if isinstance(base_queries, list):
        seeds.extend(str(it or "").strip()[:180] for it in base_queries if str(it or "").strip())
    seeds.append(query_text[:180])

    return _normalize_web_queries(query_text, seeds, limit=limit)


def _decompose_web_context_boundaries_dynamic(
    *,
    query: str,
    web_boundaries: list[dict[str, str]],
    intent_contract: dict[str, Any] | None,
    tracking_snapshot: dict[str, Any] | None,
    service: LLMService,
    provider: str,
) -> dict[str, Any]:
    query_text = (query or "").strip()
    contract = intent_contract if isinstance(intent_contract, dict) else {}
    tracking = tracking_snapshot if isinstance(tracking_snapshot, dict) else {}
    base_queries = [str(it.get("query") or "").strip() for it in web_boundaries if str(it.get("query") or "").strip()]

    fallback_queries = _build_web_query_pack_dynamic(
        query=query_text,
        base_queries=base_queries or [query_text],
        intent_contract=contract,
        tracking_snapshot=tracking,
        limit=_MAX_WEB_SUBAGENTS,
    )
    scope_map = {
        str(it.get("query") or "").strip().lower(): str(it.get("scope") or "").strip()
        for it in web_boundaries
        if str(it.get("query") or "").strip()
    }
    fallback_boundaries = [
        {"kind": "web", "query": q, "scope": (scope_map.get(q.lower()) or q)[:120]}
        for q in fallback_queries
    ]

    if provider == "rule_based" or not service.is_configured():
        return {
            "source": "fallback",
            "reason": "decomposer_unavailable",
            "boundaries": fallback_boundaries,
        }

    now_utc = datetime.now(timezone.utc).isoformat()
    prompt = (
        "You are Aelin Query Decomposer Agent.\n"
        "Dynamically create temporary web-search subagents (facets) for this request.\n"
        "Return strict JSON only with schema:\n"
        "{"
        "\"facets\": [{\"scope\": string, \"query\": string, \"priority\": number, \"why\": string}],"
        "\"reason\": string"
        "}\n"
        "Rules:\n"
        "- Create 3 to 5 facets when possible.\n"
        "- Queries must be short search-ready strings.\n"
        "- Avoid near-duplicate paraphrases.\n"
        "- Cover direct answer + verification + authoritative source.\n"
        "- If time-sensitive, include explicit date/recency facets.\n"
    )
    user_msg = (
        f"user_query: {query_text}\n"
        f"intent_contract: {json.dumps(contract, ensure_ascii=False, separators=(',', ':'))[:1200]}\n"
        f"existing_web_queries: {json.dumps(base_queries, ensure_ascii=False, separators=(',', ':'))[:600]}\n"
        f"matched_tracking_count: {_safe_int(tracking.get('matched_count'), 0)}\n"
        f"current_utc: {now_utc}\n"
        "Return JSON only."
    )

    parsed_payload: Any | None = None
    retry_used = False
    try:
        raw = service._chat(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=420,
            stream=False,
        )
        parsed_payload = _parse_json_payload(str(raw or ""))
    except Exception:
        parsed_payload = None

    if parsed_payload is None:
        retry_used = True
        retry_prompt = (
            "Return JSON only. Root can be {\"facets\": [...], \"reason\": \"...\"} "
            "or a JSON array of facets."
        )
        retry_msg = (
            f"user_query: {query_text}\n"
            f"intent_contract: {json.dumps(contract, ensure_ascii=False, separators=(',', ':'))[:800]}\n"
            f"fallback_candidates: {json.dumps(fallback_queries, ensure_ascii=False, separators=(',', ':'))[:600]}\n"
            "Generate 3-5 orthogonal facets and return JSON only."
        )
        try:
            raw_retry = service._chat(
                messages=[
                    {"role": "system", "content": retry_prompt},
                    {"role": "user", "content": retry_msg},
                ],
                max_tokens=320,
                stream=False,
            )
            parsed_payload = _parse_json_payload(str(raw_retry or ""))
        except Exception:
            parsed_payload = None

    if parsed_payload is None:
        return {
            "source": "fallback",
            "reason": "decomposer_invalid_json_retry_failed",
            "boundaries": fallback_boundaries,
        }

    parsed_reason = "decomposer_llm"
    if isinstance(parsed_payload, dict):
        parsed_reason = str(parsed_payload.get("reason") or "").strip()[:180] or parsed_reason

    raw_facets: Any = None
    if isinstance(parsed_payload, dict):
        raw_facets = (
            parsed_payload.get("facets")
            or parsed_payload.get("queries")
            or parsed_payload.get("boundaries")
            or parsed_payload.get("tasks")
        )
    elif isinstance(parsed_payload, list):
        raw_facets = parsed_payload

    if not isinstance(raw_facets, list):
        return {
            "source": "fallback",
            "reason": "decomposer_no_facets",
            "boundaries": fallback_boundaries,
        }

    normalized: list[tuple[int, dict[str, str]]] = []
    seen: set[str] = set()
    seen_sig: set[str] = set()

    def _facet_sig(text: str) -> str:
        base = str(text or "").strip().lower()
        if not base:
            return ""
        normalized_text = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9]+", " ", base)
        for phrase in (
            "latest",
            "recent",
            "today",
            "yesterday",
            "now",
            "current",
            "\u6700\u65b0",
            "\u6700\u8fd1",
            "\u4eca\u5929",
            "\u6628\u5929",
            "\u524d\u5929",
            "\u5b9e\u65f6",
            "\u521a\u521a",
            "\u6709\u4ec0\u4e48",
            "\u6709\u54ea\u4e9b",
            "\u6709\u5565",
            "\u6709\u6ca1\u6709",
            "\u8bf7\u95ee",
            "\u5e2e\u6211",
        ):
            normalized_text = normalized_text.replace(phrase, " ")
        normalized_text = re.sub(r"\s+", " ", normalized_text).strip()
        normalized_text = re.sub(r"[\u6709\u662f\u4e86\u5417\u5462\u5427\u5440\u554a\u4e48\u561b]+$", "", normalized_text).strip()
        return normalized_text or base

    for idx, row in enumerate(raw_facets):
        if isinstance(row, str):
            q = str(row or "").strip()[:180]
            scope = q[:120]
            priority = idx + 1
        elif isinstance(row, dict):
            q = str(
                row.get("query")
                or row.get("search_query")
                or row.get("q")
                or row.get("task")
                or ""
            ).strip()[:180]
            scope = str(
                row.get("scope")
                or row.get("facet")
                or row.get("goal")
                or row.get("why")
                or q
            ).strip()[:120]
            priority = max(1, min(9, _safe_int(row.get("priority"), idx + 1)))
        else:
            continue
        if not q:
            continue
        key = q.lower()
        if key in seen:
            continue
        sig = _facet_sig(q)
        if sig and sig in seen_sig:
            continue
        seen.add(key)
        if sig:
            seen_sig.add(sig)
        normalized.append((priority, {"kind": "web", "query": q, "scope": scope or q[:120]}))
        if len(normalized) >= _MAX_WEB_SUBAGENTS:
            break

    if not normalized:
        return {
            "source": "fallback",
            "reason": "decomposer_empty",
            "boundaries": fallback_boundaries,
        }

    normalized.sort(key=lambda it: it[0])
    boundaries = [row for _, row in normalized][:_MAX_WEB_SUBAGENTS]
    reason = parsed_reason
    if retry_used:
        reason = f"{reason};retry=1"
    return {
        "source": "llm",
        "reason": reason,
        "boundaries": boundaries,
    }


def _extract_search_subject(query: str) -> str:
    return _extract_search_subject_dynamic(query)


def _extract_search_subject_legacy(query: str) -> str:
    text = (query or "").strip()
    if not text:
        return ""
    cleaned = re.sub(r"[?ï¼Ÿ!ï¼,ï¼Œã€‚;ï¼›:ï¼š()ï¼ˆï¼‰ã€ã€‘\\[\\]\"'`]+", " ", text)
    lowered = cleaned.lower()
    stop_phrases = [
        "æœ€è¿‘",
        "æœ€æ–°",
        "ä»Šå¤©",
        "æ˜¨æ—¥",
        "æ˜¨å¤©",
        "å‰å¤©",
        "åˆšåˆš",
        "å®æ—¶",
        "æ‰“äº†",
        "è¿›è¡Œäº†",
        "ä»€ä¹ˆ",
        "å“ªäº›",
        "å‡ åœº",
        "æ¯”èµ›",
        "èµ›æœ",
        "æ¯”åˆ†",
        "ç»“æœ",
        "æƒ…å†µ",
        "æ˜¯å¤šå°‘",
        "å¤šå°‘",
        "å‘Šè¯‰æˆ‘",
        "å¸®æˆ‘",
        "ä¸€ä¸‹",
        "è¯·é—®",
        "æœ‰æ²¡æœ‰",
        "æ€ä¹ˆ",
        "å¦‚ä½•",
        "who won",
        "what",
        "latest",
        "recent",
        "today",
        "yesterday",
        "result",
        "results",
        "score",
        "scores",
        "game",
        "games",
        "match",
        "matches",
    ]
    subject = lowered
    for phrase in stop_phrases:
        subject = subject.replace(phrase, " ")
    subject = re.sub(r"\s+", " ", subject).strip()
    if len(subject) >= 2:
        return subject[:90]

    leagues = re.findall(r"\b(?:nba|wnba|cba|nfl|nhl|mlb|epl)\b", lowered, flags=re.I)
    if leagues:
        uniq: list[str] = []
        seen: set[str] = set()
        for row in leagues:
            key = row.lower()
            if key in seen:
                continue
            seen.add(key)
            uniq.append(row.upper())
        return " ".join(uniq)[:90]

    tokens = re.findall(r"[A-Za-z0-9]{2,}|[\u4e00-\u9fff]{2,}", cleaned)
    if tokens:
        return " ".join(tokens[:4])[:90]
    return text[:90]


def _build_web_query_pack(
    *,
    query: str,
    base_queries: list[str] | None,
    intent_contract: dict[str, Any] | None,
    tracking_snapshot: dict[str, Any] | None = None,
    limit: int = _MAX_WEB_SUBAGENTS,
) -> list[str]:
    return _build_web_query_pack_dynamic(
        query=query,
        base_queries=base_queries,
        intent_contract=intent_contract,
        tracking_snapshot=tracking_snapshot,
        limit=limit,
    )


def _build_web_query_pack_legacy(
    *,
    query: str,
    base_queries: list[str] | None,
    intent_contract: dict[str, Any] | None,
    tracking_snapshot: dict[str, Any] | None = None,
    limit: int = _MAX_WEB_SUBAGENTS,
) -> list[str]:
    query_text = (query or "").strip()
    if not query_text:
        return []

    is_cjk = _is_cjk_text(query_text)
    contract = intent_contract if isinstance(intent_contract, dict) else {}
    tracking = tracking_snapshot if isinstance(tracking_snapshot, dict) else {}

    time_scope = str(contract.get("time_scope") or "").strip().lower()
    sports_intent = bool(contract.get("sports_result_intent")) or _is_sports_result_query(query_text)
    requires_citations = bool(contract.get("requires_citations"))
    freshness_hours = max(1, min(720, _safe_int(contract.get("freshness_hours"), 72)))
    time_sensitive = time_scope in {"today", "recent", "realtime"} or _is_time_sensitive_query(query_text)

    now = datetime.now(timezone.utc)
    today = now.strftime("%Y-%m-%d")
    yesterday = (now - timedelta(days=1)).strftime("%Y-%m-%d")
    subject = _extract_search_subject(query_text) or query_text
    focused = subject if len(subject) >= 2 else query_text

    seeds: list[str] = []
    if focused and focused != query_text:
        seeds.append(focused[:180])

    if sports_intent:
        if is_cjk:
            seeds.extend(
                [
                    f"{focused} æœ€æ–° æ¯”åˆ†",
                    f"{focused} æ¯”åˆ†",
                    f"{focused} èµ›æœ",
                    f"{focused} box score",
                    f"{focused} game recap",
                    f"{focused} {today} æ¯”åˆ†",
                ]
            )
        else:
            seeds.extend(
                [
                    f"{focused} latest score",
                    f"{focused} score",
                    f"{focused} result",
                    f"{focused} box score",
                    f"{focused} game recap",
                    f"{focused} {today} score",
                ]
            )

    if time_sensitive:
        if is_cjk:
            seeds.extend(
                [
                    f"{focused} æœ€æ–°",
                    f"{focused} ä»Šå¤©",
                    f"{focused} {today}",
                    f"{focused} {yesterday}",
                ]
            )
            if freshness_hours <= 48:
                seeds.append(f"{focused} æœ€è¿‘24å°æ—¶")
        else:
            seeds.extend(
                [
                    f"{focused} latest",
                    f"{focused} today",
                    f"{focused} {today}",
                    f"{focused} {yesterday}",
                ]
            )
            if freshness_hours <= 48:
                seeds.append(f"{focused} last 24 hours")

    if requires_citations:
        if is_cjk:
            seeds.extend([f"{focused} å®˜æ–¹", f"{focused} æ•°æ®", f"{focused} æ¥æº"])
        else:
            seeds.extend([f"{focused} official", f"{focused} data", f"{focused} source"])

    matched_items = tracking.get("matched_items") if isinstance(tracking.get("matched_items"), list) else []
    for row in matched_items[:2]:
        target = str(row.get("target") or row.get("query") or "").strip()[:140]
        if not target:
            continue
        if is_cjk:
            seeds.append(f"{target} æœ€æ–°")
        else:
            seeds.append(f"{target} latest")

    if isinstance(base_queries, list):
        seeds.extend(str(it or "").strip()[:180] for it in base_queries if str(it or "").strip())
    seeds.append(query_text[:180])

    return _normalize_web_queries(query_text, seeds, limit=limit)


def _decompose_web_context_boundaries(
    *,
    query: str,
    web_boundaries: list[dict[str, str]],
    intent_contract: dict[str, Any] | None,
    tracking_snapshot: dict[str, Any] | None,
    service: LLMService,
    provider: str,
) -> dict[str, Any]:
    return _decompose_web_context_boundaries_dynamic(
        query=query,
        web_boundaries=web_boundaries,
        intent_contract=intent_contract,
        tracking_snapshot=tracking_snapshot,
        service=service,
        provider=provider,
    )


def _decompose_web_context_boundaries_legacy(
    *,
    query: str,
    web_boundaries: list[dict[str, str]],
    intent_contract: dict[str, Any] | None,
    tracking_snapshot: dict[str, Any] | None,
    service: LLMService,
    provider: str,
) -> dict[str, Any]:
    query_text = (query or "").strip()
    contract = intent_contract if isinstance(intent_contract, dict) else {}
    tracking = tracking_snapshot if isinstance(tracking_snapshot, dict) else {}
    base_queries = [str(it.get("query") or "").strip() for it in web_boundaries if str(it.get("query") or "").strip()]

    fallback_queries = _build_web_query_pack(
        query=query_text,
        base_queries=base_queries or [query_text],
        intent_contract=contract,
        tracking_snapshot=tracking,
        limit=_MAX_WEB_SUBAGENTS,
    )
    scope_map = {
        str(it.get("query") or "").strip().lower(): str(it.get("scope") or "").strip()
        for it in web_boundaries
        if str(it.get("query") or "").strip()
    }
    fallback_boundaries = [
        {"kind": "web", "query": q, "scope": (scope_map.get(q.lower()) or q)[:120]}
        for q in fallback_queries
    ]

    if provider == "rule_based" or not service.is_configured():
        return {
            "source": "fallback",
            "reason": "decomposer_unavailable",
            "boundaries": fallback_boundaries,
        }

    now_utc = datetime.now(timezone.utc).isoformat()
    prompt = (
        "You are Aelin Query Decomposer.\n"
        "Decompose one user retrieval request into multiple orthogonal web-search facets.\n"
        "Return strict JSON only with schema:\n"
        "{"
        "\"facets\": [{\"scope\": string, \"query\": string, \"priority\": number, \"why\": string}],"
        "\"reason\": string"
        "}\n"
        "Rules:\n"
        "- Create 3 to 5 facets when possible.\n"
        "- Queries must be short search-ready strings, not one long user sentence.\n"
        "- Avoid near-duplicate paraphrases.\n"
        "- Cover direct answer facet + verification facet + authoritative source facet.\n"
        "- If time-sensitive, include explicit date/recency angle.\n"
    )
    user_msg = (
        f"user_query: {query_text}\n"
        f"intent_contract: {json.dumps(contract, ensure_ascii=False, separators=(',', ':'))[:1200]}\n"
        f"existing_web_queries: {json.dumps(base_queries, ensure_ascii=False, separators=(',', ':'))[:600]}\n"
        f"matched_tracking_count: {_safe_int(tracking.get('matched_count'), 0)}\n"
        f"current_utc: {now_utc}\n"
        "Return JSON only."
    )

    try:
        raw = service._chat(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=420,
            stream=False,
        )
        parsed = _parse_json_object(str(raw or ""))
    except Exception:
        parsed = None

    if not isinstance(parsed, dict):
        return {
            "source": "fallback",
            "reason": "decomposer_invalid_json",
            "boundaries": fallback_boundaries,
        }

    raw_facets = parsed.get("facets")
    if not isinstance(raw_facets, list):
        return {
            "source": "fallback",
            "reason": "decomposer_no_facets",
            "boundaries": fallback_boundaries,
        }

    normalized: list[tuple[int, dict[str, str]]] = []
    seen: set[str] = set()
    for idx, row in enumerate(raw_facets):
        if isinstance(row, str):
            q = str(row or "").strip()[:180]
            scope = q[:120]
            priority = idx + 1
        elif isinstance(row, dict):
            q = str(
                row.get("query")
                or row.get("search_query")
                or row.get("q")
                or row.get("task")
                or ""
            ).strip()[:180]
            scope = str(
                row.get("scope")
                or row.get("facet")
                or row.get("goal")
                or row.get("why")
                or q
            ).strip()[:120]
            priority = max(1, min(9, _safe_int(row.get("priority"), idx + 1)))
        else:
            continue
        if not q:
            continue
        key = q.lower()
        if key in seen:
            continue
        seen.add(key)
        normalized.append((priority, {"kind": "web", "query": q, "scope": scope or q[:120]}))
        if len(normalized) >= _MAX_WEB_SUBAGENTS:
            break

    if not normalized:
        return {
            "source": "fallback",
            "reason": "decomposer_empty",
            "boundaries": fallback_boundaries,
        }

    normalized.sort(key=lambda it: it[0])
    boundaries = [row for _, row in normalized][:_MAX_WEB_SUBAGENTS]

    if len(boundaries) > 1:
        direct_idx = next(
            (i for i, row in enumerate(boundaries) if str(row.get("query") or "").strip().lower() == query_text.lower()),
            -1,
        )
        if direct_idx > 0:
            direct = boundaries.pop(direct_idx)
            boundaries.append(direct)

    reason = str(parsed.get("reason") or "").strip()[:180] or "decomposer_llm"
    return {
        "source": "llm",
        "reason": reason,
        "boundaries": boundaries,
    }


def _normalize_context_boundaries(
    query: str,
    raw_boundaries: Any,
    *,
    need_local_search: bool,
    need_web_search: bool,
    web_queries: list[str],
) -> list[dict[str, str]]:
    out: list[dict[str, str]] = []
    seen: set[tuple[str, str]] = set()

    def count_kind(kind: str) -> int:
        return sum(1 for it in out if it["kind"] == kind)

    def push(kind: str, q: str, scope: str = "") -> None:
        k = (kind or "").strip().lower()
        if k not in {"local", "web"}:
            return
        if k == "local" and count_kind("local") >= _MAX_LOCAL_SUBAGENTS:
            return
        if k == "web" and count_kind("web") >= _MAX_WEB_SUBAGENTS:
            return
        text = (q or "").strip()[:180]
        if not text:
            return
        key = (k, text.lower())
        if key in seen:
            return
        seen.add(key)
        out.append({"kind": k, "query": text, "scope": (scope or text).strip()[:120]})

    if isinstance(raw_boundaries, list):
        for row in raw_boundaries:
            if len(out) >= _MAX_CONTEXT_BOUNDARIES:
                break
            if isinstance(row, str):
                push("web", row, row)
                continue
            if not isinstance(row, dict):
                continue
            kind = str(row.get("kind") or row.get("type") or row.get("source") or "").strip().lower()
            query_text = str(
                row.get("query") or row.get("facet") or row.get("task") or row.get("goal") or ""
            ).strip()
            scope = str(row.get("scope") or row.get("label") or "").strip()
            if kind in {"local_search", "local"}:
                push("local", query_text or query, scope)
            elif kind in {"web_search", "web"}:
                push("web", query_text or query, scope)

    if need_local_search and not any(it["kind"] == "local" for it in out):
        push("local", query, "local context")
    if need_web_search and not any(it["kind"] == "web" for it in out):
        for q in (web_queries or [query]):
            if len(out) >= _MAX_CONTEXT_BOUNDARIES:
                break
            push("web", q, q)

    out.sort(key=lambda x: 0 if x["kind"] == "local" else 1)
    return out[:_MAX_CONTEXT_BOUNDARIES]


def _build_trace_context_boundaries(
    *,
    query: str,
    raw_boundaries: Any,
    need_local_search: bool,
    need_web_search: bool,
    web_queries: list[str],
    intent_contract: dict[str, Any] | None,
    tracking_snapshot: dict[str, Any] | None,
    max_local: int = 2,
    max_web: int = 3,
) -> list[dict[str, str]]:
    local_cap = max(0, min(_MAX_LOCAL_SUBAGENTS, int(max_local or 2)))
    web_cap = max(0, min(_MAX_WEB_SUBAGENTS, int(max_web or 3)))
    boundaries = _normalize_context_boundaries(
        query,
        raw_boundaries,
        need_local_search=need_local_search,
        need_web_search=need_web_search,
        web_queries=web_queries,
    )
    local = [it for it in boundaries if str(it.get("kind") or "") == "local"][:local_cap]
    web = [it for it in boundaries if str(it.get("kind") or "") == "web"][:web_cap]

    # When trace route is enabled but planner does not provide explicit boundaries,
    # synthesize lightweight web facets so Trace Agent can verify trackability.
    if need_web_search and (not web):
        seeds = _build_web_query_pack(
            query=(query or "").strip(),
            base_queries=web_queries or [(query or "").strip()],
            intent_contract=intent_contract if isinstance(intent_contract, dict) else None,
            tracking_snapshot=tracking_snapshot if isinstance(tracking_snapshot, dict) else None,
            limit=web_cap,
        )
        for q in seeds[:web_cap]:
            web.append({"kind": "web", "query": q[:180], "scope": q[:120]})

    if need_local_search and (not local) and query.strip() and local_cap > 0:
        local.append(
            {
                "kind": "local",
                "query": query.strip()[:180],
                "scope": "trace local context",
            }
        )

    return [*local[:local_cap], *web[:web_cap]][:_MAX_CONTEXT_BOUNDARIES]


def _normalize_search_mode(raw: str) -> str:
    text = (raw or "").strip().lower()
    if text in {"local", "localonly", "local_only", "only_local"}:
        return "local_only"
    if text in {"web", "webonly", "web_only", "only_web", "force_web"}:
        return "web_only"
    return "auto"


def _is_smalltalk_query(query: str) -> bool:
    text = (query or "").strip().lower()
    if not text:
        return True
    signals = [
        "ä½ å¥½",
        "hello",
        "hi ",
        "åœ¨å—",
        "èŠèŠ",
        "ä½ è§‰å¾—",
        "ä½ æ€ä¹ˆçœ‹",
        "å¿ƒæƒ…",
        "ç„¦è™‘",
        "emo",
        "å“ˆå“ˆ",
        "è°¢è°¢",
        "æ™šå®‰",
    ]
    return any(sig in text for sig in signals)


def _normalize_match_text(text: str) -> str:
    return re.sub(r"\s+", "", (text or "").strip().lower())


def _build_planner_tracking_snapshot(db: Session, *, user_id: int, query: str) -> dict[str, Any]:
    try:
        events = _load_tracking_events(db, user_id=user_id, limit=80)
    except Exception:
        return {"active_items": [], "matched_items": [], "active_count": 0, "matched_count": 0}

    active_items = sorted(
        [it for it in (events or {}).values() if str(it.get("target") or "").strip()],
        key=lambda it: str(it.get("updated_at") or ""),
        reverse=True,
    )
    q_norm = _normalize_match_text(query)
    matched_items: list[dict[str, Any]] = []
    if q_norm:
        for it in active_items:
            target_norm = _normalize_match_text(str(it.get("target") or ""))
            if not target_norm:
                continue
            if target_norm in q_norm or q_norm in target_norm:
                matched_items.append(it)
                if len(matched_items) >= 5:
                    break
                continue
            query_norm = _normalize_match_text(str(it.get("query") or ""))
            if query_norm and (query_norm in q_norm or q_norm in query_norm):
                matched_items.append(it)
            if len(matched_items) >= 5:
                break
    return {
        "active_items": active_items[:8],
        "matched_items": matched_items[:5],
        "active_count": len(active_items),
        "matched_count": len(matched_items),
    }


def _safe_int(value: Any, default: int) -> int:
    try:
        return int(float(value))
    except Exception:
        return int(default)


def _safe_float(value: Any, default: float) -> float:
    try:
        return float(value)
    except Exception:
        return float(default)


def _fallback_intent_contract(
    *,
    query: str,
    memory_summary: str,
    tracking_snapshot: dict[str, Any] | None,
    reason: str,
) -> dict[str, Any]:
    query_text = (query or "").strip()
    smalltalk = _is_smalltalk_query(query_text)
    time_sensitive = _is_time_sensitive_query(query_text)
    sports_result_intent = _is_sports_result_query(query_text)
    tracking_intent = _is_tracking_intent_query(query_text)
    matched_count = 0
    if isinstance(tracking_snapshot, dict):
        matched_count = _safe_int(tracking_snapshot.get("matched_count"), 0)

    intent_type = "chat"
    if tracking_intent:
        intent_type = "tracking"
    elif not smalltalk:
        intent_type = "retrieval"
    time_scope = "any"
    if time_sensitive:
        time_scope = "recent"
    if "today" in query_text.lower():
        time_scope = "today"
    freshness_hours = 720
    if time_scope == "today":
        freshness_hours = 24
    elif time_scope == "recent":
        freshness_hours = 72
    if sports_result_intent:
        freshness_hours = min(freshness_hours, 24)

    requires_citations = bool((not smalltalk) and (time_sensitive or sports_result_intent))
    requires_factuality = not smalltalk

    ambiguities: list[str] = []
    if len(query_text) <= 6:
        ambiguities.append("query_too_short")
    if intent_type == "retrieval" and matched_count > 0 and not time_sensitive:
        ambiguities.append("could_use_existing_tracking_only")
    if intent_type == "retrieval" and not (memory_summary or "").strip():
        ambiguities.append("limited_personal_memory_context")

    return {
        "goal": query_text[:240] or "chat",
        "intent_type": intent_type,
        "time_scope": time_scope,
        "freshness_hours": max(1, min(720, int(freshness_hours))),
        "requires_citations": requires_citations,
        "requires_factuality": requires_factuality,
        "sports_result_intent": sports_result_intent,
        "tracking_intent": tracking_intent,
        "ambiguities": ambiguities[:4],
        "confidence": 0.62 if not smalltalk else 0.8,
        "reason": reason[:180],
        "intent_source": "fallback",
    }


def _normalize_intent_contract(
    *,
    raw: dict[str, Any] | None,
    query: str,
    fallback: dict[str, Any],
) -> dict[str, Any]:
    out = dict(fallback)
    if not isinstance(raw, dict):
        return out

    goal = str(raw.get("goal") or "").strip()
    if goal:
        out["goal"] = goal[:240]

    intent_type = str(raw.get("intent_type") or "").strip().lower()
    if intent_type in {"chat", "retrieval", "tracking", "analysis"}:
        out["intent_type"] = intent_type

    time_scope = str(raw.get("time_scope") or "").strip().lower()
    if time_scope in {"any", "today", "recent", "historical", "realtime"}:
        out["time_scope"] = time_scope

    freshness_hours = _safe_int(raw.get("freshness_hours"), _safe_int(out.get("freshness_hours"), 72))
    out["freshness_hours"] = max(1, min(720, freshness_hours))

    if raw.get("requires_citations") is not None:
        out["requires_citations"] = bool(raw.get("requires_citations"))
    if raw.get("requires_factuality") is not None:
        out["requires_factuality"] = bool(raw.get("requires_factuality"))

    out["sports_result_intent"] = bool(raw.get("sports_result_intent")) or _is_sports_result_query(query)
    out["tracking_intent"] = bool(raw.get("tracking_intent")) or _is_tracking_intent_query(query)

    ambiguities = raw.get("ambiguities")
    if isinstance(ambiguities, list):
        normalized_ambiguities: list[str] = []
        for row in ambiguities:
            text = str(row or "").strip()
            if not text:
                continue
            normalized_ambiguities.append(text[:120])
            if len(normalized_ambiguities) >= 4:
                break
        out["ambiguities"] = normalized_ambiguities

    confidence = _safe_float(raw.get("confidence"), _safe_float(out.get("confidence"), 0.62))
    out["confidence"] = max(0.0, min(1.0, confidence))

    reason = str(raw.get("reason") or "").strip()
    if reason:
        out["reason"] = reason[:180]
    return out


def _build_intent_contract(
    *,
    query: str,
    service: LLMService,
    provider: str,
    memory_summary: str,
    tracking_snapshot: dict[str, Any] | None = None,
) -> dict[str, Any]:
    fallback = _fallback_intent_contract(
        query=query,
        memory_summary=memory_summary,
        tracking_snapshot=tracking_snapshot,
        reason="intent_fallback",
    )
    if provider == "rule_based" or not service.is_configured():
        fallback_reason = "intent_planner_unavailable"
        if provider == "rule_based":
            fallback_reason = "intent_rule_based"
        elif not service.is_configured():
            fallback_reason = "intent_not_configured"
        fallback["reason"] = fallback_reason
        return fallback

    tracking = tracking_snapshot if isinstance(tracking_snapshot, dict) else {}
    active_count = _safe_int(tracking.get("active_count"), 0)
    matched_count = _safe_int(tracking.get("matched_count"), 0)
    now_utc = datetime.now(timezone.utc).isoformat()

    prompt = (
        "You are Aelin Intent Lens Agent.\n"
        "Infer user intent with explicit time understanding and factuality requirements.\n"
        "Return strict JSON only with schema:\n"
        "{"
        "\"goal\": string,"
        "\"intent_type\": \"chat|retrieval|tracking|analysis\","
        "\"time_scope\": \"any|today|recent|historical|realtime\","
        "\"freshness_hours\": number,"
        "\"requires_citations\": boolean,"
        "\"requires_factuality\": boolean,"
        "\"sports_result_intent\": boolean,"
        "\"tracking_intent\": boolean,"
        "\"ambiguities\": string[],"
        "\"confidence\": number,"
        "\"reason\": string"
        "}\n"
        "If user uses relative time words like today/recent/latest, convert them into explicit time_scope and freshness."
    )
    user_msg = (
        f"user_query: {query.strip()}\n"
        f"memory_summary_available: {'yes' if bool((memory_summary or '').strip()) else 'no'}\n"
        f"active_tracking_count: {active_count}\n"
        f"matched_tracking_count: {matched_count}\n"
        f"current_utc: {now_utc}\n"
        "Return JSON only."
    )
    try:
        raw = service._chat(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=320,
            stream=False,
        )
        parsed = _parse_json_object(str(raw or ""))
        normalized = _normalize_intent_contract(raw=parsed if isinstance(parsed, dict) else None, query=query, fallback=fallback)
        normalized["intent_source"] = "llm"
        if not isinstance(parsed, dict):
            normalized["reason"] = "intent_invalid_json"
            normalized["intent_source"] = "fallback"
        return normalized
    except Exception:
        fallback["reason"] = "intent_error"
        return fallback


def _plan_tool_usage(
    *,
    query: str,
    service: LLMService,
    provider: str,
    memory_summary: str,
    tracking_snapshot: dict[str, Any] | None = None,
    intent_contract: dict[str, Any] | None = None,
) -> dict[str, Any]:
    def _fallback_plan(reason: str) -> dict[str, Any]:
        contract = intent_contract if isinstance(intent_contract, dict) else {}
        contract_intent_type = str(contract.get("intent_type") or "").strip().lower()
        contract_time_scope = str(contract.get("time_scope") or "").strip().lower()
        contract_requires_citations = bool(contract.get("requires_citations"))
        contract_sports_intent = bool(contract.get("sports_result_intent"))
        contract_tracking_intent = bool(contract.get("tracking_intent"))

        tracking = tracking_snapshot if isinstance(tracking_snapshot, dict) else {}
        active_items = tracking.get("active_items") if isinstance(tracking.get("active_items"), list) else []
        matched_items = tracking.get("matched_items") if isinstance(tracking.get("matched_items"), list) else []

        query_text = (query or "").strip()
        conversational = _is_smalltalk_query(query_text)
        time_sensitive = contract_time_scope in {"today", "recent", "realtime"} or _is_time_sensitive_query(query_text)
        has_memory = bool((memory_summary or "").strip())
        has_tracking_match = bool(matched_items)

        recent_tracking_match = False
        now = datetime.now(timezone.utc)
        for it in matched_items[:5]:
            updated_raw = str(it.get("updated_at") or "").strip()
            if not updated_raw:
                continue
            try:
                updated_at = datetime.fromisoformat(updated_raw.replace("Z", "+00:00"))
            except Exception:
                continue
            if updated_at.tzinfo is None:
                updated_at = updated_at.replace(tzinfo=timezone.utc)
            if (now - updated_at).total_seconds() <= 36 * 3600:
                recent_tracking_match = True
                break

        retrieval_like = bool(query_text) and (not conversational)
        if contract_intent_type == "chat":
            retrieval_like = False
        elif contract_intent_type in {"retrieval", "tracking", "analysis"}:
            retrieval_like = True
        sports_result_intent = bool(contract_sports_intent or _is_sports_result_query(query_text))
        need_local = retrieval_like and (has_memory or has_tracking_match or bool(active_items))
        need_web = False
        if retrieval_like:
            if time_sensitive or sports_result_intent or contract_requires_citations:
                need_web = not recent_tracking_match
            elif (not has_memory) and (not has_tracking_match):
                need_web = True

        web_seed: list[str] = []
        if need_web:
            web_seed.append(query_text)
            if sports_result_intent:
                web_seed.extend(
                    [
                        f"{query_text} \u6700\u65b0 \u6bd4\u5206",
                        f"{query_text} \u8d5b\u679c",
                        f"{query_text} box score",
                        f"{query_text} game recap",
                    ]
                )
            for it in matched_items[:2]:
                target = str(it.get("target") or it.get("query") or "").strip()[:120]
                if target:
                    web_seed.append(f"{target} latest")
        web_queries = _normalize_web_queries(query_text, web_seed, limit=_MAX_WEB_SUBAGENTS) if need_web else []
        context_boundaries = _normalize_context_boundaries(
            query_text,
            [],
            need_local_search=need_local,
            need_web_search=need_web,
            web_queries=web_queries,
        )
        need_local = any(str(it.get("kind") or "") == "local" for it in context_boundaries)
        need_web = any(str(it.get("kind") or "") == "web" for it in context_boundaries)
        web_queries = (
            _normalize_web_queries(
                query_text,
                [it.get("query") for it in context_boundaries if str(it.get("kind") or "") == "web"],
            )
            if need_web
            else []
        )

        trace_agent = bool((contract_tracking_intent or _is_tracking_intent_query(query_text)) and not recent_tracking_match)
        track_suggestion = None
        if trace_agent and query_text:
            track_suggestion = {
                "target": query_text[:240],
                "source": "web" if need_web else "auto",
                "reason": "fallback planner detected potential long-running tracking intent",
            }
        trace_context_boundaries = _build_trace_context_boundaries(
            query=query_text,
            raw_boundaries=[],
            need_local_search=trace_agent and need_local,
            need_web_search=trace_agent and bool(need_web or query_text),
            web_queries=web_queries,
            intent_contract=contract,
            tracking_snapshot=tracking,
        )
        reason_bits = [reason]
        if conversational:
            reason_bits.append("smalltalk")
        if time_sensitive:
            reason_bits.append("time_sensitive")
        if sports_result_intent:
            reason_bits.append("sports_result_intent")
        if recent_tracking_match:
            reason_bits.append("tracking_match_recent")
        elif has_tracking_match:
            reason_bits.append("tracking_match_stale")
        if need_local:
            reason_bits.append("local_context")
        if need_web:
            reason_bits.append("web_context")
        return {
            "need_local_search": need_local,
            "need_web_search": need_web,
            "web_queries": web_queries,
            "context_boundaries": context_boundaries,
            "trace_context_boundaries": trace_context_boundaries,
            "track_suggestion": track_suggestion,
            "route": {
                "reply_agent": True,
                "trace_agent": trace_agent,
                "allow_web_retry": bool(need_web and time_sensitive),
            },
            "reason": ";".join(reason_bits),
            "planner_source": "fallback",
        }

    if provider == "rule_based" or not service.is_configured():
        fallback_reason = "planner_unavailable"
        if provider == "rule_based":
            fallback_reason = "planner_rule_based"
        elif not service.is_configured():
            fallback_reason = "planner_not_configured"
        return _fallback_plan(fallback_reason)

    tracking = tracking_snapshot if isinstance(tracking_snapshot, dict) else {}
    active_items = tracking.get("active_items") if isinstance(tracking.get("active_items"), list) else []
    matched_items = tracking.get("matched_items") if isinstance(tracking.get("matched_items"), list) else []

    planning_prompt = (
        "You are Aelin Main Agent planner.\n"
        "Decide dynamic dispatch by context boundaries.\n"
        "You must obey intent contract constraints from Intent Lens Agent.\n"
        "Do not rely on rigid keyword-only rules; decide from query + memory + tracking context.\n"
        "Both local and web subagents are optional.\n"
        "You may dispatch up to 5 web subagents and up to 5 local subagents in parallel.\n"
        "If existing tracking already covers the asked topic, you may skip web retrieval.\n"
        "Return strict JSON only with schema:\n"
        "{"
        "\"need_local_search\": boolean,"
        "\"need_web_search\": boolean,"
        "\"web_queries\": string[],"
        "\"context_boundaries\": [{\"kind\":\"local|web\",\"query\":\"string\",\"scope\":\"string\"}],"
        "\"trace_context_boundaries\": [{\"kind\":\"local|web\",\"query\":\"string\",\"scope\":\"string\"}],"
        "\"reply_agent\": boolean,"
        "\"trace_agent\": boolean,"
        "\"allow_web_retry\": boolean,"
        "\"should_suggest_tracking\": boolean,"
        "\"tracking_target\": string,"
        "\"tracking_source\": \"auto|web|rss|x|douyin|xiaohongshu|weibo|bilibili|email\","
        "\"tracking_reason\": string,"
        "\"reason\": string"
        "}\n"
        "context_boundaries is the primary dispatch plan.\n"
        "reply_agent defaults to true and can be omitted unless you want it disabled."
    )
    matched_lines = [
        f"- {str(it.get('target') or '').strip()} ({str(it.get('source') or 'auto').strip()} / {str(it.get('updated_at') or '').strip()})"
        for it in matched_items[:5]
        if str(it.get("target") or "").strip()
    ]
    active_lines = [
        f"- {str(it.get('target') or '').strip()} ({str(it.get('source') or 'auto').strip()})"
        for it in active_items[:5]
        if str(it.get("target") or "").strip()
    ]
    user_msg = (
        f"user_query: {query.strip()}\n"
        + (
            f"intent_contract: {json.dumps(intent_contract, ensure_ascii=False, separators=(',', ':'))[:1200]}\n"
            if isinstance(intent_contract, dict)
            else ""
        )
        +
        f"memory_summary_available: {'yes' if bool((memory_summary or '').strip()) else 'no'}\n"
        f"active_tracking_count: {len(active_items)}\n"
        + ("matched_tracking:\n" + "\n".join(matched_lines) + "\n" if matched_lines else "matched_tracking: none\n")
        + ("recent_tracking:\n" + "\n".join(active_lines) + "\n" if active_lines else "recent_tracking: none\n")
        + "Return JSON only."
    )
    try:
        raw = service._chat(
            messages=[
                {"role": "system", "content": planning_prompt},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=420,
            stream=False,
        )
        parsed = _parse_json_object(str(raw or ""))
        if not isinstance(parsed, dict):
            return _fallback_plan("planner_invalid_json")

        need_local_hint = bool(parsed.get("need_local_search"))
        need_web_hint = bool(parsed.get("need_web_search"))
        web_queries = _normalize_web_queries(query, parsed.get("web_queries"))
        context_boundaries = _normalize_context_boundaries(
            query,
            parsed.get("context_boundaries"),
            need_local_search=need_local_hint,
            need_web_search=need_web_hint,
            web_queries=web_queries,
        )
        need_local = any(str(it.get("kind") or "") == "local" for it in context_boundaries)
        need_web = any(str(it.get("kind") or "") == "web" for it in context_boundaries)
        web_queries = _normalize_web_queries(
            query,
            [it.get("query") for it in context_boundaries if str(it.get("kind") or "") == "web"] or web_queries,
        )
        should_track = bool(parsed.get("should_suggest_tracking"))
        track_target = str(parsed.get("tracking_target") or "").strip()[:240]
        track_source = _normalize_track_source(str(parsed.get("tracking_source") or "auto"))
        track_reason = str(parsed.get("tracking_reason") or "").strip()[:220]
        reason = str(parsed.get("reason") or "").strip()[:200] or "llm_planner"
        reply_agent = bool(parsed.get("reply_agent", True))
        trace_agent = bool(parsed.get("trace_agent"))
        allow_web_retry_raw = parsed.get("allow_web_retry")
        allow_web_retry = bool(allow_web_retry_raw) if allow_web_retry_raw is not None else need_web

        track_suggestion = None
        if should_track and track_target:
            track_suggestion = {
                "target": track_target,
                "source": track_source,
                "reason": track_reason or "Aelin åˆ¤æ–­è¯¥ä¸»é¢˜å€¼å¾—æŒç»­è·Ÿè¸ªã€‚",
            }
            trace_agent = True

        trace_context_boundaries = _build_trace_context_boundaries(
            query=query,
            raw_boundaries=parsed.get("trace_context_boundaries"),
            need_local_search=trace_agent and need_local,
            need_web_search=trace_agent and bool(need_web or track_suggestion),
            web_queries=web_queries,
            intent_contract=intent_contract if isinstance(intent_contract, dict) else None,
            tracking_snapshot=tracking_snapshot if isinstance(tracking_snapshot, dict) else None,
        )

        if need_web and not web_queries:
            web_queries = [query.strip()[:180]] if query.strip() else []
        return {
            "need_local_search": need_local,
            "need_web_search": need_web,
            "web_queries": web_queries,
            "context_boundaries": context_boundaries,
            "trace_context_boundaries": trace_context_boundaries,
            "track_suggestion": track_suggestion,
            "route": {
                "reply_agent": reply_agent,
                "trace_agent": trace_agent,
                "allow_web_retry": allow_web_retry,
            },
            "reason": f"llm:{reason}",
            "planner_source": "llm",
        }
    except Exception:
        return _fallback_plan("planner_error")


def _critic_tool_plan(
    *,
    query: str,
    intent_contract: dict[str, Any] | None,
    tool_plan: dict[str, Any],
    service: LLMService,
    provider: str,
) -> dict[str, Any]:
    def _fallback_critic(reason: str) -> dict[str, Any]:
        contract = intent_contract if isinstance(intent_contract, dict) else {}
        requires_citations = bool(contract.get("requires_citations"))
        intent_type = str(contract.get("intent_type") or "").strip().lower()
        sports_result_intent = bool(contract.get("sports_result_intent")) or _is_sports_result_query(query)
        tracking_intent = bool(contract.get("tracking_intent")) or _is_tracking_intent_query(query)

        need_local = bool(tool_plan.get("need_local_search"))
        need_web = bool(tool_plan.get("need_web_search"))
        web_queries = _normalize_web_queries(query, tool_plan.get("web_queries"))
        boundaries = _normalize_context_boundaries(
            query,
            tool_plan.get("context_boundaries"),
            need_local_search=need_local,
            need_web_search=need_web,
            web_queries=web_queries,
        )
        has_local = any(str(it.get("kind") or "") == "local" for it in boundaries)
        has_web = any(str(it.get("kind") or "") == "web" for it in boundaries)
        route = tool_plan.get("route") if isinstance(tool_plan.get("route"), dict) else {}
        issues: list[str] = []
        patch: dict[str, Any] = {}

        retrieval_intent = intent_type in {"retrieval", "tracking", "analysis"} or (not _is_smalltalk_query(query))
        if retrieval_intent and (not has_local) and (not has_web):
            issues.append("no_retrieval_path")
            patch["need_local_search"] = True
            patch["context_boundaries"] = [{"kind": "local", "query": query.strip()[:180], "scope": "critic_local_context"}]

        if (requires_citations or sports_result_intent) and (not has_web):
            issues.append("missing_web_path_for_factual_intent")
            patch["need_web_search"] = True
            patch["web_queries"] = _normalize_web_queries(
                query,
                [
                    query.strip()[:180],
                    f"{query.strip()[:160]} æœ€æ–°",
                    f"{query.strip()[:160]} æ¯”åˆ†" if sports_result_intent else f"{query.strip()[:160]} å®˜æ–¹",
                ],
                limit=_MAX_WEB_SUBAGENTS,
            )
            patch_boundaries = patch.get("context_boundaries")
            if not isinstance(patch_boundaries, list):
                patch_boundaries = list(boundaries)
            patch_boundaries.extend(
                {"kind": "web", "query": q, "scope": q}
                for q in patch.get("web_queries", [])[:2]
            )
            patch["context_boundaries"] = patch_boundaries

        if tracking_intent and (not bool(route.get("trace_agent"))):
            issues.append("missing_trace_route")
            patch["route"] = {
                "reply_agent": bool(route.get("reply_agent", True)),
                "trace_agent": True,
                "allow_web_retry": bool(route.get("allow_web_retry", False) or requires_citations or sports_result_intent),
            }
            patch["trace_context_boundaries"] = _build_trace_context_boundaries(
                query=query,
                raw_boundaries=tool_plan.get("trace_context_boundaries"),
                need_local_search=has_local,
                need_web_search=bool(has_web or patch.get("need_web_search")),
                web_queries=patch.get("web_queries") if isinstance(patch.get("web_queries"), list) else web_queries,
                intent_contract=contract,
                tracking_snapshot=None,
            )

        accepted = not issues
        return {
            "accepted": accepted,
            "issues": issues,
            "patch": patch if patch else None,
            "reason": reason if accepted else f"{reason}:{','.join(issues)}",
            "critic_source": "fallback",
        }

    if provider == "rule_based" or not service.is_configured():
        fallback_reason = "critic_unavailable"
        if provider == "rule_based":
            fallback_reason = "critic_rule_based"
        elif not service.is_configured():
            fallback_reason = "critic_not_configured"
        return _fallback_critic(fallback_reason)

    contract_payload = intent_contract if isinstance(intent_contract, dict) else {}
    prompt = (
        "You are Aelin Plan Critic Agent.\n"
        "Evaluate whether dispatch plan fully covers intent contract.\n"
        "If weak, provide a corrective patch.\n"
        "Return strict JSON only with schema:\n"
        "{"
        "\"accepted\": boolean,"
        "\"issues\": string[],"
        "\"patch\": {"
        "\"need_local_search\": boolean,"
        "\"need_web_search\": boolean,"
        "\"web_queries\": string[],"
        "\"context_boundaries\": [{\"kind\":\"local|web\",\"query\":\"string\",\"scope\":\"string\"}],"
        "\"trace_context_boundaries\": [{\"kind\":\"local|web\",\"query\":\"string\",\"scope\":\"string\"}],"
        "\"route\": {\"reply_agent\": boolean,\"trace_agent\": boolean,\"allow_web_retry\": boolean}"
        "},"
        "\"reason\": string"
        "}\n"
        "Rules:\n"
        "- For time-sensitive factual intents, ensure evidence path exists.\n"
        "- For sports result intents, prefer web path with score/result oriented queries.\n"
        "- Keep patch minimal and deterministic."
    )
    user_msg = (
        f"user_query: {query.strip()}\n"
        f"intent_contract: {json.dumps(contract_payload, ensure_ascii=False, separators=(',', ':'))[:1200]}\n"
        f"tool_plan: {json.dumps(tool_plan, ensure_ascii=False, separators=(',', ':'))[:1800]}\n"
        "Return JSON only."
    )
    try:
        raw = service._chat(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=320,
            stream=False,
        )
        parsed = _parse_json_object(str(raw or ""))
        if not isinstance(parsed, dict):
            return _fallback_critic("critic_invalid_json")
        accepted = bool(parsed.get("accepted"))
        issues_raw = parsed.get("issues")
        issues: list[str] = []
        if isinstance(issues_raw, list):
            for row in issues_raw:
                text = str(row or "").strip()
                if not text:
                    continue
                issues.append(text[:120])
                if len(issues) >= 6:
                    break
        patch_raw = parsed.get("patch")
        patch: dict[str, Any] | None = None
        if isinstance(patch_raw, dict):
            patch = {}
            if patch_raw.get("need_local_search") is not None:
                patch["need_local_search"] = bool(patch_raw.get("need_local_search"))
            if patch_raw.get("need_web_search") is not None:
                patch["need_web_search"] = bool(patch_raw.get("need_web_search"))
            if patch_raw.get("web_queries") is not None:
                patch["web_queries"] = _normalize_web_queries(query, patch_raw.get("web_queries"), limit=_MAX_WEB_SUBAGENTS)
            if isinstance(patch_raw.get("context_boundaries"), list):
                patch["context_boundaries"] = patch_raw.get("context_boundaries")
            if isinstance(patch_raw.get("trace_context_boundaries"), list):
                patch["trace_context_boundaries"] = patch_raw.get("trace_context_boundaries")
            if isinstance(patch_raw.get("route"), dict):
                route_raw = patch_raw.get("route") or {}
                patch["route"] = {
                    "reply_agent": bool(route_raw.get("reply_agent", True)),
                    "trace_agent": bool(route_raw.get("trace_agent", False)),
                    "allow_web_retry": bool(route_raw.get("allow_web_retry", False)),
                }
        reason = str(parsed.get("reason") or "").strip()[:180] or "critic_llm"
        if (not accepted) and (not patch):
            fallback = _fallback_critic(f"critic_patch_missing:{reason}")
            fallback["critic_source"] = "fallback"
            return fallback
        return {
            "accepted": accepted,
            "issues": issues,
            "patch": patch,
            "reason": reason,
            "critic_source": "llm",
        }
    except Exception:
        return _fallback_critic("critic_error")


def _apply_plan_patch(
    *,
    query: str,
    tool_plan: dict[str, Any],
    patch: dict[str, Any],
) -> dict[str, Any]:
    out = dict(tool_plan or {})
    need_local = bool(patch.get("need_local_search", out.get("need_local_search")))
    need_web = bool(patch.get("need_web_search", out.get("need_web_search")))
    web_queries_seed = patch.get("web_queries") if patch.get("web_queries") is not None else out.get("web_queries")
    web_queries = _normalize_web_queries(query, web_queries_seed, limit=_MAX_WEB_SUBAGENTS)
    context_seed = patch.get("context_boundaries") if isinstance(patch.get("context_boundaries"), list) else out.get("context_boundaries")
    context_boundaries = _normalize_context_boundaries(
        query,
        context_seed,
        need_local_search=need_local,
        need_web_search=need_web,
        web_queries=web_queries,
    )
    need_local = any(str(it.get("kind") or "") == "local" for it in context_boundaries)
    need_web = any(str(it.get("kind") or "") == "web" for it in context_boundaries)
    web_queries = _normalize_web_queries(
        query,
        [it.get("query") for it in context_boundaries if str(it.get("kind") or "") == "web"] or web_queries,
        limit=_MAX_WEB_SUBAGENTS,
    )

    base_route = out.get("route") if isinstance(out.get("route"), dict) else {}
    patch_route = patch.get("route") if isinstance(patch.get("route"), dict) else {}
    merged_route = {
        "reply_agent": bool(patch_route.get("reply_agent", base_route.get("reply_agent", True))),
        "trace_agent": bool(patch_route.get("trace_agent", base_route.get("trace_agent", False))),
        "allow_web_retry": bool(patch_route.get("allow_web_retry", base_route.get("allow_web_retry", need_web))),
    }
    trace_seed = patch.get("trace_context_boundaries") if isinstance(patch.get("trace_context_boundaries"), list) else out.get("trace_context_boundaries")
    trace_enabled = bool(merged_route.get("trace_agent")) or bool(out.get("track_suggestion"))
    trace_context_boundaries = _build_trace_context_boundaries(
        query=query,
        raw_boundaries=trace_seed,
        need_local_search=trace_enabled and need_local,
        need_web_search=trace_enabled and bool(need_web or merged_route.get("allow_web_retry")),
        web_queries=web_queries,
        intent_contract=None,
        tracking_snapshot=None,
    )

    out["need_local_search"] = need_local
    out["need_web_search"] = need_web
    out["web_queries"] = web_queries
    out["context_boundaries"] = context_boundaries
    out["trace_context_boundaries"] = trace_context_boundaries
    out["route"] = merged_route
    out["planner_source"] = str(out.get("planner_source") or "fallback") + "+critic_patch"
    out["reason"] = str(out.get("reason") or "planner") + ";critic_patch"
    return out


def _is_tracking_intent_query(query: str) -> bool:
    text = (query or "").strip().lower()
    if not text:
        return False
    signals = [
        "\u8ffd\u8e2a",
        "\u8ddf\u8e2a",
        "\u540e\u7eed",
        "\u6301\u7eed",
        "\u8ba2\u9605",
        "\u63d0\u9192",
        "\u76d1\u63a7",
        "watch",
        "follow",
        "track",
    ]
    return any(sig in text for sig in signals)


def _is_sports_result_query(query: str) -> bool:
    text = (query or "").strip().lower()
    if not text:
        return False
    signals = [
        "nba",
        "wnba",
        "cba",
        "nfl",
        "nhl",
        "mlb",
        "epl",
        "\u6bd4\u8d5b",
        "\u6bd4\u5206",
        "\u8d5b\u7a0b",
        "\u8d5b\u679c",
        "\u6218\u7ee9",
        "\u6253\u4e86\u4ec0\u4e48",
        "\u8c01\u8d62\u4e86",
        "\u5bf9\u9635",
        "\u5b63\u540e\u8d5b",
        "\u5e38\u89c4\u8d5b",
        "score",
        "box score",
        "result",
        "results",
        "fixture",
        "fixtures",
        "match",
        "matches",
        "who won",
        "standings",
        "game recap",
    ]
    return any(sig in text for sig in signals)


def _is_time_sensitive_query(query: str) -> bool:
    text = (query or "").strip().lower()
    if not text:
        return False
    signals = [
        "\u4eca\u5929",
        "\u6628\u5929",
        "\u524d\u5929",
        "\u521a\u521a",
        "\u6700\u65b0",
        "\u6700\u8fd1",
        "\u8fd1\u671f",
        "\u8fd1\u51e0\u5929",
        "\u5b9e\u65f6",
        "\u5373\u65f6",
        "\u76ee\u524d",
        "\u6bd4\u5206",
        "\u6218\u7ee9",
        "\u8d5b\u679c",
        "\u65b0\u95fb",
        "\u80a1\u4ef7",
        "\u4ef7\u683c",
        "\u6c47\u7387",
        "now",
        "today",
        "yesterday",
        "latest",
        "recent",
        "recently",
        "breaking",
        "live",
        "score",
        "result",
        "results",
        "price",
        "quote",
        "this week",
        "last week",
        "past",
    ]
    if any(sig in text for sig in signals):
        return True
    if re.search(r"\b(last|past)\s+(24|48|72)\s*(h|hour|hours|d|day|days)\b", text):
        return True
    if re.search(r"\b(last|past|recent)\s+\d+\s*(day|days|week|weeks|month|months)\b", text):
        return True
    return False


def _main_agent_route(
    *,
    need_local_search: bool,
    need_web_search: bool,
    planned_track_suggestion: dict[str, str] | None,
    planned_route: dict[str, Any] | None,
) -> dict[str, Any]:
    reply_agent = True
    trace_agent = bool(planned_track_suggestion)
    allow_web_retry = bool(need_web_search)
    if isinstance(planned_route, dict):
        reply_agent = bool(planned_route.get("reply_agent", True))
        trace_agent = bool(planned_route.get("trace_agent", trace_agent))
        allow_web_retry = bool(planned_route.get("allow_web_retry", allow_web_retry))
    multi_agent = bool((reply_agent and (need_local_search or need_web_search)) or trace_agent)
    reasons: list[str] = []
    if isinstance(planned_route, dict):
        reasons.append("planner_route")
    if need_local_search:
        reasons.append("local_context")
    if need_web_search:
        reasons.append("web_facts")
    if trace_agent:
        reasons.append("trace_intent")
    if not reasons:
        reasons.append("chat_only")
    return {
        "multi_agent": multi_agent,
        "reply_agent": reply_agent,
        "trace_agent": trace_agent,
        "allow_web_retry": allow_web_retry,
        "reason": ",".join(reasons),
    }


def _answer_has_fact_signal(answer: str) -> bool:
    text = (answer or "").strip()
    if not text:
        return False
    if re.search(r"\d{1,4}\s*[:ï¼š-]\s*\d{1,4}", text):
        return True
    if re.search(r"\d+(?:\.\d+)?\s*(?:%|å…ƒ|ç¾å…ƒ|ä¸‡|äº¿|åˆ†|ç§’|ç‚¹|å¹´|æœˆ|æ—¥)", text):
        return True
    signals = ["æˆªè‡³", "ç›®å‰", "å®˜æ–¹", "æ•°æ®æ˜¾ç¤º", "æ¥æº", "åŒ—äº¬æ—¶é—´", "æ›´æ–°äº"]
    return any(sig in text for sig in signals)


def _verify_reply_answer(
    *,
    query: str,
    answer: str,
    need_web_search: bool,
    citations: list[AelinCitation],
) -> tuple[bool, str]:
    text = (answer or "").strip()
    if not text:
        return False, "empty_answer"
    needs_evidence = bool(need_web_search or (_is_time_sensitive_query(query) and not _is_smalltalk_query(query)))
    if needs_evidence and not citations:
        return False, "evidence_missing"
    if needs_evidence and _looks_like_link_dump_answer(text):
        return False, "link_dump_answer"
    if needs_evidence and citations and not _answer_has_fact_signal(text):
        return False, "fact_sparse"
    return True, "pass"


def _check_evidence_coverage(
    *,
    query: str,
    intent_contract: dict[str, Any] | None,
    answer: str,
    citations: list[AelinCitation],
    web_results: list[WebSearchResult],
) -> tuple[bool, str]:
    contract = intent_contract if isinstance(intent_contract, dict) else {}
    requires_citations = bool(contract.get("requires_citations"))
    if not requires_citations:
        requires_citations = bool(_is_time_sensitive_query(query) and not _is_smalltalk_query(query))

    if requires_citations and not citations:
        return False, "missing_evidence"

    freshness_hours = max(1, min(720, _safe_int(contract.get("freshness_hours"), 72)))
    if requires_citations and freshness_hours <= 48:
        has_web = any(str(it.source or "").strip().lower() == "web" for it in citations)
        if not has_web:
            return False, "freshness_unmet_no_web"

    sports_result_intent = bool(contract.get("sports_result_intent")) or _is_sports_result_query(query)
    if sports_result_intent:
        has_score = bool(_extract_score_clues(answer))
        if not has_score:
            for row in web_results[:10]:
                blob = f"{row.title} {row.snippet} {(getattr(row, 'fetched_excerpt', '') or '')}".strip()
                if _extract_score_clues(blob):
                    has_score = True
                    break
        if not has_score:
            for cite in citations[:10]:
                if _extract_score_clues(str(cite.title or "")):
                    has_score = True
                    break
        if not has_score:
            return False, "missing_score_evidence"

    return True, "coverage_pass"


def _judge_answer_grounding(
    *,
    query: str,
    answer: str,
    citations: list[AelinCitation],
    intent_contract: dict[str, Any] | None,
    service: LLMService,
    provider: str,
) -> tuple[bool, str]:
    text = (answer or "").strip()
    if not text:
        return False, "empty_answer"
    contract = intent_contract if isinstance(intent_contract, dict) else {}
    requires_factuality = bool(contract.get("requires_factuality"))
    requires_citations = bool(contract.get("requires_citations"))
    if not requires_factuality:
        requires_factuality = not _is_smalltalk_query(query)
    if requires_citations and not citations:
        return False, "missing_citations"
    if not requires_factuality:
        return True, "chat_mode"

    def _heuristic_judge() -> tuple[bool, str]:
        if _looks_like_link_dump_answer(text):
            return False, "link_dump"
        if citations and _answer_has_fact_signal(text):
            return True, "heuristic_grounded"
        if citations and (not _is_time_sensitive_query(query)):
            return True, "heuristic_non_time_sensitive"
        if citations:
            return False, "fact_signal_missing"
        return False, "no_citation_grounding"

    if provider == "rule_based" or not service.is_configured():
        return _heuristic_judge()

    evidence_lines = [
        f"- [{it.source}] {it.title} ({it.received_at})"
        for it in citations[:8]
    ]
    prompt = (
        "You are Aelin Grounding Judge.\n"
        "Decide whether answer is grounded by provided evidence.\n"
        "Return strict JSON only with schema: {\"grounded\": boolean, \"reason\": string, \"risk\": \"low|medium|high\"}.\n"
        "High risk if answer makes factual claims unsupported by evidence or asks user to search manually despite evidence."
    )
    user_msg = (
        f"user_query: {query.strip()}\n"
        f"answer: {text[:1600]}\n"
        + (f"evidence:\n{chr(10).join(evidence_lines)}\n" if evidence_lines else "evidence: none\n")
        + "Return JSON only."
    )
    try:
        raw = service._chat(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=180,
            stream=False,
        )
        parsed = _parse_json_object(str(raw or ""))
        if not isinstance(parsed, dict):
            return _heuristic_judge()
        grounded = bool(parsed.get("grounded"))
        reason = str(parsed.get("reason") or "").strip()[:160] or "judge_llm"
        return grounded, reason
    except Exception:
        return _heuristic_judge()


def _build_retry_web_queries(
    query: str,
    used_queries: list[str],
    *,
    intent_contract: dict[str, Any] | None = None,
    tracking_snapshot: dict[str, Any] | None = None,
) -> list[str]:
    base = (query or "").strip()
    if not base:
        return []
    used = {q.strip().lower() for q in used_queries if q.strip()}
    query_pack = _build_web_query_pack(
        query=base,
        base_queries=[base],
        intent_contract=intent_contract if isinstance(intent_contract, dict) else None,
        tracking_snapshot=tracking_snapshot if isinstance(tracking_snapshot, dict) else None,
        limit=min(_MAX_WEB_SUBAGENTS + 2, 7),
    )
    out: list[str] = []
    for candidate in query_pack:
        text = candidate.strip()[:180]
        if not text:
            continue
        key = text.lower()
        if key in used:
            continue
        used.add(key)
        out.append(text)
        if len(out) >= 3:
            break
    return out


def _trace_agent_suggestion(
    *,
    query: str,
    planned_track_suggestion: dict[str, str] | None,
    citations: list[AelinCitation],
    need_web_search: bool,
) -> tuple[dict[str, str] | None, str]:
    if planned_track_suggestion:
        target = str(planned_track_suggestion.get("target") or "").strip()[:240]
        source = _normalize_track_source(str(planned_track_suggestion.get("source") or "auto"))
        reason = str(planned_track_suggestion.get("reason") or "").strip()[:220]
        if target:
            return (
                {
                    "target": target,
                    "source": source,
                    "reason": reason or "Trace Agent é‡‡çº³äº† Reply Agent çš„è·Ÿè¸ªå»ºè®®ã€‚",
                },
                "use_planned_track_suggestion",
            )

    if _is_tracking_intent_query(query):
        source = "web" if (need_web_search or any(it.source == "web" for it in citations)) else "auto"
        return (
            {
                "target": query.strip()[:240],
                "source": source,
                "reason": "Trace Agent è¯†åˆ«åˆ°æ˜ç¡®çš„æŒç»­è¿½è¸ªæ„å›¾ã€‚",
            },
            "tracking_intent_matched",
        )

    return None, "no_trace_action"


def _domain_from_url(url: str) -> str:
    try:
        host = urlparse(url).netloc.strip().lower()
        return host or "web"
    except Exception:
        return "web"


def _extract_score_clues(text: str) -> list[str]:
    src = (text or "").strip()
    if not src:
        return []
    out: list[str] = []
    seen: set[str] = set()
    pattern = re.compile(
        r"([A-Za-z\u4e00-\u9fffÂ·]{1,24})?\s*(\d{2,3})\s*[-:ï¼š]\s*(\d{2,3})\s*([A-Za-z\u4e00-\u9fffÂ·]{1,24})?"
    )
    for m in pattern.finditer(src):
        a = int(m.group(2))
        b = int(m.group(3))
        if a < 50 or b < 50 or a > 200 or b > 200:
            continue
        left = (m.group(1) or "").strip()
        right = (m.group(4) or "").strip()
        clue = re.sub(r"\s+", " ", f"{left} {a}:{b} {right}".strip())
        if not clue or clue in seen:
            continue
        seen.add(clue)
        out.append(clue)
        if len(out) >= 8:
            break
    return out


def _looks_like_link_dump_answer(answer: str) -> bool:
    text = (answer or "").strip().lower()
    if not text:
        return False
    bad_signals = [
        "å¯ä»¥åœ¨å¤šä¸ªç½‘ç«™",
        "ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä¾›å‚è€ƒçš„ç½‘ç«™",
        "æ‚¨å¯ä»¥è®¿é—®è¿™äº›ç½‘ç«™",
        "ä½ å¯ä»¥è®¿é—®è¿™äº›ç½‘ç«™",
        "ç½‘ç«™æŸ¥è¯¢åˆ°",
        "duckduckgo",
        "yahoo",
    ]
    return any(sig in text for sig in bad_signals)


def _compose_web_first_answer(query: str, results: list[WebSearchResult]) -> str:
    if not results:
        return ""
    score_clues: list[str] = []
    highlights: list[str] = []
    seen_highlights: set[str] = set()
    for row in results[:10]:
        blob = f"{row.title} {row.snippet}".strip()
        for clue in _extract_score_clues(blob):
            if clue not in score_clues:
                score_clues.append(clue)
            if len(score_clues) >= 6:
                break
        snippet = (row.snippet or "").strip()
        if snippet:
            line = f"{row.title}ï¼š{snippet}"
            if line not in seen_highlights:
                seen_highlights.add(line)
                highlights.append(line)
        if len(highlights) >= 4 and len(score_clues) >= 6:
            break

    if score_clues:
        return (
            f"æˆ‘å…ˆè”ç½‘æ£€ç´¢äº†â€œ{query.strip()}â€ï¼Œå½“å‰æŠ“åˆ°çš„æ¯”åˆ†çº¿ç´¢å¦‚ä¸‹ï¼š\n"
            + "\n".join(f"- {item}" for item in score_clues[:6])
            + "\n\nè¿™äº›æ¥è‡ªå…¬å¼€ç½‘é¡µæŠ“å–ï¼Œè‹¥ä½ æ„¿æ„æˆ‘å¯ä»¥ç»§ç»­è‡ªåŠ¨è·Ÿè¸ªå¹¶æŒç»­æ›´æ–°ã€‚"
        )
    if highlights:
        return (
            f"æˆ‘å·²ç»å…ˆè”ç½‘æ£€ç´¢äº†â€œ{query.strip()}â€ã€‚ç›®å‰å¯ç¡®è®¤çš„ä¿¡æ¯ï¼š\n"
            + "\n".join(f"- {item}" for item in highlights[:4])
            + "\n\nå¦‚æœä½ å¸Œæœ›ï¼Œæˆ‘å¯ä»¥ç»§ç»­è‡ªåŠ¨è·Ÿè¸ªè¿™ä¸ªä¸»é¢˜ã€‚"
        )
    first = results[0]
    return (
        f"æˆ‘å·²ç»å…ˆè”ç½‘æ£€ç´¢äº†â€œ{query.strip()}â€ï¼Œä½†å½“å‰æŠ“åˆ°çš„ç»“æœç»†èŠ‚ä¸è¶³ä»¥ç›´æ¥ä¸‹ç»“è®ºã€‚"
        f"\n\nç›®å‰æœ€ç›¸å…³çº¿ç´¢ï¼š{first.title}ï¼ˆ{_domain_from_url(first.url)}ï¼‰"
        "\n\næˆ‘å¯ä»¥ç»§ç»­è¡¥æŠ“æ›´é«˜è´¨é‡çš„ç»“æœåå†ç»™ä½ æ›´å…·ä½“çš„ç­”æ¡ˆã€‚"
    )


def _persist_web_search_results(
    db: Session,
    user_id: int,
    *,
    query: str,
    results: list[WebSearchResult],
) -> list[AelinCitation]:
    if not results:
        return []
    contact = crud.upsert_contact(db, user_id=user_id, handle="web:search", display_name="Web Search")
    now = datetime.now(timezone.utc)
    citations: list[AelinCitation] = []
    for idx, item in enumerate(results[:10]):
        title = (item.title or "").strip()[:220]
        url = (item.url or "").strip()
        snippet = (item.snippet or "").strip()
        fetched = (getattr(item, "fetched_excerpt", "") or "").strip()
        if fetched and len(snippet) < 120:
            snippet = fetched
        snippet = snippet[:2200]
        if not title or not url:
            continue
        external_id = f"web:{hashlib.sha1(url.encode('utf-8')).hexdigest()}"
        body = f"{snippet}\n\nURL: {url}\næŸ¥è¯¢: {query.strip()[:180]}"
        msg = crud.create_message(
            db,
            user_id=user_id,
            contact_id=contact.id,
            source="web",
            external_id=external_id,
            sender=_domain_from_url(url),
            subject=title,
            body=body,
            received_at=now,
            summary=snippet or title,
        )
        if msg is not None and getattr(msg, "id", None) is None:
            db.flush()
        if msg is None:
            msg = db.scalar(
                select(Message).where(
                    Message.user_id == user_id,
                    Message.source == "web",
                    Message.external_id == external_id,
                )
            )
        if msg is None:
            continue
        crud.touch_contact_last_message(db, contact=contact, received_at=now)
        citations.append(
            AelinCitation(
                message_id=int(msg.id),
                source="web",
                source_label="Web",
                sender=_domain_from_url(url),
                sender_avatar_url=None,
                title=title,
                received_at=now.strftime("%Y-%m-%d %H:%M"),
                score=max(0.2, 6.0 - float(idx)),
            )
        )
    if citations:
        db.flush()
    return citations


def _rule_based_chat_answer(query: str, *, memory_summary: str = "", brief_summary: str = "") -> str:
    q = (query or "").strip()
    if not q:
        return "æˆ‘åœ¨ã€‚ä½ å¯ä»¥ç›´æ¥å‘Šè¯‰æˆ‘æƒ³èŠä»€ä¹ˆï¼Œæˆ–è®©æˆ‘å¸®ä½ è·Ÿè¿›æŸä¸ªæ¥æºçš„æ›´æ–°ã€‚"
    if any(token in q.lower() for token in ["ä½ å¥½", "hi", "hello"]):
        return "ä½ å¥½ï¼Œæˆ‘åœ¨è¿™ã€‚ä½ å¯ä»¥æŠŠæˆ‘å½“ä½œé•¿æœŸè®°å¿†å‹åŠ©æ‰‹ï¼ŒèŠæƒ³æ³•æˆ–è®©æˆ‘å»è·Ÿè¿›ä½ çš„ä¿¡æ¯æºéƒ½å¯ä»¥ã€‚"
    if re.search(r"[?ï¼Ÿå—ä¹ˆå˜›]$", q) or "æ˜¯ä¸æ˜¯" in q or "æœ‰æ²¡æœ‰" in q:
        base = f"å…ˆç»™ä½ ç›´æ¥ç»“è®ºï¼šå›´ç»•â€œ{q[:36]}â€ï¼Œæˆ‘å»ºè®®å…ˆä»¥å½“å‰ä¸Šä¸‹æ–‡åšåˆ¤æ–­ï¼Œå†æŒ‰éœ€è¡¥è¯æ®ã€‚"
    elif any(token in q for token in ["æ€ä¹ˆçœ‹", "çœ‹æ³•", "è§‰å¾—", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ"]):
        base = f"æˆ‘çš„ç›´æ¥çœ‹æ³•æ˜¯ï¼šå…³äºâ€œ{q[:36]}â€ï¼Œè¦å…ˆæŠ“ä½æœ€è¿‘å˜åŒ–ï¼Œå†ç»“åˆä½ é•¿æœŸå…³æ³¨ç‚¹æ¥åˆ¤æ–­ã€‚"
    else:
        base = f"ç›´æ¥å›ç­”ï¼šä½ æåˆ°çš„â€œ{q[:36]}â€å¯ä»¥å…ˆæŒ‰å½“å‰å·²çŸ¥ä¿¡æ¯å¤„ç†ã€‚"
    if memory_summary:
        base += "\n\næˆ‘ä¹Ÿä¼šå‚è€ƒä½ å·²æœ‰çš„é•¿æœŸè®°å¿†æ¥ä¿æŒä¸Šä¸‹æ–‡è¿ç»­ã€‚"
    if brief_summary:
        base += f"\n\nå¦‚æœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥åŸºäºä»Šæ—¥ç®€æŠ¥ç»§ç»­å±•å¼€ï¼š{brief_summary}"
    base += "\n\nå¦‚æœé—®é¢˜æ¶‰åŠå¤–éƒ¨äº‹å®ï¼Œæˆ‘ä¼šå…ˆè‡ªåŠ¨æ£€ç´¢ï¼Œå†ç›´æ¥ç»™ä½ ç»“è®ºã€‚"
    return base


def _looks_like_non_answer(answer: str) -> bool:
    text = re.sub(r"\s+", " ", (answer or "").strip().lower())
    if not text:
        return True
    bad_starts = (
        "è¿™æ˜¯ä¸ªå¥½é—®é¢˜",
        "æˆ‘ä¹Ÿä¼šå‚è€ƒä½ å·²æœ‰çš„é•¿æœŸè®°å¿†",
        "å¦‚æœä½ éœ€è¦",
        "å¯ä»¥ç›´æ¥è¯´",
        "å¸®æˆ‘æ£€ç´¢ç›¸å…³æ›´æ–°",
    )
    if any(text.startswith(s) for s in bad_starts):
        return True
    if "å¸®ä½ æ£€ç´¢" in text and ("ç»“è®º" not in text and "å›ç­”" not in text):
        return True
    if "ä½ å¯ä»¥æ‰‹åŠ¨" in text:
        return True
    if len(text) < 24:
        return True
    return False


def _enforce_answer_first(
    *,
    query: str,
    answer: str,
    citations: list[AelinCitation],
    web_results: list[WebSearchResult],
    memory_summary: str,
    brief_summary: str,
    todo_titles: list[str] | None = None,
    image_count: int = 0,
) -> str:
    text = (answer or "").strip()
    if text and not _looks_like_non_answer(text):
        return text
    if citations:
        return _rule_based_answer(
            query,
            memory_summary,
            citations,
            brief_summary=brief_summary,
            todo_titles=todo_titles or [],
            image_count=image_count,
        )
    if web_results:
        guarded = _compose_web_first_answer(query, web_results)
        if guarded:
            return guarded
    return _rule_based_chat_answer(query, memory_summary=memory_summary, brief_summary=brief_summary)


def _normalize_expression_id(raw: str | None) -> str | None:
    text = (raw or "").strip().lower()
    if not text:
        return None
    alias = _AELIN_EXPRESSION_ALIASES.get(text)
    if alias:
        return alias
    if text.isdigit():
        n = int(text)
        if 1 <= n <= 11:
            text = f"exp-{n:02d}"
    if text.startswith("exp_"):
        text = "exp-" + text[4:]
    if re.fullmatch(r"exp-\d{1,2}", text):
        n = int(text.split("-", 1)[1])
        if 1 <= n <= 11:
            text = f"exp-{n:02d}"
    if text in _AELIN_EXPRESSION_IDS:
        return text
    return None


def _extract_expression_tag(answer: str) -> tuple[str, str | None]:
    text = (answer or "").strip()
    if not text:
        return "", None
    patterns = [
        r"\[(?:expression|expr|sticker|è¡¨æƒ…|æƒ…ç»ª)\s*[:ï¼š]\s*([A-Za-z0-9_-]{1,16})\]",
        r"<(?:expression|expr|sticker)\s*[:ï¼š]\s*([A-Za-z0-9_-]{1,16})>",
    ]
    expression: str | None = None
    cleaned = text
    for pat in patterns:
        match = re.search(pat, cleaned, flags=re.I)
        if not match:
            continue
        expression = _normalize_expression_id(match.group(1))
        cleaned = re.sub(pat, "", cleaned, flags=re.I).strip()
        if expression:
            break
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned).strip()
    return cleaned, expression


def _contains_emoji(text: str) -> bool:
    return bool(_EMOJI_CHAR_RE.search(str(text or "")))


def _normalize_emoji_token(raw: str | None) -> str | None:
    text = str(raw or "").strip()
    if not text:
        return None
    snippet = text[:8]
    if not _contains_emoji(snippet):
        return None
    return snippet


def _extract_emoji_tag(answer: str) -> tuple[str, str | None]:
    text = (answer or "").strip()
    if not text:
        return "", None
    pattern = r"\[(?:emoji|emj|è¡¨æƒ…ç¬¦å·|emoji_tag)\s*[:ï¼š]\s*([^\]\n]{1,16})\]"
    match = re.search(pattern, text, flags=re.I)
    if not match:
        return text, None
    emoji = _normalize_emoji_token(match.group(1))
    cleaned = re.sub(pattern, "", text, flags=re.I).strip()
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned).strip()
    return cleaned, emoji


def _apply_answer_emoji(answer: str, expression: str, *, explicit_emoji: str | None = None) -> str:
    text = (answer or "").strip()
    if not text:
        return explicit_emoji or _AELIN_EMOJI_BY_EXPRESSION.get(expression, "ğŸ™‚")
    if _contains_emoji(text):
        return text
    emoji = _normalize_emoji_token(explicit_emoji) or _AELIN_EMOJI_BY_EXPRESSION.get(expression)
    if not emoji:
        return text
    return f"{text} {emoji}"


def _pick_expression(query: str, answer: str, *, generation_failed: bool = False) -> str:
    q = (query or "").lower()
    a = (answer or "").lower()
    text = f"{q}\n{a}"

    if generation_failed or any(token in text for token in ["å¤±è´¥", "é”™è¯¯", "æŠ±æ­‰", "æ— æ³•", "æš‚ä¸æ”¯æŒ", "ä¸ç¡®å®š"]):
        return "exp-07"
    if any(token in text for token in ["ç”Ÿæ°”", "æ„¤æ€’", "æ°”æ­»", "ç«å¤§", "ç¦»è°±"]):
        return "exp-08"
    if any(token in text for token in ["é£é™©", "è°¨æ…", "è­¦å‘Š", "ä¸¥è‚ƒ", "æ³¨æ„", "ä¸å»ºè®®"]):
        return "exp-05"
    if any(token in text for token in ["è¿‡è½½", "å¤ªå›°", "ç¡äº†", "æ™šå®‰", "ä¼‘æ¯", "ç´¯", "å´©æºƒ", "èººå¹³"]):
        return "exp-11"
    if any(token in text for token in ["è§‚å¯Ÿ", "å›´è§‚", "åç»­", "ç»§ç»­è·Ÿè¸ª", "ç­‰ç­‰çœ‹"]):
        return "exp-06"
    if any(token in text for token in ["çˆ±ä½ ", "å–œæ¬¢", "å¿ƒåŠ¨", "å¯çˆ±", "æµªæ¼«", "å®³ç¾", "è„¸çº¢"]):
        return "exp-01"
    if any(token in text for token in ["èµš", "ç›ˆåˆ©", "æ‹¿ä¸‹", "æå®š", "é«˜æ”¶ç›Š", "å‘è´¢"]):
        return "exp-10"
    if any(token in text for token in ["æ­å–œ", "å¤ªæ£’", "å‰å®³", "ä¼˜ç§€", "å¥½è€¶", "å¼€å¿ƒ"]):
        return "exp-02"
    if any(token in text for token in ["è°¢è°¢", "æ„Ÿè°¢", "æ”¯æŒ", "åŠ æ²¹", "è¾›è‹¦äº†"]):
        return "exp-03"
    if any(token in text for token in ["å“ˆå“ˆ", "hh", "ç¬‘æ­»", "æœ‰è¶£", "å¥½ç©"]):
        return "exp-09"
    if ("?" in q) or ("ï¼Ÿ" in q) or any(token in q for token in ["ä¸ºä»€ä¹ˆ", "æ€ä¹ˆ", "å—", "å•¥", "ä»€ä¹ˆ", "å¦‚ä½•"]):
        return "exp-04"
    if any(token in text for token in ["æ”¶åˆ°", "æ˜ç™½", "ok", "å¥½çš„", "å®‰æ’"]):
        return "exp-06"
    return "exp-04"


def _expression_mapping_prompt() -> str:
    lines = []
    for exp_id in sorted(_AELIN_EXPRESSION_META.keys()):
        meta = _AELIN_EXPRESSION_META[exp_id]
        lines.append(f"- {exp_id}: {meta['label']}ï¼ˆ{meta['usage']}ï¼‰")
    return "\n".join(lines)


def _now_ms() -> int:
    return int(datetime.now(timezone.utc).timestamp() * 1000)


def _sse_event(event: str, payload: dict[str, Any]) -> str:
    data = json.dumps(payload, ensure_ascii=False, default=str)
    return f"event: {event}\ndata: {data}\n\n"


def _dedupe_citations(rows: list[AelinCitation], *, limit: int) -> list[AelinCitation]:
    out: list[AelinCitation] = []
    seen: set[tuple[int, str, str]] = set()
    safe_limit = max(1, min(20, int(limit or 6)))
    sorted_rows = sorted(rows, key=lambda it: float(it.score or 0.0), reverse=True)
    for it in sorted_rows:
        key = (int(it.message_id or 0), str(it.source or ""), str(it.title or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
        if len(out) >= safe_limit:
            break
    return out


def _aelin_chat_impl(
    payload: AelinChatRequest,
    db: Session,
    current_user: User,
    *,
    event_cb: Callable[[str, dict[str, Any]], None] | None = None,
) -> AelinChatResponse:
    tool_trace: list[AelinToolStep] = []
    trace_index: dict[str, int] = {}

    def emit(event: str, data: dict[str, Any]) -> None:
        if event_cb is None:
            return
        try:
            event_cb(event, data)
        except Exception:
            pass

    def add_trace(stage: str, *, status: str = "completed", detail: str = "", count: int = 0) -> None:
        safe_stage = str(stage or "stage").strip().lower()[:80] or "stage"
        safe_status = str(status or "completed").strip().lower()[:24] or "completed"
        safe_detail = str(detail or "").strip()[:240]
        safe_count = max(0, int(count or 0))
        ts = _now_ms()
        step = AelinToolStep(
            stage=safe_stage,
            status=safe_status,
            detail=safe_detail,
            count=safe_count,
            ts=ts,
        )
        idx = trace_index.get(safe_stage)
        if idx is None:
            trace_index[safe_stage] = len(tool_trace)
            tool_trace.append(step)
        else:
            prev = tool_trace[idx]
            step = step.model_copy(update={"ts": int(prev.ts or 0) if int(prev.ts or 0) > 0 else ts})
            tool_trace[idx] = step
        emit("trace", {"step": step.model_dump()})

    service, provider = _resolve_llm_service(db, current_user)
    search_mode = _normalize_search_mode(getattr(payload, "search_mode", "auto"))
    llm_generation_failed = False

    base_bundle = _build_context_bundle(
        db,
        current_user.id,
        workspace=payload.workspace,
        query="",
    )
    active_bundle = base_bundle
    memory_summary = str(base_bundle.get("summary") or "")
    brief_summary = base_bundle["daily_brief"].summary if base_bundle.get("daily_brief") else ""
    todo_titles = [item.title for item in base_bundle.get("todos", [])]
    images = _normalize_images(payload.images)
    history_turns = _normalize_history(payload.history)

    tracking_snapshot = _build_planner_tracking_snapshot(db, user_id=current_user.id, query=payload.query)
    intent_contract = _build_intent_contract(
        query=payload.query,
        service=service,
        provider=provider,
        memory_summary=memory_summary,
        tracking_snapshot=tracking_snapshot,
    )
    intent_source = str(intent_contract.get("intent_source") or "fallback")
    intent_type = str(intent_contract.get("intent_type") or "retrieval")
    time_scope = str(intent_contract.get("time_scope") or "any")
    freshness_hours = max(1, min(720, _safe_int(intent_contract.get("freshness_hours"), 72)))
    intent_conf = max(0.0, min(1.0, _safe_float(intent_contract.get("confidence"), 0.62)))
    add_trace(
        "intent_lens",
        status="completed",
        detail=f"type={intent_type}; scope={time_scope}; freshness_h={freshness_hours}; conf={intent_conf:.2f}; src={intent_source}",
    )

    tool_plan = _plan_tool_usage(
        query=payload.query,
        service=service,
        provider=provider,
        memory_summary=memory_summary,
        tracking_snapshot=tracking_snapshot,
        intent_contract=intent_contract,
    )
    critic = _critic_tool_plan(
        query=payload.query,
        intent_contract=intent_contract,
        tool_plan=tool_plan,
        service=service,
        provider=provider,
    )
    critic_source = str(critic.get("critic_source") or "fallback")
    critic_reason = str(critic.get("reason") or "").strip()[:180]
    if bool(critic.get("accepted", True)):
        add_trace("plan_critic", status="completed", detail=f"{critic_source}:{critic_reason or 'accepted'}")
    else:
        add_trace("plan_critic", status="failed", detail=f"{critic_source}:{critic_reason or 'rejected'}")
        patch = critic.get("patch") if isinstance(critic.get("patch"), dict) else None
        if isinstance(patch, dict):
            tool_plan = _apply_plan_patch(
                query=payload.query,
                tool_plan=tool_plan,
                patch=patch,
            )
            add_trace("plan_critic", status="completed", detail=f"{critic_source}:patched")

    planner_source = str(tool_plan.get("planner_source") or "fallback").strip().lower()
    planning_reason = str(tool_plan.get("reason") or "planner:none")
    if planner_source:
        planning_reason = f"{planning_reason}; planner={planner_source}"
    if critic_reason:
        planning_reason = f"{planning_reason}; critic={critic_reason}"
    need_local_search = bool(tool_plan.get("need_local_search"))
    need_web_search = bool(tool_plan.get("need_web_search"))
    web_queries = _normalize_web_queries(payload.query, tool_plan.get("web_queries"))
    context_boundaries = _normalize_context_boundaries(
        payload.query,
        tool_plan.get("context_boundaries"),
        need_local_search=need_local_search,
        need_web_search=need_web_search,
        web_queries=web_queries,
    )
    if search_mode == "local_only":
        context_boundaries = [it for it in context_boundaries if str(it.get("kind") or "") == "local"]
        if not context_boundaries:
            context_boundaries = [
                {
                    "kind": "local",
                    "query": payload.query.strip()[:180],
                    "scope": "forced local",
                }
            ]
        planning_reason += ";search_mode=local_only"
    elif search_mode == "web_only":
        context_boundaries = [it for it in context_boundaries if str(it.get("kind") or "") == "web"]
        if not context_boundaries:
            fallback_web = _normalize_web_queries(payload.query, web_queries, limit=_MAX_WEB_SUBAGENTS)
            context_boundaries = [
                {"kind": "web", "query": q, "scope": q}
                for q in (fallback_web or [payload.query.strip()[:180]])
            ]
        planning_reason += ";search_mode=web_only"

    local_boundaries = [it for it in context_boundaries if str(it.get("kind") or "") == "local"][:_MAX_LOCAL_SUBAGENTS]
    web_boundaries = [it for it in context_boundaries if str(it.get("kind") or "") == "web"][:_MAX_WEB_SUBAGENTS]
    if web_boundaries:
        decomposed = _decompose_web_context_boundaries(
            query=payload.query,
            web_boundaries=web_boundaries,
            intent_contract=intent_contract,
            tracking_snapshot=tracking_snapshot,
            service=service,
            provider=provider,
        )
        decompose_source = str(decomposed.get("source") or "fallback")
        decompose_reason = str(decomposed.get("reason") or "").strip()[:180]
        decomposed_boundaries = (
            decomposed.get("boundaries")
            if isinstance(decomposed.get("boundaries"), list)
            else []
        )
        normalized_decomposed = _normalize_context_boundaries(
            payload.query,
            decomposed_boundaries,
            need_local_search=False,
            need_web_search=True,
            web_queries=[str(it.get("query") or "") for it in decomposed_boundaries if isinstance(it, dict)],
        )
        web_boundaries = [
            it
            for it in normalized_decomposed
            if str(it.get("kind") or "") == "web"
        ][:_MAX_WEB_SUBAGENTS] or web_boundaries
        planning_reason = f"{planning_reason};web_decomposer={decompose_source}:{len(web_boundaries)}"
        add_trace(
            "query_decomposer",
            status="completed" if decompose_source == "llm" else "completed",
            detail=f"{decompose_source}:{decompose_reason or 'ok'}; web={len(web_boundaries)}",
            count=len(web_boundaries),
        )
    else:
        add_trace("query_decomposer", status="skipped", detail="no web boundary")
    context_boundaries = [*local_boundaries, *web_boundaries]
    need_local_search = bool(local_boundaries)
    need_web_search = bool(web_boundaries)
    web_queries = [str(it.get("query") or "") for it in web_boundaries if str(it.get("query") or "").strip()]
    planned_track_suggestion = tool_plan.get("track_suggestion") if isinstance(tool_plan.get("track_suggestion"), dict) else None
    route = _main_agent_route(
        need_local_search=need_local_search,
        need_web_search=need_web_search,
        planned_track_suggestion=planned_track_suggestion if isinstance(planned_track_suggestion, dict) else None,
        planned_route=tool_plan.get("route") if isinstance(tool_plan.get("route"), dict) else None,
    )
    trace_route_enabled = bool(route.get("trace_agent")) or bool(planned_track_suggestion)
    trace_context_boundaries = _build_trace_context_boundaries(
        query=payload.query,
        raw_boundaries=tool_plan.get("trace_context_boundaries"),
        need_local_search=trace_route_enabled and need_local_search,
        need_web_search=trace_route_enabled and bool(need_web_search or route.get("allow_web_retry")),
        web_queries=web_queries,
        intent_contract=intent_contract,
        tracking_snapshot=tracking_snapshot,
    )
    if search_mode == "local_only":
        trace_context_boundaries = [
            it for it in trace_context_boundaries if str(it.get("kind") or "") == "local"
        ]
    elif search_mode == "web_only":
        trace_context_boundaries = [
            it for it in trace_context_boundaries if str(it.get("kind") or "") == "web"
        ]
    trace_local_boundaries = [
        it for it in trace_context_boundaries if str(it.get("kind") or "") == "local"
    ][:2]
    trace_web_boundaries = [
        it for it in trace_context_boundaries if str(it.get("kind") or "") == "web"
    ][:3]

    add_trace(
        "main_agent",
        status="completed",
        detail=(
            f"{planning_reason}; mode={search_mode}; local={len(local_boundaries)}; web={len(web_boundaries)}; "
            f"trace_local={len(trace_local_boundaries)}; trace_web={len(trace_web_boundaries)}; "
            f"matched_tracking={int(tracking_snapshot.get('matched_count') or 0)}"
        ),
    )
    add_trace(
        "reply_agent",
        status="completed",
        detail=(
            f"route reply={1 if route.get('reply_agent') else 0}, "
            f"trace={1 if route.get('trace_agent') else 0}, "
            f"retry={1 if route.get('allow_web_retry') else 0}"
        ),
    )
    add_trace(
        "reply_dispatch",
        status="completed",
        detail=f"context_boundaries={len(context_boundaries)}; trace_boundaries={len(trace_context_boundaries)}",
    )

    local_citations: list[AelinCitation] = []
    if need_local_search and route.get("reply_agent", True):
        add_trace(
            "local_search",
            status="running",
            detail=f"dispatching {len(local_boundaries)} local subagents",
        )
        best_local_count = -1
        local_jobs: list[tuple[int, dict[str, str], str, str]] = []
        for idx, boundary in enumerate(local_boundaries, start=1):
            sub_query = str(boundary.get("query") or payload.query).strip()[:180]
            sub_scope = str(boundary.get("scope") or sub_query).strip()[:120]
            add_trace(f"local_search_subagent_{idx}", status="running", detail=sub_scope or sub_query)
            local_jobs.append((idx, boundary, sub_query, sub_scope))

        def _fetch_local_bundle(raw_query: str) -> tuple[dict[str, Any] | None, list[AelinCitation], str]:
            local_db = create_session()
            try:
                bundle = _build_context_bundle(
                    local_db,
                    current_user.id,
                    workspace=payload.workspace,
                    query=raw_query,
                )
                cites = _to_citations(bundle["focus_items_raw"], payload.max_citations)
                return bundle, cites, ""
            except Exception as exc:
                return None, [], str(exc)[:140]
            finally:
                try:
                    local_db.close()
                except Exception:
                    pass

        futures: dict[Any, tuple[int, dict[str, str], str, str]] = {}
        max_workers = max(1, min(len(local_jobs), _MAX_LOCAL_SUBAGENTS))
        with ThreadPoolExecutor(max_workers=max_workers) as pool:
            for idx, boundary, sub_query, sub_scope in local_jobs:
                futures[pool.submit(_fetch_local_bundle, sub_query)] = (idx, boundary, sub_query, sub_scope)

            for fut in as_completed(futures):
                idx, boundary, sub_query, sub_scope = futures[fut]
                sub_stage = f"local_search_subagent_{idx}"
                try:
                    bundle, cites, local_error = fut.result()
                except Exception as e:
                    add_trace(sub_stage, status="failed", detail=f"{sub_scope or sub_query}: {str(e)[:140]}")
                    continue
                if local_error or (not isinstance(bundle, dict)):
                    add_trace(sub_stage, status="failed", detail=f"{sub_scope or sub_query}: {local_error or 'local error'}")
                    continue
                local_citations.extend(cites)
                if len(cites) > best_local_count:
                    best_local_count = len(cites)
                    active_bundle = bundle
                add_trace(sub_stage, status="completed", detail=sub_scope or sub_query, count=len(cites))

        if local_citations:
            local_citations = _hydrate_citation_avatars(db, current_user.id, local_citations)
        add_trace(
            "local_search",
            status="completed",
            detail="local search finished",
            count=len(local_citations),
        )
    else:
        add_trace("local_search", status="skipped", detail="local search skipped by route")

    web_citations: list[AelinCitation] = []
    web_results_for_answer: list[WebSearchResult] = []
    web_evidence_lines: list[str] = []
    used_web_queries: list[str] = []
    web_provider_totals: Counter[str] = Counter()
    web_fetch_mode_totals: Counter[str] = Counter()

    if need_web_search and route.get("reply_agent", True):
        add_trace(
            "web_search",
            status="running",
            detail=f"dispatching {len(web_boundaries)} web subagents",
        )
        total = len(web_boundaries)
        completed = 0
        evidence_count = 0
        for idx, boundary in enumerate(web_boundaries, start=1):
            add_trace(f"web_search_subagent_{idx}", status="running", detail=str(boundary.get("scope") or boundary.get("query") or ""))

        def _fetch_web_rows(raw_query: str) -> list[WebSearchResult]:
            return _web_search.search_and_fetch(raw_query, max_results=6, fetch_top_k=3)

        futures: dict[Any, tuple[int, dict[str, str], str]] = {}
        max_workers = max(1, min(len(web_boundaries), _MAX_WEB_SUBAGENTS))
        with ThreadPoolExecutor(max_workers=max_workers) as pool:
            for idx, boundary in enumerate(web_boundaries, start=1):
                q = str(boundary.get("query") or payload.query).strip()[:180]
                used_web_queries.append(q)
                futures[pool.submit(_fetch_web_rows, q)] = (idx, boundary, q)

            for fut in as_completed(futures):
                idx, boundary, q = futures[fut]
                sub_stage = f"web_search_subagent_{idx}"
                completed += 1
                try:
                    rows = fut.result() or []
                except Exception as e:
                    add_trace(sub_stage, status="failed", detail=f"{q}: {str(e)[:140]}")
                    continue
                if not rows:
                    add_trace(sub_stage, status="failed", detail=f"{q}: no result")
                    continue

                web_results_for_answer.extend(rows[:5])
                provider_counts = Counter(str(getattr(it, "provider", "") or "unknown") for it in rows[:8])
                fetch_counts = Counter(str(getattr(it, "fetch_mode", "") or "none") for it in rows[:8])
                web_provider_totals.update(provider_counts)
                web_fetch_mode_totals.update(fetch_counts)
                provider_note = ",".join(f"{name}:{count}" for name, count in provider_counts.most_common(3))
                fetch_note = ",".join(f"{name}:{count}" for name, count in fetch_counts.most_common(3))
                try:
                    persisted = _persist_web_search_results(
                        db,
                        current_user.id,
                        query=q,
                        results=rows,
                    )
                except Exception:
                    persisted = []
                web_citations.extend(persisted)
                for item in rows[:5]:
                    host = _domain_from_url(item.url)
                    snippet = ((getattr(item, "fetched_excerpt", "") or "").strip() or (item.snippet or "").strip())
                    provider_name = str(getattr(item, "provider", "") or "unknown")
                    fetch_mode = str(getattr(item, "fetch_mode", "") or "none")
                    line = f"- [Web/{provider_name}/{fetch_mode}] {item.title} ({host})"
                    if snippet:
                        line += f" | {snippet}"
                    web_evidence_lines.append(line)
                for ridx, cite in enumerate(persisted, start=1):
                    evidence_count += 1
                    snippet = ""
                    provider_name = "unknown"
                    fetch_mode = "none"
                    if ridx - 1 < len(rows):
                        row = rows[ridx - 1]
                        snippet = (
                            (getattr(row, "fetched_excerpt", "") or "").strip()
                            or (row.snippet or "").strip()
                        )[:280]
                        provider_name = str(getattr(row, "provider", "") or "unknown")
                        fetch_mode = str(getattr(row, "fetch_mode", "") or "none")
                    emit(
                        "evidence",
                        {
                            "citation": cite.model_dump(),
                            "snippet": snippet,
                            "query": q,
                            "provider": provider_name,
                            "fetch_mode": fetch_mode,
                            "progress": {
                                "query_index": completed,
                                "query_total": total,
                                "evidence_count": evidence_count,
                            },
                        },
                    )
                add_trace(
                    sub_stage,
                    status="completed",
                    detail=f"{str(boundary.get('scope') or q)}; p={provider_note or 'unknown'}; f={fetch_note or 'none'}",
                    count=len(persisted),
                )

        provider_total_note = ",".join(f"{name}:{count}" for name, count in web_provider_totals.most_common(4))
        fetch_total_note = ",".join(f"{name}:{count}" for name, count in web_fetch_mode_totals.most_common(4))
        add_trace(
            "web_search",
            status="completed" if web_citations else "failed",
            detail=(
                f"web search finished; p={provider_total_note or 'none'}; f={fetch_total_note or 'none'}"
                if web_citations
                else "web search empty"
            ),
            count=len(web_citations),
        )
    else:
        add_trace("web_search", status="skipped", detail="web search skipped by route")

    max_citations = max(1, min(20, int(payload.max_citations or 6)))
    citations = _dedupe_citations([*local_citations, *web_citations], limit=max_citations)
    add_trace(
        "message_hub",
        status="completed",
        detail=f"merged local={len(local_citations)} web={len(web_citations)}",
        count=len(citations),
    )

    pin_lines = [
        f"{item.display_name}(score {item.score:.1f}, unread {item.unread_count})"
        for item in active_bundle.get("pin_recommendations", [])[:4]
    ]
    memory_prompt = _memory.build_system_memory_prompt(
        db,
        current_user.id,
        query=payload.query if need_local_search else "",
    )

    add_trace("generation", status="running", detail="composing answer")
    generation_detail = "generation completed"

    if provider == "rule_based":
        if local_citations:
            answer = _rule_based_answer(
                payload.query,
                memory_summary,
                citations,
                brief_summary=brief_summary,
                todo_titles=todo_titles,
                image_count=len(images),
            )
            generation_detail = "rule_based with local evidence"
        elif web_evidence_lines:
            answer = _compose_web_first_answer(payload.query, web_results_for_answer)
            generation_detail = "rule_based with web evidence"
        else:
            answer = _rule_based_chat_answer(
                payload.query,
                memory_summary=memory_summary,
                brief_summary=brief_summary,
            )
            generation_detail = "rule_based chat-only"
    elif not service.is_configured():
        answer = (
            "å½“å‰æ¨¡å‹è¿æ¥ä¸å¯ç”¨ï¼ŒAelin æš‚æ—¶æ— æ³•è°ƒç”¨å¤–éƒ¨æ¨¡å‹ã€‚\n\n"
            "è¯·å…ˆæ£€æŸ¥ Provider / Base URL / API Key é…ç½®åé‡è¯•ã€‚"
        )
        generation_detail = "llm not configured"
    else:
        evidence_block = "\n".join(
            f"- [{it.source_label}] {it.title} ({it.sender}, {it.received_at})"
            for it in citations[:8]
        ) if citations else ""
        prompt = (
            "You are Aelin, a signal-native assistant.\n"
            "Always answer in Simplified Chinese.\n"
            "Answer the user's question directly first.\n"
            "If retrieval evidence is provided, use it directly and do not ask user to search manually.\n"
            "If evidence is weak, state uncertainty and avoid fabrication.\n"
            "Keep response concise and practical.\n"
            "You may use 0-2 natural emoji in the answer body when it helps tone.\n"
            "Aelin has 11 expressions. Choose one according to semantics below:\n"
            + _expression_mapping_prompt()
            + "\n"
            "You MUST append exactly one tag at the end: [expression:exp-XX].\n"
            "Optional emoji control tag is allowed only before the final expression tag: [emoji:ğŸ™‚]."
        )
        retrieval_note = (
            f"planner={planning_reason}; "
            f"local={'on' if need_local_search else 'off'}; "
            f"web={'on' if need_web_search else 'off'}"
        )
        user_msg = (
            f"ç”¨æˆ·é—®é¢˜: {payload.query.strip()}\n\n"
            f"å·¥å…·è§„åˆ’: {retrieval_note}\n\n"
            + (
                "æœ€è¿‘å¯¹è¯:\n"
                + "\n".join(
                    f"- {'ç”¨æˆ·' if turn['role'] == 'user' else 'Aelin'}: {turn['content'][:220]}"
                    for turn in history_turns[-6:]
                )
                + "\n\n"
                if history_turns else ""
            )
            + f"é•¿æœŸè®°å¿†æ‘˜è¦: {memory_summary or 'æš‚æ— '}\n\n"
            + f"ä»Šæ—¥ç®€æŠ¥: {brief_summary or 'æš‚æ— '}\n\n"
            + f"å¾…è·Ÿè¿›äº‹é¡¹: {'; '.join(todo_titles[:5]) if todo_titles else 'æš‚æ— '}\n\n"
            + f"ç½®é¡¶å»ºè®®: {'; '.join(pin_lines) if pin_lines else 'æš‚æ— '}\n\n"
            + (
                "ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡:\n"
                + "\n".join(f"- {img['name'] or 'image'}" for img in images)
                + "\n\n"
                if images else ""
            )
            + (f"æœ¬åœ°è¯æ®:\n{evidence_block}\n\n" if evidence_block else "")
            + (f"è”ç½‘è¯æ®:\n{chr(10).join(web_evidence_lines[:8])}\n" if web_evidence_lines else "")
        )
        llm_messages: list[dict[str, Any]] = [{"role": "system", "content": prompt}]
        if memory_prompt:
            llm_messages.append({"role": "system", "content": memory_prompt})
        if history_turns:
            llm_messages.extend(history_turns[-10:])
        if images:
            user_content: list[dict[str, Any]] = [{"type": "text", "text": user_msg}]
            for img in images:
                user_content.append({"type": "image_url", "image_url": {"url": img["data_url"]}})
            llm_messages.append({"role": "user", "content": user_content})
        else:
            llm_messages.append({"role": "user", "content": user_msg})

        llm_error: str | None = None
        answer = ""
        try:
            raw = service._chat(
                messages=llm_messages,
                max_tokens=520,
                stream=False,
            )
            answer = str(raw).strip() if raw else ""
            generation_detail = "llm generation succeeded"
        except Exception as e:
            llm_error = str(e)
            llm_generation_failed = True
            generation_detail = f"llm failed: {llm_error[:120]}"
            if images:
                fallback_messages: list[dict[str, Any]] = [{"role": "system", "content": prompt}]
                if memory_prompt:
                    fallback_messages.append({"role": "system", "content": memory_prompt})
                fallback_messages.append({"role": "user", "content": user_msg})
                try:
                    raw = service._chat(
                        messages=fallback_messages,
                        max_tokens=520,
                        stream=False,
                    )
                    maybe = str(raw).strip() if raw else ""
                    if maybe:
                        answer = "å½“å‰æ¨¡å‹å¯èƒ½ä¸æ”¯æŒå›¾ç‰‡è¾“å…¥ï¼Œä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬ä¸Šä¸‹æ–‡çš„å›å¤ï¼š\n\n" + maybe
                        generation_detail = "llm fallback text-only succeeded"
                except Exception as e2:
                    if not llm_error:
                        llm_error = str(e2)
        if not answer:
            if citations:
                answer = _rule_based_answer(
                    payload.query,
                    memory_summary,
                    citations,
                    brief_summary=brief_summary,
                    todo_titles=todo_titles,
                    image_count=len(images),
                )
                generation_detail = "fallback to rule_based with citations"
            else:
                answer = (
                    "æˆ‘åˆšæ‰è°ƒç”¨å¤–éƒ¨æ¨¡å‹å¤±è´¥ï¼Œå…ˆç»™ä½ ä¸€ä¸ªä¿åº•å›å¤ã€‚"
                    + (f"\n\né”™è¯¯ï¼š{llm_error}" if llm_error else "")
                    + "\n\n"
                    + _rule_based_chat_answer(
                        payload.query,
                        memory_summary=memory_summary,
                        brief_summary=brief_summary,
                    )
                )
                generation_detail = "fallback to rule_based chat"
        if answer and web_results_for_answer and _looks_like_link_dump_answer(answer):
            guarded = _compose_web_first_answer(payload.query, web_results_for_answer)
            if guarded:
                answer = guarded
            generation_detail = f"{generation_detail}; retrieval evidence guard applied"

    answer = _enforce_answer_first(
        query=payload.query,
        answer=answer,
        citations=citations,
        web_results=web_results_for_answer,
        memory_summary=memory_summary,
        brief_summary=brief_summary,
        todo_titles=todo_titles,
        image_count=len(images),
    )

    answer, tagged_expression = _extract_expression_tag(answer)
    answer, tagged_emoji = _extract_emoji_tag(answer)
    expression = tagged_expression or _pick_expression(payload.query, answer, generation_failed=llm_generation_failed)
    answer = _apply_answer_emoji(answer, expression, explicit_emoji=tagged_emoji)
    add_trace("generation", status="completed", detail=generation_detail, count=len(citations))

    add_trace("grounding_judge", status="running", detail="checking grounding", count=len(citations))
    grounded, grounding_reason = _judge_answer_grounding(
        query=payload.query,
        answer=answer,
        citations=citations,
        intent_contract=intent_contract,
        service=service,
        provider=provider,
    )
    add_trace(
        "grounding_judge",
        status="completed" if grounded else "failed",
        detail=grounding_reason,
        count=len(citations),
    )

    add_trace("coverage_verifier", status="running", detail="checking evidence coverage", count=len(citations))
    coverage_ok, coverage_reason = _check_evidence_coverage(
        query=payload.query,
        intent_contract=intent_contract,
        answer=answer,
        citations=citations,
        web_results=web_results_for_answer,
    )
    add_trace(
        "coverage_verifier",
        status="completed" if coverage_ok else "failed",
        detail=coverage_reason,
        count=len(citations),
    )

    add_trace("reply_verifier", status="running", detail="verifying reply quality", count=len(citations))
    verified, verify_reason = _verify_reply_answer(
        query=payload.query,
        answer=answer,
        need_web_search=need_web_search,
        citations=citations,
    )
    retried_web = 0
    has_web_evidence = any(str(it.source or "").strip().lower() == "web" for it in citations)
    requires_citations = bool(intent_contract.get("requires_citations")) if isinstance(intent_contract, dict) else False
    quality_failed = (not verified) or (not grounded) or (not coverage_ok)
    allow_quality_retry = bool(route.get("allow_web_retry")) or (requires_citations and (not has_web_evidence))
    if quality_failed and allow_quality_retry:
        retry_queries = _build_retry_web_queries(
            payload.query,
            used_web_queries,
            intent_contract=intent_contract,
            tracking_snapshot=tracking_snapshot,
        )
        if retry_queries:
            retried_web = len(retry_queries)
            add_trace("web_search", status="running", detail=f"verifier retry x{len(retry_queries)}", count=len(web_citations))
            base_idx = len(web_boundaries)
            evidence_count = len(web_citations)
            retry_provider_totals: Counter[str] = Counter()
            retry_fetch_totals: Counter[str] = Counter()
            for idx, rq in enumerate(retry_queries, start=1):
                sub_stage = f"web_search_subagent_{base_idx + idx}"
                add_trace(sub_stage, status="running", detail=rq)
                try:
                    rows = _web_search.search_and_fetch(rq, max_results=6, fetch_top_k=3)
                except Exception as e:
                    add_trace(sub_stage, status="failed", detail=f"{rq}: {str(e)[:140]}")
                    continue
                if not rows:
                    add_trace(sub_stage, status="failed", detail=f"{rq}: no result")
                    continue
                web_results_for_answer.extend(rows[:5])
                provider_counts = Counter(str(getattr(it, "provider", "") or "unknown") for it in rows[:8])
                fetch_counts = Counter(str(getattr(it, "fetch_mode", "") or "none") for it in rows[:8])
                retry_provider_totals.update(provider_counts)
                retry_fetch_totals.update(fetch_counts)
                web_provider_totals.update(provider_counts)
                web_fetch_mode_totals.update(fetch_counts)
                provider_note = ",".join(f"{name}:{count}" for name, count in provider_counts.most_common(3))
                fetch_note = ",".join(f"{name}:{count}" for name, count in fetch_counts.most_common(3))
                persisted = _persist_web_search_results(
                    db,
                    current_user.id,
                    query=rq,
                    results=rows,
                )
                web_citations.extend(persisted)
                for ridx, cite in enumerate(persisted, start=1):
                    evidence_count += 1
                    snippet = ""
                    provider_name = "unknown"
                    fetch_mode = "none"
                    if ridx - 1 < len(rows):
                        row = rows[ridx - 1]
                        snippet = (
                            (getattr(row, "fetched_excerpt", "") or "").strip()
                            or (row.snippet or "").strip()
                        )[:280]
                        provider_name = str(getattr(row, "provider", "") or "unknown")
                        fetch_mode = str(getattr(row, "fetch_mode", "") or "none")
                    emit(
                        "evidence",
                        {
                            "citation": cite.model_dump(),
                            "snippet": snippet,
                            "query": rq,
                            "provider": provider_name,
                            "fetch_mode": fetch_mode,
                            "progress": {
                                "query_index": idx,
                                "query_total": len(retry_queries),
                                "evidence_count": evidence_count,
                            },
                        },
                    )
                add_trace(
                    sub_stage,
                    status="completed",
                    detail=f"{rq}; p={provider_note or 'unknown'}; f={fetch_note or 'none'}",
                    count=len(persisted),
                )
            retry_provider_note = ",".join(f"{name}:{count}" for name, count in retry_provider_totals.most_common(4))
            retry_fetch_note = ",".join(f"{name}:{count}" for name, count in retry_fetch_totals.most_common(4))
            add_trace(
                "web_search",
                status="completed" if web_citations else "failed",
                detail=f"verifier retry finished; p={retry_provider_note or 'none'}; f={retry_fetch_note or 'none'}",
                count=len(web_citations),
            )
            citations = _dedupe_citations([*local_citations, *web_citations], limit=max_citations)
            add_trace(
                "message_hub",
                status="completed",
                detail=f"post-retry merge local={len(local_citations)} web={len(web_citations)}",
                count=len(citations),
            )
            if web_results_for_answer and (
                provider == "rule_based"
                or _looks_like_link_dump_answer(answer)
                or not _answer_has_fact_signal(answer)
            ):
                guarded = _compose_web_first_answer(payload.query, web_results_for_answer)
                if guarded:
                    answer = guarded
                add_trace(
                    "generation",
                    status="completed",
                    detail="response refreshed after verifier retry; retrieval evidence guard applied",
                    count=len(citations),
                )
            verified, verify_reason = _verify_reply_answer(
                query=payload.query,
                answer=answer,
                need_web_search=bool(need_web_search or retried_web),
                citations=citations,
            )
            grounded, grounding_reason = _judge_answer_grounding(
                query=payload.query,
                answer=answer,
                citations=citations,
                intent_contract=intent_contract,
                service=service,
                provider=provider,
            )
            add_trace(
                "grounding_judge",
                status="completed" if grounded else "failed",
                detail=f"post_retry:{grounding_reason}",
                count=len(citations),
            )
            coverage_ok, coverage_reason = _check_evidence_coverage(
                query=payload.query,
                intent_contract=intent_contract,
                answer=answer,
                citations=citations,
                web_results=web_results_for_answer,
            )
            add_trace(
                "coverage_verifier",
                status="completed" if coverage_ok else "failed",
                detail=f"post_retry:{coverage_reason}",
                count=len(citations),
            )

    verifier_detail = verify_reason
    if retried_web:
        verifier_detail = f"{verify_reason}; retried_web={retried_web}"
    if not grounded:
        verifier_detail = f"{verifier_detail}; grounding={grounding_reason}"
    if not coverage_ok:
        verifier_detail = f"{verifier_detail}; coverage={coverage_reason}"
    add_trace(
        "reply_verifier",
        status="completed" if (verified and grounded and coverage_ok) else "failed",
        detail=verifier_detail,
        count=len(citations),
    )
    answer = _enforce_answer_first(
        query=payload.query,
        answer=answer,
        citations=citations,
        web_results=web_results_for_answer,
        memory_summary=memory_summary,
        brief_summary=brief_summary,
        todo_titles=todo_titles,
        image_count=len(images),
    )
    answer, maybe_expression = _extract_expression_tag(answer)
    if maybe_expression:
        expression = maybe_expression

    trace_should_run = bool(route.get("trace_agent")) or bool(planned_track_suggestion)
    track_suggestion = planned_track_suggestion if isinstance(planned_track_suggestion, dict) else None
    trace_local_citations: list[AelinCitation] = []
    trace_web_citations: list[AelinCitation] = []
    trace_web_results: list[WebSearchResult] = []
    if trace_should_run:
        add_trace(
            "trace_agent",
            status="running",
            detail=f"dispatching local={len(trace_local_boundaries)} web={len(trace_web_boundaries)}",
        )
        add_trace(
            "trace_dispatch",
            status="completed",
            detail=f"context_boundaries={len(trace_local_boundaries) + len(trace_web_boundaries)}",
            count=len(trace_local_boundaries) + len(trace_web_boundaries),
        )
        trace_jobs: list[dict[str, Any]] = []
        for idx, boundary in enumerate(trace_local_boundaries, start=1):
            sub_query = str(boundary.get("query") or payload.query).strip()[:180]
            sub_scope = str(boundary.get("scope") or sub_query).strip()[:120]
            add_trace(f"trace_local_subagent_{idx}", status="running", detail=sub_scope or sub_query)
            trace_jobs.append(
                {
                    "kind": "local",
                    "idx": idx,
                    "query": sub_query,
                    "scope": sub_scope,
                }
            )
        for idx, boundary in enumerate(trace_web_boundaries, start=1):
            sub_query = str(boundary.get("query") or payload.query).strip()[:180]
            sub_scope = str(boundary.get("scope") or sub_query).strip()[:120]
            add_trace(f"trace_web_subagent_{idx}", status="running", detail=sub_scope or sub_query)
            trace_jobs.append(
                {
                    "kind": "web",
                    "idx": idx,
                    "query": sub_query,
                    "scope": sub_scope,
                }
            )

        def _trace_local_lookup(raw_query: str) -> tuple[list[AelinCitation], str]:
            local_db = create_session()
            try:
                bundle = _build_context_bundle(
                    local_db,
                    current_user.id,
                    workspace=payload.workspace,
                    query=raw_query,
                )
                cites = _to_citations(bundle["focus_items_raw"], payload.max_citations)
                return cites, ""
            except Exception as exc:
                return [], str(exc)[:140]
            finally:
                try:
                    local_db.close()
                except Exception:
                    pass

        def _trace_web_lookup(raw_query: str) -> list[WebSearchResult]:
            return _web_search.search_and_fetch(raw_query, max_results=5, fetch_top_k=2)

        futures: dict[Any, dict[str, Any]] = {}
        if trace_jobs:
            max_trace_workers = max(1, min(len(trace_jobs), _MAX_LOCAL_SUBAGENTS + _MAX_WEB_SUBAGENTS))
            with ThreadPoolExecutor(max_workers=max_trace_workers) as pool:
                for job in trace_jobs:
                    if job["kind"] == "local":
                        futures[pool.submit(_trace_local_lookup, str(job["query"]))] = job
                    else:
                        futures[pool.submit(_trace_web_lookup, str(job["query"]))] = job

                for fut in as_completed(futures):
                    job = futures[fut]
                    kind = str(job.get("kind") or "")
                    idx = int(job.get("idx") or 0)
                    query_text = str(job.get("query") or "")
                    scope_text = str(job.get("scope") or query_text)
                    if kind == "local":
                        sub_stage = f"trace_local_subagent_{idx}"
                        try:
                            cites, trace_local_error = fut.result()
                        except Exception as e:
                            add_trace(sub_stage, status="failed", detail=f"{scope_text or query_text}: {str(e)[:140]}")
                            continue
                        if trace_local_error:
                            add_trace(sub_stage, status="failed", detail=f"{scope_text or query_text}: {trace_local_error}")
                            continue
                        trace_local_citations.extend(cites or [])
                        add_trace(sub_stage, status="completed", detail=scope_text or query_text, count=len(cites or []))
                        continue

                    sub_stage = f"trace_web_subagent_{idx}"
                    try:
                        rows = fut.result() or []
                    except Exception as e:
                        add_trace(sub_stage, status="failed", detail=f"{scope_text or query_text}: {str(e)[:140]}")
                        continue
                    if not rows:
                        add_trace(sub_stage, status="failed", detail=f"{scope_text or query_text}: no result")
                        continue
                    trace_web_results.extend(rows[:5])
                    provider_counts = Counter(str(getattr(it, "provider", "") or "unknown") for it in rows[:8])
                    fetch_counts = Counter(str(getattr(it, "fetch_mode", "") or "none") for it in rows[:8])
                    provider_note = ",".join(f"{name}:{count}" for name, count in provider_counts.most_common(3))
                    fetch_note = ",".join(f"{name}:{count}" for name, count in fetch_counts.most_common(3))
                    try:
                        persisted = _persist_web_search_results(
                            db,
                            current_user.id,
                            query=query_text,
                            results=rows,
                        )
                    except Exception:
                        persisted = []
                    trace_web_citations.extend(persisted)
                    add_trace(
                        sub_stage,
                        status="completed",
                        detail=f"{scope_text or query_text}; p={provider_note or 'unknown'}; f={fetch_note or 'none'}",
                        count=len(persisted),
                    )

        if trace_local_citations:
            trace_local_citations = _hydrate_citation_avatars(db, current_user.id, trace_local_citations)
        trace_merged = _dedupe_citations([*trace_local_citations, *trace_web_citations], limit=max_citations)
        if trace_merged:
            citations = _dedupe_citations([*citations, *trace_merged], limit=max_citations)
            add_trace(
                "message_hub",
                status="completed",
                detail=f"trace merge local={len(trace_local_citations)} web={len(trace_web_citations)}",
                count=len(citations),
            )
            web_results_for_answer.extend(trace_web_results[:5])

        suggestion, trace_reason = _trace_agent_suggestion(
            query=payload.query,
            planned_track_suggestion=track_suggestion if isinstance(track_suggestion, dict) else None,
            citations=citations,
            need_web_search=bool(need_web_search or retried_web or trace_web_citations),
        )
        if suggestion:
            track_suggestion = suggestion
            source_list = sorted({str(it.source or "").strip() for it in citations if str(it.source or "").strip()})
            emit(
                "confirmed",
                {
                    "items": [str(track_suggestion.get("target") or "").strip()[:240]],
                    "source_count": len(source_list),
                    "sources": source_list[:5],
                },
            )
            add_trace("trace_agent", status="completed", detail=trace_reason, count=1)
        else:
            add_trace("trace_agent", status="completed", detail=trace_reason, count=0)
    else:
        add_trace("trace_agent", status="skipped", detail="trace route disabled")

    if payload.use_memory and answer:
        try:
            _memory.update_after_turn(
                db,
                current_user.id,
                [{"role": "user", "content": payload.query}],
                answer,
            )
        except Exception:
            pass
    try:
        if payload.use_memory and answer:
            db.commit()
        elif web_citations or trace_web_citations:
            db.commit()
    except Exception:
        db.rollback()

    final_memory_summary = str(active_bundle.get("summary") or memory_summary or "")
    response = AelinChatResponse(
        answer=answer,
        expression=expression,
        citations=citations,
        actions=_build_actions(
            payload.query,
            citations,
            has_todos=bool(todo_titles),
            track_suggestion=track_suggestion if isinstance(track_suggestion, dict) else None,
        ),
        tool_trace=tool_trace[:64],
        memory_summary=final_memory_summary,
        generated_at=datetime.now(timezone.utc),
    )
    return response


@router.get("/context", response_model=AelinContextResponse)
def get_aelin_context(
    workspace: str = Query(default="default", min_length=1, max_length=64),
    query: str = Query(default="", max_length=400),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user),
):
    bundle = _build_context_bundle(
        db,
        current_user.id,
        workspace=workspace,
        query=query,
    )
    return AelinContextResponse(
        workspace=bundle["workspace"],
        summary=bundle["summary"],
        focus_items=bundle["focus_items"],
        notes=bundle["notes"],
        notes_count=bundle["notes_count"],
        todos=bundle["todos"],
        pin_recommendations=bundle["pin_recommendations"],
        daily_brief=bundle["daily_brief"],
        layout_cards=bundle["layout_cards"],
        memory_layers=bundle["memory_layers"],
        notifications=bundle["notifications"],
        generated_at=datetime.now(timezone.utc),
    )


@router.get("/proactive/poll", response_model=AelinProactivePollResponse)
def poll_aelin_proactive_events(
    workspace: str = Query(default="default", min_length=1, max_length=64),
    limit: int = Query(default=8, ge=1, le=24),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user),
):
    now = datetime.now(timezone.utc)
    workspace_norm = _normalize_workspace(workspace)
    max_items = max(1, min(24, int(limit or 8)))

    existing, state = _load_proactive_state(db, user_id=current_user.id, workspace=workspace_norm)
    initialized = bool(state.get("initialized"))
    seen_focus_ids = _safe_int_list(state.get("seen_focus_message_ids"), max_items=_PROACTIVE_SEEN_LIMIT)
    seen_focus_set = set(seen_focus_ids)
    tracking_status_prev = state.get("tracking_status") if isinstance(state.get("tracking_status"), dict) else {}

    events: list[dict[str, Any]] = []
    brief = _memory.build_daily_brief(db, current_user.id)
    top_updates = brief.get("top_updates") if isinstance(brief, dict) else []
    if not isinstance(top_updates, list):
        top_updates = []

    for row in top_updates[:10]:
        if not isinstance(row, dict):
            continue
        try:
            message_id = int(row.get("message_id") or 0)
        except Exception:
            message_id = 0
        if message_id <= 0:
            continue
        if message_id in seen_focus_set:
            continue
        title = str(row.get("title") or "").strip()
        if not title:
            continue
        source_label = str(row.get("source_label") or row.get("source") or "æ¥æº")
        sender = str(row.get("sender") or "unknown")
        events.append(
            {
                "id": f"proactive-focus-{message_id}",
                "level": "info",
                "title": f"å‘ç°æ–°åŠ¨æ€: {title[:80]}",
                "detail": f"{source_label} Â· {sender}",
                "source": "proactive",
                "ts": now.isoformat(),
                "action_kind": "open_message",
                "action_payload": {"message_id": str(message_id)},
            }
        )
        seen_focus_set.add(message_id)
        if len(events) >= max_items:
            break

    tracking_events = _load_tracking_events(db, user_id=current_user.id, limit=80)
    tracking_status_next: dict[str, str] = {}
    for key, event in tracking_events.items():
        if not isinstance(event, dict):
            continue
        status = str(event.get("status") or "active").strip().lower() or "active"
        tracking_status_next[key] = status
        prev = str(tracking_status_prev.get(key) or "").strip().lower()
        if not initialized:
            continue
        if prev and prev == status:
            continue
        target = str(event.get("target") or "").strip()
        source = str(event.get("source") or "auto").strip()
        query = str(event.get("query") or "").strip()
        message_id = int(event.get("message_id") or 0) if str(event.get("message_id") or "").isdigit() else 0
        detail_bits = [f"{source} Â· çŠ¶æ€ {status}"]
        if query:
            detail_bits.append(f"è§¦å‘: {query[:80]}")
        payload: dict[str, str] = {"target": target, "source": source}
        if message_id > 0:
            payload["message_id"] = str(message_id)
        events.append(
            {
                "id": f"proactive-track-{key}-{status}",
                "level": "success" if status in {"active", "sync_started", "tracking_enabled"} else "info",
                "title": f"è·Ÿè¸ªçŠ¶æ€æ›´æ–°: {target or 'æœªçŸ¥ç›®æ ‡'}",
                "detail": "ï¼›".join(detail_bits),
                "source": "tracking",
                "ts": str(event.get("updated_at") or now.isoformat()),
                "action_kind": "open_tracking",
                "action_payload": payload,
            }
        )
        if len(events) >= max_items:
            break

    unread_count = int(
        db.scalar(select(func.count(Message.id)).where(Message.user_id == current_user.id, Message.is_read.is_(False))) or 0
    )
    last_unread_count = int(state.get("last_unread_count") or 0)
    unread_alert_at = _parse_iso_datetime(str(state.get("last_unread_alert_at") or ""))
    unread_alert_due = unread_alert_at is None or (now - unread_alert_at) >= timedelta(hours=2)
    unread_spike = unread_count >= 6 and (unread_count >= (last_unread_count + 3))
    if (unread_spike or (unread_count >= 10 and unread_alert_due)) and len(events) < max_items:
        events.append(
            {
                "id": f"proactive-unread-{now.strftime('%Y%m%d%H')}",
                "level": "warning",
                "title": "æœªè¯»æ¶ˆæ¯å †ç§¯æé†’",
                "detail": f"å½“å‰æœ‰ {unread_count} æ¡æœªè¯»ï¼Œå»ºè®®ç°åœ¨æ¸…ç†é«˜ä»·å€¼æ›´æ–°ã€‚",
                "source": "proactive",
                "ts": now.isoformat(),
                "action_kind": "open_brief",
                "action_payload": {"path": "/"},
            }
        )
        state["last_unread_alert_at"] = now.isoformat()

    process_alert_at = _parse_iso_datetime(str(state.get("last_process_alert_at") or ""))
    process_alert_due = process_alert_at is None or (now - process_alert_at) >= timedelta(minutes=40)
    process_alert_pid = int(state.get("last_process_alert_pid") or 0)
    process_rows = _collect_device_process_items(sort_by="cpu", limit=6)
    top_process = process_rows[0] if process_rows else None
    if (
        top_process
        and top_process.anomaly_score >= 2.2
        and len(events) < max_items
        and (process_alert_due or int(top_process.pid) != process_alert_pid)
    ):
        reason = "ï¼›".join(top_process.anomaly_reasons[:2]) or "èµ„æºå ç”¨åé«˜"
        events.append(
            {
                "id": f"proactive-proc-{int(top_process.pid)}-{now.strftime('%Y%m%d%H%M')}",
                "level": "warning",
                "title": f"è®¾å¤‡è´Ÿè½½æé†’: {top_process.name}",
                "detail": (
                    f"CPU {top_process.cpu_percent:.1f}% Â· å†…å­˜ {top_process.memory_mb:.0f}MBï¼›{reason}"
                ),
                "source": "device",
                "ts": now.isoformat(),
                "action_kind": "open_device",
                "action_payload": {"pid": str(int(top_process.pid)), "view": "processes"},
            }
        )
        state["last_process_alert_at"] = now.isoformat()
        state["last_process_alert_pid"] = int(top_process.pid)

    if not initialized and not events and top_updates:
        row = top_updates[0] if isinstance(top_updates[0], dict) else {}
        title = str(row.get("title") or "").strip()
        if title:
            events.append(
                {
                    "id": f"proactive-hello-{now.strftime('%Y%m%d%H%M')}",
                    "level": "info",
                    "title": "Aelin å·²ä¸ºä½ å‡†å¤‡ä»Šæ—¥é‡ç‚¹",
                    "detail": title[:120],
                    "source": "proactive",
                    "ts": now.isoformat(),
                    "action_kind": "open_brief",
                    "action_payload": {"path": "/"},
                }
            )

    # é¦–æ¬¡è½®è¯¢ä¸æ¨é€å¤§æ‰¹å†å²å†…å®¹ï¼Œé¿å…ä¸€æ¬¡æ€§æ‰“æ‰°ã€‚
    if not initialized:
        events = events[:1]

    next_seen = [*seen_focus_set]
    next_seen.sort(reverse=True)
    next_state: dict[str, Any] = {
        "initialized": True,
        "workspace": workspace_norm,
        "seen_focus_message_ids": next_seen[:_PROACTIVE_SEEN_LIMIT],
        "tracking_status": tracking_status_next,
        "last_unread_count": unread_count,
        "last_unread_alert_at": str(state.get("last_unread_alert_at") or ""),
        "last_process_alert_at": str(state.get("last_process_alert_at") or ""),
        "last_process_alert_pid": int(state.get("last_process_alert_pid") or 0),
        "last_poll_at": now.isoformat(),
    }
    _save_proactive_state(
        db,
        user_id=current_user.id,
        workspace=workspace_norm,
        existing=existing,
        state=next_state,
    )
    db.commit()

    items = [AelinNotificationItem(**item) for item in events[:max_items]]
    return AelinProactivePollResponse(
        workspace=workspace_norm,
        total=len(items),
        items=items,
        generated_at=now,
    )


@router.get("/device/processes", response_model=AelinDeviceProcessResponse)
def list_device_processes(
    sort_by: str = Query(default="cpu", min_length=1, max_length=16),
    limit: int = Query(default=40, ge=1, le=200),
    current_user: User = Depends(get_current_user),
):
    _ = current_user  # Auth guard for local device APIs.
    sort_key = "memory" if str(sort_by or "").strip().lower() == "memory" else "cpu"
    items = _collect_device_process_items(sort_by=sort_key, limit=limit)
    return AelinDeviceProcessResponse(
        sort_by=sort_key,
        total=len(items),
        items=items,
        generated_at=datetime.now(timezone.utc),
    )


@router.post("/device/processes/{pid}/action", response_model=AelinDeviceProcessActionResponse)
def run_device_process_action(
    pid: int,
    payload: AelinDeviceProcessActionRequest,
    current_user: User = Depends(get_current_user),
):
    _ = current_user  # Auth guard for local device APIs.
    action = str(payload.action or "").strip().lower()
    if action not in _DEVICE_ALLOWED_PROCESS_ACTIONS:
        return AelinDeviceProcessActionResponse(
            pid=int(pid),
            action=action,
            ok=False,
            detail=f"unsupported action: {action}",
            generated_at=datetime.now(timezone.utc),
        )
    if psutil is None:
        return AelinDeviceProcessActionResponse(
            pid=int(pid),
            action=action,
            ok=False,
            detail="psutil unavailable",
            generated_at=datetime.now(timezone.utc),
        )

    try:
        proc = psutil.Process(int(pid))
        proc_name = str(proc.name() or "").strip().lower()
    except Exception as exc:
        return AelinDeviceProcessActionResponse(
            pid=int(pid),
            action=action,
            ok=False,
            detail=str(exc),
            generated_at=datetime.now(timezone.utc),
        )

    critical_names = {"system", "idle", "csrss.exe", "wininit.exe", "services.exe", "lsass.exe", "svchost.exe"}
    if action == "terminate" and proc_name in critical_names:
        return AelinDeviceProcessActionResponse(
            pid=int(pid),
            action=action,
            ok=False,
            detail=f"blocked critical process: {proc_name}",
            generated_at=datetime.now(timezone.utc),
        )

    if action == "terminate":
        try:
            proc.terminate()
            try:
                proc.wait(timeout=2.5)
            except Exception:
                proc.kill()
            return AelinDeviceProcessActionResponse(
                pid=int(pid),
                action=action,
                ok=True,
                detail="process terminated",
                generated_at=datetime.now(timezone.utc),
            )
        except Exception as exc:
            return AelinDeviceProcessActionResponse(
                pid=int(pid),
                action=action,
                ok=False,
                detail=str(exc),
                generated_at=datetime.now(timezone.utc),
            )

    target = "high" if action == "set_high_priority" else "low"
    ok, detail = _set_process_priority(int(pid), target)
    return AelinDeviceProcessActionResponse(
        pid=int(pid),
        action=action,
        ok=ok,
        detail=detail,
        generated_at=datetime.now(timezone.utc),
    )


@router.post("/device/processes/optimize", response_model=AelinDeviceOptimizeResponse)
def optimize_device_processes(
    current_user: User = Depends(get_current_user),
):
    _ = current_user  # Auth guard for local device APIs.
    candidates = _collect_device_process_items(sort_by="cpu", limit=40)
    steps: list[str] = []
    warnings: list[str] = []
    affected: list[int] = []
    for row in candidates:
        if row.anomaly_score < 1.6:
            continue
        if not row.safe_to_terminate:
            continue
        ok, detail = _set_process_priority(int(row.pid), "low")
        if ok:
            affected.append(int(row.pid))
            steps.append(f"{row.name} (PID {row.pid}) -> low priority")
        else:
            warnings.append(f"{row.name} (PID {row.pid}) è°ƒæ•´å¤±è´¥: {detail}")
        if len(affected) >= 4:
            break
    if not steps:
        steps.append("æ²¡æœ‰å¯ä¼˜åŒ–çš„é«˜å ç”¨ç”¨æˆ·è¿›ç¨‹ã€‚")
    return AelinDeviceOptimizeResponse(
        optimized_count=len(affected),
        affected_pids=affected,
        steps=steps[:12],
        warnings=warnings[:12],
        generated_at=datetime.now(timezone.utc),
    )


@router.get("/device/mode", response_model=AelinDeviceModeApplyResponse)
def get_device_mode_state(
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user),
):
    _, state = _load_device_mode_state(db, user_id=current_user.id)
    mode = _normalize_device_mode(str(state.get("mode") or "normal"))
    status = str(state.get("status") or "applied").strip().lower() or "applied"
    summary = str(state.get("summary") or f"å½“å‰æ¨¡å¼: {mode}").strip()
    steps = state.get("steps") if isinstance(state.get("steps"), list) else []
    warnings = state.get("warnings") if isinstance(state.get("warnings"), list) else []
    return AelinDeviceModeApplyResponse(
        mode=mode,
        status=status,
        summary=summary,
        steps=[str(x) for x in steps][:12],
        warnings=[str(x) for x in warnings][:12],
        generated_at=datetime.now(timezone.utc),
    )


@router.post("/device/mode/apply", response_model=AelinDeviceModeApplyResponse)
def apply_device_mode(
    payload: AelinDeviceModeApplyRequest,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user),
):
    mode, status, summary, steps, warnings = _apply_device_mode(payload.mode)
    existing, _ = _load_device_mode_state(db, user_id=current_user.id)
    state = {
        "mode": mode,
        "status": status,
        "summary": summary,
        "steps": steps[:12],
        "warnings": warnings[:12],
        "updated_at": datetime.now(timezone.utc).isoformat(),
    }
    _save_device_mode_state(db, user_id=current_user.id, existing=existing, payload=state)
    db.commit()
    return AelinDeviceModeApplyResponse(
        mode=mode,
        status=status,
        summary=summary,
        steps=steps[:12],
        warnings=warnings[:12],
        generated_at=datetime.now(timezone.utc),
    )


@router.post("/chat", response_model=AelinChatResponse)
def aelin_chat(
    payload: AelinChatRequest,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user),
):
    return _aelin_chat_impl(payload, db, current_user)


@router.get("/notifications", response_model=AelinNotificationResponse)
def list_aelin_notifications(
    limit: int = Query(default=24, ge=1, le=100),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user),
):
    items = [AelinNotificationItem(**item) for item in _memory.build_notifications(db, current_user.id, limit=limit)]
    return AelinNotificationResponse(
        total=len(items),
        items=items,
        generated_at=datetime.now(timezone.utc),
    )

    # Legacy implementation kept below temporarily for reference.
    tool_trace: list[AelinToolStep] = []

    def add_trace(stage: str, *, status: str = "completed", detail: str = "", count: int = 0) -> None:
        tool_trace.append(AelinToolStep(stage=stage, status=status, detail=detail[:240], count=max(0, int(count or 0))))

    service, provider = _resolve_llm_service(db, current_user)
    llm_generation_failed = False

    base_bundle = _build_context_bundle(
        db,
        current_user.id,
        workspace=payload.workspace,
        query="",
    )
    memory_summary = str(base_bundle["summary"] or "")
    brief_summary = base_bundle["daily_brief"].summary if base_bundle.get("daily_brief") else ""
    todo_titles = [item.title for item in base_bundle["todos"]]
    images = _normalize_images(payload.images)
    history_turns = _normalize_history(payload.history)

    tool_plan = _plan_tool_usage(
        query=payload.query,
        service=service,
        provider=provider,
        memory_summary=memory_summary,
    )
    need_local_search = bool(tool_plan.get("need_local_search"))
    need_web_search = bool(tool_plan.get("need_web_search"))
    planning_reason = str(tool_plan.get("reason") or "planner:none")
    web_queries = _normalize_web_queries(payload.query, tool_plan.get("web_queries"))
    track_suggestion = tool_plan.get("track_suggestion")
    add_trace(
        "planner",
        status="completed",
        detail=f"{planning_reason}; local={'on' if need_local_search else 'off'}; web={'on' if need_web_search else 'off'}",
    )

    active_bundle = base_bundle
    local_citations: list[AelinCitation] = []
    if need_local_search:
        active_bundle = _build_context_bundle(
            db,
            current_user.id,
            workspace=payload.workspace,
            query=payload.query,
        )
        local_citations = _to_citations(active_bundle["focus_items_raw"], payload.max_citations)
        local_citations = _hydrate_citation_avatars(db, current_user.id, local_citations)
        add_trace("local_search", status="completed", detail="retrieved local memory signals", count=len(local_citations))
    else:
        add_trace("local_search", status="skipped", detail="planner disabled local search")

    web_citations: list[AelinCitation] = []
    web_evidence_lines: list[str] = []
    web_results_for_answer: list[WebSearchResult] = []
    if need_web_search and web_queries:
        for q in web_queries[:3]:
            rows = _web_search.search_and_fetch(q, max_results=5, fetch_top_k=3)
            if not rows:
                continue
            web_results_for_answer.extend(rows[:5])
            web_citations.extend(_persist_web_search_results(db, current_user.id, query=q, results=rows))
            for item in rows[:5]:
                host = _domain_from_url(item.url)
                snippet = ((getattr(item, "fetched_excerpt", "") or "").strip() or (item.snippet or "").strip())
                evidence = f"- [Web] {item.title} ({host})"
                if snippet:
                    evidence += f" | {snippet}"
                web_evidence_lines.append(evidence)
        add_trace("web_search", status="completed", detail="; ".join(web_queries[:3]), count=len(web_citations))
    elif need_web_search:
        add_trace("web_search", status="failed", detail="planner enabled but no query/result")
    else:
        add_trace("web_search", status="skipped", detail="planner disabled web search")

    citations = sorted(
        [*local_citations, *web_citations],
        key=lambda x: float(x.score or 0.0),
        reverse=True,
    )[: max(1, min(20, payload.max_citations))]

    pin_lines = [
        f"{item.display_name}ï¼ˆscore {item.score:.1f}ï¼Œæœªè¯» {item.unread_count}ï¼‰"
        for item in active_bundle["pin_recommendations"][:4]
    ]
    memory_prompt = _memory.build_system_memory_prompt(
        db,
        current_user.id,
        query=payload.query if need_local_search else "",
    )

    if provider == "rule_based":
        add_trace("generation", status="completed", detail="rule_based response path")
        if local_citations:
            answer = _rule_based_answer(
                payload.query,
                memory_summary,
                citations,
                brief_summary=brief_summary,
                todo_titles=todo_titles,
                image_count=len(images),
            )
        elif web_evidence_lines:
            answer = _compose_web_first_answer(payload.query, web_results_for_answer)
        else:
            answer = _rule_based_chat_answer(
                payload.query,
                memory_summary=memory_summary,
                brief_summary=brief_summary,
            )
    elif not service.is_configured():
        add_trace("generation", status="failed", detail="llm client not configured")
        answer = (
            "å½“å‰æ— æ³•åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼ŒAelin å·²åœæ­¢é™é»˜é™çº§ã€‚"
            "\n\nè¯·æ£€æŸ¥è®¾ç½®ä¸­çš„ Provider / Base URL / API Key æ˜¯å¦æ­£ç¡®ï¼Œç„¶åé‡è¯•ã€‚"
            "\n\næç¤ºï¼šBase URL åº”å¡«å†™ API æ ¹åœ°å€ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„ /chat/completions è·¯å¾„ã€‚"
        )
    else:
        evidence_block = "\n".join(
            f"- [{it.source_label}] {it.title} ({it.sender}, {it.received_at})"
            for it in citations[:8]
        ) if citations else ""
        prompt = (
            "You are Aelin, a signal-native chat agent.\n"
            "Answer in Simplified Chinese.\n"
            "Default to normal conversational interaction.\n"
            "You must answer the user's question directly first.\n"
            "Do not ask users to manually search websites when web evidence is already provided.\n"
            "Only cite synced evidence when retrieval is actually provided.\n"
            "You MUST answer the user's actual question directly first.\n"
            "Tracking/subscription suggestions are optional and must be placed after the direct answer.\n"
            "If evidence is insufficient, say uncertainty explicitly and avoid fabricating details.\n"
            "Keep answer concise, practical, and natural.\n"
            "You may use 0-2 natural emoji in the answer body when it helps tone.\n"
            "Use daily brief and pending todos only when they help this specific user question.\n"
            "Aelin has 11 expressions. Choose one according to semantics below:\n"
            + _expression_mapping_prompt()
            + "\n"
            "You MUST append exactly one tag at the very end: [expression:exp-XX].\n"
            "Optional emoji control tag is allowed only before the final expression tag: [emoji:ğŸ™‚].\n"
            "Do not output any other expression format.\n"
        )
        retrieval_note = f"è§„åˆ’ç»“æœï¼š{planning_reason}ã€‚"
        retrieval_note += f" local_search={'on' if need_local_search else 'off'}; web_search={'on' if need_web_search else 'off'}ã€‚"
        user_msg = (
            f"ç”¨æˆ·é—®é¢˜ï¼š{payload.query.strip()}\n\n"
            f"å·¥å…·è§„åˆ’ï¼š{retrieval_note}\n\n"
            + (
                "æœ€è¿‘å¯¹è¯ï¼ˆä¾›è¿ç»­ä¸Šä¸‹æ–‡å‚è€ƒï¼‰ï¼š\n"
                + "\n".join(
                    f"- {'ç”¨æˆ·' if turn['role'] == 'user' else 'Aelin'}: {turn['content'][:220]}"
                    for turn in history_turns[-6:]
                )
                + "\n\n"
                if history_turns
                else ""
            )
            + f"é•¿æœŸè®°å¿†æ‘˜è¦ï¼š{memory_summary or 'æš‚æ— '}\n\n"
            + f"ä»Šæ—¥ç®€æŠ¥ï¼š{brief_summary or 'æš‚æ— '}\n\n"
            + f"å¾…è·Ÿè¿›äº‹é¡¹ï¼š{'; '.join(todo_titles[:5]) if todo_titles else 'æš‚æ— '}\n\n"
            + f"ç½®é¡¶å»ºè®®ï¼š{'; '.join(pin_lines) if pin_lines else 'æš‚æ— '}\n\n"
            + (
                "ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ï¼š\n"
                + "\n".join(f"- {img['name'] or 'image'}" for img in images)
                + "\n\n"
                if images
                else ""
            )
            + (f"æœ¬åœ°å¯ç”¨è¯æ®ï¼š\n{evidence_block}\n\n" if evidence_block else "")
            + (f"è”ç½‘æœç´¢ç»“æœï¼š\n{chr(10).join(web_evidence_lines[:8])}\n" if web_evidence_lines else "")
        )
        llm_messages = [{"role": "system", "content": prompt}]
        if memory_prompt:
            llm_messages.append({"role": "system", "content": memory_prompt})
        if history_turns:
            llm_messages.extend(history_turns[-10:])
        if images:
            user_content: list[dict[str, Any]] = [{"type": "text", "text": user_msg}]
            for img in images:
                user_content.append({"type": "image_url", "image_url": {"url": img["data_url"]}})
            llm_messages.append({"role": "user", "content": user_content})
        else:
            llm_messages.append({"role": "user", "content": user_msg})
        llm_error: str | None = None
        try:
            raw = service._chat(
                messages=llm_messages,
                max_tokens=520,
                stream=False,
            )
            answer = str(raw).strip() if raw else ""
            add_trace("generation", status="completed", detail="llm generation succeeded")
        except Exception as e:
            llm_error = str(e)
            answer = ""
            llm_generation_failed = True
            add_trace("generation", status="failed", detail=f"llm error: {llm_error[:180]}")
            if images:
                # Some providers/models are text-only; retry once without image payload.
                fallback_messages: list[dict[str, Any]] = [{"role": "system", "content": prompt}]
                if memory_prompt:
                    fallback_messages.append({"role": "system", "content": memory_prompt})
                fallback_messages.append({"role": "user", "content": user_msg})
                try:
                    raw = service._chat(
                        messages=fallback_messages,
                        max_tokens=520,
                        stream=False,
                    )
                    maybe = str(raw).strip() if raw else ""
                    if maybe:
                        answer = (
                            "å½“å‰æ¨¡å‹å¯èƒ½ä¸æ”¯æŒå›¾ç‰‡è¾“å…¥ï¼Œæˆ‘å·²å…ˆåŸºäºæ–‡æœ¬ä¸Šä¸‹æ–‡å›ç­”ã€‚\n\n"
                            + maybe
                        )
                        add_trace("generation", status="completed", detail="fallback text-only generation succeeded")
                except Exception as e2:
                    llm_error = llm_error or str(e2)
                    answer = ""
        if not answer:
            if citations:
                answer = _rule_based_answer(
                    payload.query,
                    memory_summary,
                    citations,
                    brief_summary=brief_summary,
                    todo_titles=todo_titles,
                    image_count=len(images),
                )
            else:
                answer = (
                    "æˆ‘åˆšæ‰è°ƒç”¨å¤–éƒ¨æ¨¡å‹å¤±è´¥äº†ï¼Œå…ˆç»™ä½ ä¸€ä¸ªä¿åº•å›å¤ã€‚"
                    + (f"\n\né”™è¯¯ï¼š{llm_error}" if llm_error else "")
                    + "\n\nä½ å¯ä»¥å…ˆåœ¨è®¾ç½®é¡µæµ‹è¯• Provider è¿é€šæ€§ï¼Œç„¶åæˆ‘å†ç»§ç»­æ­£å¸¸å¯¹è¯ã€‚"
                    + "\n\n"
                    + _rule_based_chat_answer(
                        payload.query,
                        memory_summary=memory_summary,
                        brief_summary=brief_summary,
                    )
                )
        if answer and web_results_for_answer and _looks_like_link_dump_answer(answer):
            answer = _compose_web_first_answer(payload.query, web_results_for_answer)

    answer, tagged_expression = _extract_expression_tag(answer)
    answer, tagged_emoji = _extract_emoji_tag(answer)
    expression = tagged_expression or _pick_expression(payload.query, answer, generation_failed=llm_generation_failed)
    answer = _apply_answer_emoji(answer, expression, explicit_emoji=tagged_emoji)

    if payload.use_memory and answer:
        _memory.update_after_turn(
            db,
            current_user.id,
            [{"role": "user", "content": payload.query}],
            answer,
        )
        db.commit()
    elif need_web_search and web_queries:
        # Persist fetched web records even if this round does not update chat memory.
        db.commit()

    return AelinChatResponse(
        answer=answer,
        expression=expression,
        citations=citations,
        actions=_build_actions(
            payload.query,
            citations,
            has_todos=bool(todo_titles),
            track_suggestion=track_suggestion if isinstance(track_suggestion, dict) else None,
        ),
        tool_trace=tool_trace[:8],
        memory_summary=memory_summary,
        generated_at=datetime.now(timezone.utc),
    )


@router.post("/chat/stream")
def aelin_chat_stream(
    payload: AelinChatRequest,
    current_user: User = Depends(get_current_user),
):
    def _event_iter():
        event_queue: queue.Queue[tuple[str, dict[str, Any]]] = queue.Queue()
        done_token = "__done__"

        def _push(event: str, data: dict[str, Any]) -> None:
            event_queue.put((event, data))

        def _worker() -> None:
            local_db = create_session()
            try:
                user = local_db.get(User, int(current_user.id)) or current_user
                result = _aelin_chat_impl(payload, local_db, user, event_cb=_push)
                _push("final", {"result": result.model_dump()})
            except Exception as e:
                _push("error", {"message": str(e)[:500] or "stream error"})
            finally:
                try:
                    local_db.close()
                except Exception:
                    pass
                _push("done", {"ts": _now_ms(), "status": done_token})

        _push(
            "start",
            {
                "ts": _now_ms(),
                "query": payload.query.strip()[:180],
                "workspace": payload.workspace,
                "search_mode": _normalize_search_mode(getattr(payload, "search_mode", "auto")),
            },
        )
        worker = threading.Thread(target=_worker, daemon=True)
        worker.start()

        while True:
            event, data = event_queue.get()
            yield _sse_event(event, data)
            if event == "done":
                break

    return StreamingResponse(_event_iter(), media_type="text/event-stream")


def _infer_tracking_source(target: str) -> str:
    text = (target or "").strip().lower()
    if any(token in text for token in ["æŠ–éŸ³", "douyin"]):
        return "douyin"
    if any(token in text for token in ["å°çº¢ä¹¦", "xiaohongshu", "xhs"]):
        return "xiaohongshu"
    if any(token in text for token in ["å¾®åš", "weibo"]):
        return "weibo"
    if any(token in text for token in ["bilibili", "bç«™", "upä¸»"]):
        return "bilibili"
    if any(token in text for token in ["twitter", "x.com", "æ¨ç‰¹", "x "]):
        return "x"
    if any(token in text for token in ["é‚®ä»¶", "é‚®ç®±", "email"]):
        return "email"
    if any(token in text for token in ["rss", "è®¢é˜…"]):
        return "rss"
    return "web"


def _extract_x_username(target: str) -> str:
    text = (target or "").strip()
    if not text:
        return ""
    match = re.search(r"(?:x\.com/|twitter\.com/)?@?([A-Za-z0-9_]{1,15})", text, flags=re.I)
    if not match:
        return ""
    return match.group(1).lstrip("@").strip()


def _extract_bilibili_uid(target: str) -> str:
    text = (target or "").strip()
    if not text:
        return ""
    match = re.search(r"(?:space\.bilibili\.com/)?([1-9]\d{3,19})", text)
    return match.group(1).strip() if match else ""


def _build_tracking_account_seed(source: str, target: str, query: str) -> dict[str, str] | None:
    text = (target or query or "").strip()
    if not text:
        return None

    if source == "x":
        username = _extract_x_username(text)
        if not username:
            return None
        return {
            "provider": "x",
            "identifier": f"x:{username}",
            "feed_url": "",
            "feed_homepage_url": f"https://x.com/{username}",
            "feed_display_name": f"X @{username}",
        }
    if source == "douyin":
        sec_uid = extract_douyin_uid(text)
        if not sec_uid:
            return None
        return {
            "provider": "douyin",
            "identifier": sec_uid,
            "feed_url": "",
            "feed_homepage_url": f"https://www.douyin.com/user/{sec_uid}",
            "feed_display_name": "æŠ–éŸ³ç”¨æˆ·",
        }
    if source == "xiaohongshu":
        user_id = extract_xhs_uid(text)
        if not user_id:
            return None
        return {
            "provider": "xiaohongshu",
            "identifier": user_id,
            "feed_url": "",
            "feed_homepage_url": f"https://www.xiaohongshu.com/user/profile/{user_id}",
            "feed_display_name": "å°çº¢ä¹¦ç”¨æˆ·",
        }
    if source == "weibo":
        uid = extract_weibo_uid(text)
        if not uid:
            return None
        return {
            "provider": "weibo",
            "identifier": uid,
            "feed_url": "",
            "feed_homepage_url": f"https://weibo.com/u/{uid}",
            "feed_display_name": "å¾®åšç”¨æˆ·",
        }
    if source == "bilibili":
        uid = _extract_bilibili_uid(text)
        if not uid:
            return None
        return {
            "provider": "bilibili",
            "identifier": f"bilibili:{uid}",
            "feed_url": "",
            "feed_homepage_url": f"https://space.bilibili.com/{uid}",
            "feed_display_name": f"Bç«™ UP {uid}",
        }
    return None


def _ensure_tracking_account(
    db: Session,
    *,
    user_id: int,
    source: str,
    target: str,
    query: str,
) -> Any | None:
    seed = _build_tracking_account_seed(source, target, query)
    if not seed:
        return None

    existing = crud.get_account_by_provider_identifier(
        db,
        user_id=user_id,
        provider=seed["provider"],
        identifier=seed["identifier"],
    )
    if existing is not None:
        return existing

    try:
        return crud.create_connected_account(
            db,
            user_id=user_id,
            provider=seed["provider"],
            identifier=seed["identifier"],
            access_token=None,
            refresh_token=None,
            feed_url=seed.get("feed_url"),
            feed_homepage_url=seed.get("feed_homepage_url"),
            feed_display_name=seed.get("feed_display_name"),
        )
    except IntegrityError:
        db.rollback()
        return crud.get_account_by_provider_identifier(
            db,
            user_id=user_id,
            provider=seed["provider"],
            identifier=seed["identifier"],
        )
    except Exception:
        db.rollback()
        return None


def _persist_tracking_event(
    db: Session,
    *,
    user_id: int,
    target: str,
    source: str,
    query: str,
    status: str,
) -> int | None:
    contact = crud.upsert_contact(db, user_id=user_id, handle="aelin:tracking", display_name="Aelin Tracking")
    now = datetime.now(timezone.utc)
    seed = f"{target}|{query}|{status}|{now.strftime('%Y%m%d%H%M%S')}"
    external_id = f"aelin-track:{source}:{hashlib.sha1(seed.encode('utf-8')).hexdigest()}"
    body = (
        f"è·Ÿè¸ªç›®æ ‡: {target}\n"
        f"æ¥æº: {source}\n"
        f"çŠ¶æ€: {status}\n"
        f"è§¦å‘é—®é¢˜: {query or 'æœªæä¾›'}\n"
        f"æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S UTC')}"
    )
    msg = crud.create_message(
        db,
        user_id=user_id,
        contact_id=contact.id,
        source="aelin",
        external_id=external_id,
        sender="Aelin",
        subject=f"è·Ÿè¸ªä»»åŠ¡ï¼š{target[:80]}",
        body=body,
        received_at=now,
        summary=f"{source} / {status}",
    )
    if msg is not None and getattr(msg, "id", None) is None:
        db.flush()
    if msg is None:
        msg = db.scalar(
            select(Message).where(
                Message.user_id == user_id,
                Message.source == "aelin",
                Message.external_id == external_id,
            )
        )
    if msg is None:
        return None
    crud.touch_contact_last_message(db, contact=contact, received_at=now)
    db.flush()
    return int(msg.id)


def _extract_tracking_field(text: str, label: str) -> str:
    if not text:
        return ""
    match = re.search(rf"{re.escape(label)}\s*[:ï¼š]\s*(.+)", text, flags=re.I)
    if not match:
        return ""
    return (match.group(1) or "").strip().splitlines()[0].strip()


def _parse_tracking_payload(raw: str) -> dict[str, str]:
    text = (raw or "").strip()
    return {
        "target": _extract_tracking_field(text, "è·Ÿè¸ªç›®æ ‡"),
        "source": _normalize_track_source(_extract_tracking_field(text, "æ¥æº") or "auto"),
        "status": _extract_tracking_field(text, "çŠ¶æ€"),
        "query": _extract_tracking_field(text, "è§¦å‘é—®é¢˜"),
        "time": _extract_tracking_field(text, "æ—¶é—´"),
    }


def _tracking_key(source: str, target: str) -> str:
    return f"{(source or 'auto').strip().lower()}::{(target or '').strip().lower()}"


def _load_tracking_events(db: Session, *, user_id: int, limit: int) -> dict[str, dict[str, Any]]:
    contact = db.scalar(
        select(Contact).where(
            Contact.user_id == user_id,
            Contact.handle == "aelin:tracking",
        )
    )
    if contact is None:
        return {}

    rows = crud.list_messages(
        db,
        user_id=user_id,
        contact_id=int(contact.id),
        limit=max(20, min(500, int(limit) * 4)),
    )
    out: dict[str, dict[str, Any]] = {}
    for msg in rows:
        parsed = _parse_tracking_payload(msg.body or "")
        target = (parsed.get("target") or "").strip()
        if not target:
            continue
        source = _normalize_track_source(parsed.get("source") or "auto")
        key = _tracking_key(source, target)
        if key in out:
            continue
        received = msg.received_at.isoformat() if msg.received_at else ""
        out[key] = {
            "message_id": int(msg.id),
            "target": target,
            "source": source,
            "query": (parsed.get("query") or "").strip(),
            "status": (parsed.get("status") or "").strip() or "active",
            "updated_at": received,
        }
    return out


@router.get("/tracking", response_model=AelinTrackingListResponse)
def list_trackings(
    limit: int = Query(default=80, ge=1, le=300),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user),
):
    notes = db.scalars(
        select(AgentMemoryNote)
        .where(
            AgentMemoryNote.user_id == current_user.id,
            AgentMemoryNote.kind == "tracking",
        )
        .order_by(AgentMemoryNote.updated_at.desc(), AgentMemoryNote.id.desc())
        .limit(limit)
    ).all()

    events_by_key = _load_tracking_events(db, user_id=current_user.id, limit=limit)
    seen_keys: set[str] = set()
    items: list[AelinTrackingItem] = []

    for note in notes:
        parsed = _parse_tracking_payload(note.content or "")
        note_source = ""
        if (note.source or "").startswith("track:"):
            note_source = (note.source or "").split(":", 1)[1].strip()
        source = _normalize_track_source(parsed.get("source") or note_source or "auto")
        target = (parsed.get("target") or "").strip()
        if not target:
            continue
        key = _tracking_key(source, target)
        seen_keys.add(key)
        event = events_by_key.get(key)
        note_updated = note.updated_at.isoformat() if note.updated_at else ""
        items.append(
            AelinTrackingItem(
                note_id=int(note.id),
                message_id=int(event["message_id"]) if event and event.get("message_id") else None,
                target=target,
                source=source,
                query=(parsed.get("query") or "").strip(),
                status=(event.get("status") if event else "") or "active",
                updated_at=note_updated,
                status_updated_at=(event.get("updated_at") if event else "") or None,
            )
        )

    for key, event in events_by_key.items():
        if key in seen_keys:
            continue
        items.append(
            AelinTrackingItem(
                note_id=None,
                message_id=int(event["message_id"]) if event.get("message_id") else None,
                target=str(event.get("target") or "").strip(),
                source=_normalize_track_source(str(event.get("source") or "auto")),
                query=str(event.get("query") or "").strip(),
                status=str(event.get("status") or "").strip() or "active",
                updated_at=str(event.get("updated_at") or ""),
                status_updated_at=str(event.get("updated_at") or "") or None,
            )
        )

    items.sort(
        key=lambda row: (row.status_updated_at or row.updated_at or ""),
        reverse=True,
    )
    trimmed = items[:limit]
    return AelinTrackingListResponse(
        total=len(trimmed),
        items=trimmed,
        generated_at=datetime.now(timezone.utc),
    )


def _matching_accounts_for_tracking(accounts: list[Any], source: str) -> list[Any]:
    if source == "email":
        email_providers = {"imap", "gmail", "outlook", "forward"}
        return [a for a in accounts if str(getattr(a, "provider", "")).lower() in email_providers]
    return [a for a in accounts if str(getattr(a, "provider", "")).lower() == source]


@router.post("/track/confirm", response_model=AelinTrackConfirmResponse)
def confirm_track_subscription(
    payload: AelinTrackConfirmRequest,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user),
):
    target = payload.target.strip()[:240]
    query = (payload.query or "").strip()[:500]
    source = _normalize_track_source(payload.source)
    if source == "auto":
        source = _infer_tracking_source(target)

    note_content = (
        f"è·Ÿè¸ªç›®æ ‡: {target}\n"
        f"æ¥æº: {source}\n"
        f"è§¦å‘é—®é¢˜: {query or 'æœªæä¾›'}"
    )
    try:
        _memory.add_note(db, current_user.id, note_content, kind="tracking", source=f"track:{source}")
    except Exception:
        pass

    tracking_message_id = _persist_tracking_event(
        db,
        user_id=current_user.id,
        target=target,
        source=source,
        query=query,
        status="created",
    )

    if source == "web":
        search_query = query or target
        rows = _web_search.search_and_fetch(search_query, max_results=7, fetch_top_k=4)
        citations = _persist_web_search_results(db, current_user.id, query=search_query, results=rows)
        tracking_message_id = _persist_tracking_event(
            db,
            user_id=current_user.id,
            target=target,
            source=source,
            query=query,
            status="seeded",
        ) or tracking_message_id
        db.commit()
        action_payload: dict[str, str] = {"path": "/desk"}
        if tracking_message_id:
            action_payload["message_id"] = str(tracking_message_id)
        if target:
            action_payload["query"] = target[:120]
        return AelinTrackConfirmResponse(
            status="tracking_enabled",
            message=(
                f"å·²å¼€å¯â€œ{target}â€çš„é•¿æœŸè·Ÿè¸ªã€‚"
                + ("æˆ‘å·²å…ˆæŠ“å–ä¸€æ‰¹å…¬å¼€ä¿¡æ¯å¹¶æŒä¹…åŒ–åˆ°æœ¬åœ°ã€‚" if citations else "æˆ‘ä¼šåœ¨åç»­å¯¹è¯ä¸­ç»§ç»­è¡¥å……æ•°æ®ã€‚")
            ),
            provider="web",
            actions=[
                AelinAction(
                    kind="open_desk",
                    title="æŸ¥çœ‹å·²ä¿å­˜æ•°æ®",
                    detail="æ‰“å¼€ Desk æŸ¥çœ‹åˆšä¿å­˜çš„è·Ÿè¸ªç»“æœ",
                    payload=action_payload,
                )
            ],
            generated_at=datetime.now(timezone.utc),
        )

    all_accounts = crud.list_accounts(db, user_id=current_user.id)
    matched = _matching_accounts_for_tracking(all_accounts, source)
    if not matched and source in {"x", "douyin", "xiaohongshu", "weibo", "bilibili"}:
        created = _ensure_tracking_account(
            db,
            user_id=current_user.id,
            source=source,
            target=target,
            query=query,
        )
        if created is not None:
            all_accounts = crud.list_accounts(db, user_id=current_user.id)
            matched = _matching_accounts_for_tracking(all_accounts, source)

    if not matched:
        tracking_message_id = _persist_tracking_event(
            db,
            user_id=current_user.id,
            target=target,
            source=source,
            query=query,
            status="needs_config",
        ) or tracking_message_id
        db.commit()
        payload_settings = {"path": "/settings", "provider": source}
        if target:
            payload_settings["target"] = target[:120]
        return AelinTrackConfirmResponse(
            status="needs_config",
            message=f"è¦è·Ÿè¸ªâ€œ{target}â€ï¼Œä½ éœ€è¦å…ˆé…ç½® {source} æ•°æ®æºã€‚",
            provider=source,
            actions=[
                AelinAction(
                    kind="open_settings",
                    title="å»è®¾ç½®æ•°æ®æº",
                    detail=f"å½“å‰ç¼ºå°‘ {source} é…ç½®ï¼Œæ‰“å¼€è®¾ç½®é¡µå®Œæˆæ¥å…¥",
                    payload=payload_settings,
                )
            ],
            generated_at=datetime.now(timezone.utc),
        )

    for account in matched[:4]:
        try:
            enqueue_sync_job(user_id=current_user.id, account_id=int(account.id), force_full=False)
        except Exception:
            continue
    tracking_message_id = _persist_tracking_event(
        db,
        user_id=current_user.id,
        target=target,
        source=source,
        query=query,
        status="sync_started",
    ) or tracking_message_id
    db.commit()
    action_payload = {"path": "/desk"}
    if tracking_message_id:
        action_payload["message_id"] = str(tracking_message_id)
    if target:
        action_payload["query"] = target[:120]
    return AelinTrackConfirmResponse(
        status="sync_started",
        message=f"å·²ä¸ºâ€œ{target}â€å¯åŠ¨ {len(matched[:4])} ä¸ªåŒæ­¥ä»»åŠ¡ï¼Œåç»­ä¼šæŒç»­æ›´æ–°å¹¶å†™å…¥æœ¬åœ°ã€‚",
        provider=source,
        actions=[
            AelinAction(
                kind="open_desk",
                title="æŸ¥çœ‹åŒæ­¥è¿›åº¦",
                detail="æ‰“å¼€ Desk è§‚å¯Ÿæ–°æ•°æ®å†™å…¥",
                payload=action_payload,
            )
        ],
        generated_at=datetime.now(timezone.utc),
    )
